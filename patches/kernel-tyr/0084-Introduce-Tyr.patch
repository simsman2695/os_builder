From 0c22c4680ba15936eef51bbd351088d2c35f99a4 Mon Sep 17 00:00:00 2001
From: Daniel Almeida <daniel.almeida@collabora.com>
Date: Sat, 25 Jan 2025 13:49:16 -0300
Subject: [PATCH 084/161] Introduce Tyr

Rust driver for ARM Mali CSF-based GPUs

The skeleton is basically taken from Nova and also rust_platform_driver.rs.

The name "Tyr" is inspired by Norse mythology, reflecting ARM's tradition of
naming their GPUs after Nordic mythological figures and places.

Co-developed-by: Alice Ryhl <alice.ryhl@google.com>
Co-developed-by: Beata Michalska  <beata.michalska@arm.com>
Co-developed-by: Carsten Haitzler <carsten.haitzler@foss.arm.com>
Co-developed-by: Rob Herring <robh@kernel.org>
Signed-off-by: Alice Ryhl <alice.ryhl@google.com>
Signed-off-by: Beata Michalska  <beata.michalska@arm.com>
Signed-off-by: Carsten Haitzler <carsten.haitzler@foss.arm.com>
Signed-off-by: Daniel Almeida <daniel.almeida@collabora.com>
Signed-off-by: Rob Herring <robh@kernel.org>
---
 drivers/gpu/drm/Kconfig                   |   2 +
 drivers/gpu/drm/Makefile                  |   1 +
 drivers/gpu/drm/tyr/Kconfig               |  33 ++
 drivers/gpu/drm/tyr/Makefile              |   3 +
 drivers/gpu/drm/tyr/driver.rs             | 459 ++++++++++++++++++
 drivers/gpu/drm/tyr/file.rs               | 412 ++++++++++++++++
 drivers/gpu/drm/tyr/flags.rs              | 137 ++++++
 drivers/gpu/drm/tyr/fw.rs                 | 369 ++++++++++++++
 drivers/gpu/drm/tyr/fw/global.rs          | 550 +++++++++++++++++++++
 drivers/gpu/drm/tyr/fw/global/cs.rs       | 539 +++++++++++++++++++++
 drivers/gpu/drm/tyr/fw/global/csg.rs      | 360 ++++++++++++++
 drivers/gpu/drm/tyr/fw/irq.rs             |  88 ++++
 drivers/gpu/drm/tyr/fw/parse.rs           | 554 ++++++++++++++++++++++
 drivers/gpu/drm/tyr/fw/parse/cursor.rs    |  91 ++++
 drivers/gpu/drm/tyr/gem.rs                | 229 +++++++++
 drivers/gpu/drm/tyr/gpu.rs                | 212 +++++++++
 drivers/gpu/drm/tyr/gpu/irq.rs            |  80 ++++
 drivers/gpu/drm/tyr/mmu.rs                | 212 +++++++++
 drivers/gpu/drm/tyr/mmu/as_lock.rs        |  89 ++++
 drivers/gpu/drm/tyr/mmu/faults.rs         | 126 +++++
 drivers/gpu/drm/tyr/mmu/irq.rs            |  70 +++
 drivers/gpu/drm/tyr/mmu/slot_allocator.rs |  59 +++
 drivers/gpu/drm/tyr/mmu/vm.rs             | 380 +++++++++++++++
 drivers/gpu/drm/tyr/mmu/vm/gpuvm.rs       | 323 +++++++++++++
 drivers/gpu/drm/tyr/mmu/vm/map_flags.rs   |  66 +++
 drivers/gpu/drm/tyr/mmu/vm/pool.rs        |  80 ++++
 drivers/gpu/drm/tyr/regs.rs               | 252 ++++++++++
 drivers/gpu/drm/tyr/sched.rs              | 429 +++++++++++++++++
 drivers/gpu/drm/tyr/sched/events.rs       | 273 +++++++++++
 drivers/gpu/drm/tyr/sched/group.rs        | 412 ++++++++++++++++
 drivers/gpu/drm/tyr/sched/job.rs          | 218 +++++++++
 drivers/gpu/drm/tyr/sched/queue.rs        | 271 +++++++++++
 drivers/gpu/drm/tyr/sched/syncs.rs        | 124 +++++
 drivers/gpu/drm/tyr/sched/tick.rs         |  36 ++
 drivers/gpu/drm/tyr/tyr.rs                |  55 +++
 drivers/gpu/drm/tyr/wait.rs               | 142 ++++++
 rust/kernel/device.rs                     |   2 +-
 rust/kernel/sync/aref.rs                  |   2 +-
 rust/uapi/uapi_helper.h                   |   1 +
 39 files changed, 7739 insertions(+), 2 deletions(-)
 create mode 100644 drivers/gpu/drm/tyr/Kconfig
 create mode 100644 drivers/gpu/drm/tyr/Makefile
 create mode 100644 drivers/gpu/drm/tyr/driver.rs
 create mode 100644 drivers/gpu/drm/tyr/file.rs
 create mode 100644 drivers/gpu/drm/tyr/flags.rs
 create mode 100644 drivers/gpu/drm/tyr/fw.rs
 create mode 100644 drivers/gpu/drm/tyr/fw/global.rs
 create mode 100644 drivers/gpu/drm/tyr/fw/global/cs.rs
 create mode 100644 drivers/gpu/drm/tyr/fw/global/csg.rs
 create mode 100644 drivers/gpu/drm/tyr/fw/irq.rs
 create mode 100644 drivers/gpu/drm/tyr/fw/parse.rs
 create mode 100644 drivers/gpu/drm/tyr/fw/parse/cursor.rs
 create mode 100644 drivers/gpu/drm/tyr/gem.rs
 create mode 100644 drivers/gpu/drm/tyr/gpu.rs
 create mode 100644 drivers/gpu/drm/tyr/gpu/irq.rs
 create mode 100644 drivers/gpu/drm/tyr/mmu.rs
 create mode 100644 drivers/gpu/drm/tyr/mmu/as_lock.rs
 create mode 100644 drivers/gpu/drm/tyr/mmu/faults.rs
 create mode 100644 drivers/gpu/drm/tyr/mmu/irq.rs
 create mode 100644 drivers/gpu/drm/tyr/mmu/slot_allocator.rs
 create mode 100644 drivers/gpu/drm/tyr/mmu/vm.rs
 create mode 100644 drivers/gpu/drm/tyr/mmu/vm/gpuvm.rs
 create mode 100644 drivers/gpu/drm/tyr/mmu/vm/map_flags.rs
 create mode 100644 drivers/gpu/drm/tyr/mmu/vm/pool.rs
 create mode 100644 drivers/gpu/drm/tyr/regs.rs
 create mode 100644 drivers/gpu/drm/tyr/sched.rs
 create mode 100644 drivers/gpu/drm/tyr/sched/events.rs
 create mode 100644 drivers/gpu/drm/tyr/sched/group.rs
 create mode 100644 drivers/gpu/drm/tyr/sched/job.rs
 create mode 100644 drivers/gpu/drm/tyr/sched/queue.rs
 create mode 100644 drivers/gpu/drm/tyr/sched/syncs.rs
 create mode 100644 drivers/gpu/drm/tyr/sched/tick.rs
 create mode 100644 drivers/gpu/drm/tyr/tyr.rs
 create mode 100644 drivers/gpu/drm/tyr/wait.rs

diff --git a/drivers/gpu/drm/Kconfig b/drivers/gpu/drm/Kconfig
index f7ea8e895c..fda1707304 100644
--- a/drivers/gpu/drm/Kconfig
+++ b/drivers/gpu/drm/Kconfig
@@ -396,6 +396,8 @@ source "drivers/gpu/drm/sprd/Kconfig"
 
 source "drivers/gpu/drm/imagination/Kconfig"
 
+source "drivers/gpu/drm/tyr/Kconfig"
+
 config DRM_HYPERV
 	tristate "DRM Support for Hyper-V synthetic video device"
 	depends on DRM && PCI && HYPERV
diff --git a/drivers/gpu/drm/Makefile b/drivers/gpu/drm/Makefile
index 4dafbdc8f8..4b2f7d7942 100644
--- a/drivers/gpu/drm/Makefile
+++ b/drivers/gpu/drm/Makefile
@@ -220,6 +220,7 @@ obj-$(CONFIG_DRM_VBOXVIDEO) += vboxvideo/
 obj-$(CONFIG_DRM_LIMA)  += lima/
 obj-$(CONFIG_DRM_PANFROST) += panfrost/
 obj-$(CONFIG_DRM_PANTHOR) += panthor/
+obj-$(CONFIG_DRM_TYR) += tyr/
 obj-$(CONFIG_DRM_ASPEED_GFX) += aspeed/
 obj-$(CONFIG_DRM_MCDE) += mcde/
 obj-$(CONFIG_DRM_TIDSS) += tidss/
diff --git a/drivers/gpu/drm/tyr/Kconfig b/drivers/gpu/drm/tyr/Kconfig
new file mode 100644
index 0000000000..ce693d430b
--- /dev/null
+++ b/drivers/gpu/drm/tyr/Kconfig
@@ -0,0 +1,33 @@
+# SPDX-License-Identifier: GPL-2.0 or MIT
+
+# We should ideally fix this, except that we need these things to be Y (this is
+# a requirement from the Rust infrastructre) even if DRM_TYR=m.
+#
+config TYR_DRM_DEPS
+    bool "Tyr DRM dependencies"
+    select DRM_GPUVM
+    select DRM_GEM_SHMEM_HELPER
+	select DRM_SCHED
+    help
+      Tyr DRM deps (used to force deps to be =y even when the driver is =m)
+
+config DRM_TYR
+	tristate "Tyr (Rust DRM support for ARM Mali CSF-based GPUs)"
+	depends on DRM=y
+	depends on RUST
+	depends on RUST_FW_LOADER_ABSTRACTIONS
+	depends on ARM || ARM64 || COMPILE_TEST
+	depends on !GENERIC_ATOMIC64  # for IOMMU_IO_PGTABLE_LPAE
+	depends on MMU
+	depends on TYR_DRM_DEPS
+	select IOMMU_IO_PGTABLE_LPAE
+	depends on IOMMU_SUPPORT
+	help
+	  Rust DRM driver for ARM Mali CSF-based GPUs.
+
+	  This driver is for Mali (or Immortalis) Valhall Gxxx GPUs.
+
+	  Note that the Mali-G68 and Mali-G78, while Valhall architecture, will
+	  be supported with the panfrost driver as they are not CSF GPUs.
+
+	  if M is selected, the module will be called tyr.
diff --git a/drivers/gpu/drm/tyr/Makefile b/drivers/gpu/drm/tyr/Makefile
new file mode 100644
index 0000000000..ba545f65f2
--- /dev/null
+++ b/drivers/gpu/drm/tyr/Makefile
@@ -0,0 +1,3 @@
+# SPDX-License-Identifier: GPL-2.0 or MIT
+
+obj-$(CONFIG_DRM_TYR) += tyr.o
diff --git a/drivers/gpu/drm/tyr/driver.rs b/drivers/gpu/drm/tyr/driver.rs
new file mode 100644
index 0000000000..0404e2ec49
--- /dev/null
+++ b/drivers/gpu/drm/tyr/driver.rs
@@ -0,0 +1,459 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use core::pin::Pin;
+use kernel::device;
+use pin_init::pin_init_from_closure;
+
+use kernel::bits::bit_u32;
+use kernel::c_str;
+use kernel::clk::Clk;
+use kernel::device::Core;
+use kernel::devres::Devres;
+use kernel::drm;
+use kernel::drm::ioctl;
+use kernel::io;
+use kernel::io::mem::IoMem;
+use kernel::irq::request::IrqReturn;
+use kernel::irq::request::ThreadedIrqReturn;
+use kernel::irq::ThreadedHandler;
+use kernel::irq::ThreadedRegistration;
+use kernel::new_delayed_work;
+use kernel::new_mutex;
+use kernel::new_work;
+use kernel::of;
+use kernel::platform;
+use kernel::prelude::*;
+use kernel::regulator;
+use kernel::regulator::Regulator;
+use kernel::sync::Arc;
+use kernel::sync::Mutex;
+use kernel::time;
+use kernel::types::ARef;
+use kernel::workqueue::DelayedWork;
+use kernel::workqueue::OwnedQueue;
+use kernel::workqueue::Work;
+use kernel::workqueue::WqFlags;
+
+use crate::file::File;
+use crate::fw;
+use crate::fw::irq::JobIrq;
+use crate::fw::Firmware;
+use crate::gpu;
+use crate::gpu::irq::GpuIrq;
+use crate::gpu::GpuInfo;
+use crate::mmu;
+use crate::mmu::irq::MmuIrq;
+use crate::mmu::Mmu;
+use crate::regs::*;
+use crate::sched::Scheduler;
+use crate::sched::SchedulerState;
+use crate::wait::Wait;
+use crate::wait::WaitResult;
+
+/// Convienence type alias for the DRM device type for this driver
+pub(crate) type TyrDevice = drm::device::Device<TyrDriver>;
+
+#[pin_data(PinnedDrop)]
+pub(crate) struct TyrDriver {
+    device: ARef<TyrDevice>,
+
+    #[pin]
+    gpu_irq: ThreadedRegistration<TyrIrq<GpuIrq>>,
+
+    #[pin]
+    mmu_irq: ThreadedRegistration<TyrIrq<MmuIrq>>,
+
+    #[pin]
+    job_irq: ThreadedRegistration<TyrIrq<JobIrq>>,
+}
+
+#[pin_data]
+pub(crate) struct TyrData {
+    pub(crate) pdev: ARef<platform::Device>,
+
+    #[pin]
+    clks: Mutex<Clocks>,
+
+    #[pin]
+    regulators: Mutex<Regulators>,
+
+    // Some inforation on the GPU. This is mainly queried by userspace (mesa).
+    pub(crate) gpu_info: GpuInfo,
+
+    /// The firmware running on the MCU.
+    #[pin]
+    pub(crate) fw: Firmware,
+
+    /// True if the CPU/GPU are memory coherent.
+    pub(crate) coherent: bool,
+
+    /// MMU management.
+    mmu: Pin<KBox<Mutex<Mmu>>>,
+
+    /// The MMIO region.
+    pub(crate) iomem: Arc<Devres<IoMem>>,
+
+    /// The firmware ping work.
+    #[pin]
+    pub(crate) ping_work: DelayedWork<Self, 0>,
+
+    /// The scheduler logic.
+    #[pin]
+    sched: Mutex<SchedulerState>,
+
+    #[pin]
+    pub(crate) tick_work: Work<Self, 1>,
+
+    #[pin]
+    pub(crate) fw_events_work: Work<Self, 2>,
+
+    /// The work to process group status updates.
+    #[pin]
+    pub(crate) group_upd_work: Work<Self, 3>,
+
+    pub(crate) reset_wq: OwnedQueue,
+}
+
+impl TyrData {
+    /// Execute a function with the scheduler locked.
+    ///
+    /// This is implemented as a closure to reduce the scope of the scheduler
+    /// lock.
+    pub(crate) fn with_locked_scheduler<F, R>(&self, f: F) -> Result<R>
+    where
+        F: FnOnce(&mut Scheduler) -> Result<R>,
+    {
+        let mut sched = self.sched.lock();
+        f(sched.enabled_mut()?)
+    }
+
+    /// Execute a function with the mmu locked.
+    ///
+    /// This is implemented as a closure to reduce the scope of the mmu
+    /// lock.
+    pub(crate) fn with_locked_mmu<F, R>(&self, f: F) -> Result<R>
+    where
+        F: FnOnce(&mut Mmu) -> Result<R>,
+    {
+        let mut mmu = self.mmu.lock();
+        f(&mut mmu)
+    }
+}
+
+unsafe impl Send for TyrData {}
+unsafe impl Sync for TyrData {}
+
+fn issue_soft_reset(iomem: &Devres<IoMem<0>>) -> Result<()> {
+    let irq_enable_cmd = 1 | bit_u32(8);
+    GPU_CMD.write(iomem, irq_enable_cmd)?;
+
+    let op = || GPU_INT_RAWSTAT.read(iomem);
+    let cond = |raw_stat: &u32| -> bool { (*raw_stat >> 8) & 1 == 1 };
+    let res = io::poll::read_poll_timeout(
+        op,
+        cond,
+        time::Delta::from_millis(100),
+        Some(time::Delta::from_micros(20000)),
+    );
+
+    if let Err(e) = res {
+        pr_err!("GPU reset failed with errno {}\n", e.to_errno());
+        pr_err!("GPU_INT_RAWSTAT is {}\n", GPU_INT_RAWSTAT.read(iomem)?);
+    }
+
+    Ok(())
+}
+
+kernel::of_device_table!(
+    OF_TABLE,
+    MODULE_OF_TABLE,
+    <TyrDriver as platform::Driver>::IdInfo,
+    [
+        (of::DeviceId::new(c_str!("rockchip,rk3588-mali")), ()),
+        (of::DeviceId::new(c_str!("arm,mali-valhall-csf")), ())
+    ]
+);
+
+impl platform::Driver for TyrDriver {
+    type IdInfo = ();
+    const OF_ID_TABLE: Option<of::IdTable<Self::IdInfo>> = Some(&OF_TABLE);
+
+    fn probe(
+        pdev: &platform::Device<Core>,
+        _info: Option<&Self::IdInfo>,
+    ) -> Result<Pin<KBox<Self>>> {
+        dev_dbg!(pdev.as_ref(), "Probed Tyr\n");
+
+        let core_clk = Clk::get(pdev.as_ref(), Some(c_str!("core")))?;
+        let stacks_clk = Clk::get(pdev.as_ref(), Some(c_str!("stacks")))?;
+        let coregroup_clk = Clk::get(pdev.as_ref(), Some(c_str!("coregroup")))?;
+
+        core_clk.prepare_enable()?;
+        stacks_clk.prepare_enable()?;
+        coregroup_clk.prepare_enable()?;
+
+        let mali_regulator = Regulator::<regulator::Enabled>::get(pdev.as_ref(), c_str!("mali"))?;
+        let sram_regulator = Regulator::<regulator::Enabled>::get(pdev.as_ref(), c_str!("sram"))?;
+
+        let resource = pdev.resource_by_index(0).ok_or(EINVAL)?;
+
+        let iomem = Arc::new(pdev.iomap_resource(resource)?, GFP_KERNEL)?;
+
+        issue_soft_reset(&iomem)?;
+        gpu::l2_power_on(&iomem)?;
+
+        let gpu_info = GpuInfo::new(&iomem)?;
+        gpu_info.log(pdev);
+
+        pdev.as_ref().dma_set_max_seg_size(u32::MAX);
+        pdev.as_ref()
+            .dma_set_mask_and_coherent(u64::from(gpu_info.pa_bits()))?;
+
+        let platform: ARef<platform::Device> = pdev.into();
+        //TODO: This is very temporary
+        // SAFETY: This should be safe as data is not touched by the driver
+        // untill it gets fully initialised.
+        // Additionally implementation of Drop trait is still pending
+        // so no data will be accessed util proper init.
+        let uninit = unsafe {
+            pin_init_from_closure::<TyrData, kernel::error::Error>(|_slot| Ok(core::mem::zeroed()))
+        };
+        let data = Arc::pin_init(uninit, GFP_KERNEL)?;
+
+        let tdev: ARef<TyrDevice> = drm::device::Device::new(pdev.as_ref(), data.clone())?;
+
+        let mmu = KBox::pin_init(new_mutex!(Mmu::new()?), GFP_KERNEL)?;
+
+        let fw_event_wait = Wait::new()?;
+        let fw_boot_wait = Wait::new()?;
+        let fw = Firmware::init(
+            &tdev,
+            pdev,
+            &gpu_info,
+            mmu.as_ref(),
+            iomem.clone(),
+            fw_event_wait.clone(),
+        )?;
+
+        // Ideally we'd find a way around this useless clone too...
+        let i = iomem.clone();
+        let data_init = try_pin_init!(TyrData {
+                pdev: platform.clone(),
+                clks <- new_mutex!(Clocks {
+                    core: core_clk,
+                    stacks: stacks_clk,
+                    coregroup: coregroup_clk,
+                }),
+                regulators <- new_mutex!(Regulators {
+                    mali: mali_regulator,
+                    sram: sram_regulator,
+                }),
+                gpu_info,
+                fw <- fw,
+                coherent: false, // TODO. The GPU is not IO coherent on rk3588, which is what I am testing on.
+                mmu,
+                iomem: i,
+                ping_work <- new_delayed_work!("tyr-ping-work"),
+                sched <- new_mutex!(SchedulerState::Disabled),
+                tick_work <- new_work!("tyr_tick"),
+                fw_events_work <- new_work!("tyr-fw-events"),
+                group_upd_work <- new_work!("tyr-group-upd"),
+                reset_wq: OwnedQueue::new(c_str!("tyr-reset"), WqFlags::UNBOUND, 0)? // TODO: add WqFlags::ORDERED once it's available.
+        });
+
+        unsafe {
+            data_init.__pinned_init(Arc::<TyrData>::as_ptr(&data) as *mut TyrData)?;
+        }
+
+        // We must find a way around this. It's being discussed on Zulip already.
+        //
+        // Note that this is a problem, because if we fail at probe, then the
+        // drop code expects the data to be set, which leads to a crash.
+        drm::driver::Registration::new_foreign_owned(&tdev, pdev.as_ref(), 0)?;
+
+        let poweron_wait = Wait::new_with_data(false)?;
+        let pow = poweron_wait.clone();
+
+        let t = tdev.clone();
+        let i = iomem.clone();
+        let fwe = fw_event_wait.clone();
+        let fbbw = fw_boot_wait.clone();
+        let driver = KBox::pin_init(
+            try_pin_init!(TyrDriver {
+                device: t.clone(),
+                gpu_irq <- gpu::irq::gpu_irq_init(t.clone(), pdev, i.clone(), pow)?,
+                mmu_irq <- mmu::irq::mmu_irq_init(t.clone(), pdev, i.clone())?,
+                job_irq <- fw::irq::job_irq_init(t.clone(), pdev, i.clone(), fwe.clone(), fbbw.clone())?,
+            }),
+            GFP_KERNEL,
+        )?;
+
+        poweron_wait.wait_interruptible_timeout(100, |powered_on| {
+            if *powered_on {
+                Ok(WaitResult::Ok)
+            } else {
+                Ok(WaitResult::Retry)
+            }
+        })?;
+
+        MCU_CONTROL.write(&iomem, MCU_CONTROL_AUTO)?;
+
+        let gpu_info = &tdev.gpu_info;
+        let core_clk = &tdev.clks.lock().core;
+
+        fw_boot_wait.clone().wait_interruptible_timeout(100, |()| {
+            data.fw.with_locked_global_iface(|glb| {
+                if glb.booted() {
+                    glb.enable(&tdev, gpu_info, core_clk)?;
+                    Ok(WaitResult::Ok)
+                } else {
+                    Ok(WaitResult::Retry)
+                }
+            })
+        })?;
+
+        data.sched.lock().init(&tdev.clone())?;
+        dev_info!(pdev.as_ref(), "Tyr initialized correctly.\n");
+        Ok(driver)
+    }
+}
+
+#[pinned_drop]
+impl PinnedDrop for TyrDriver {
+    fn drop(self: Pin<&mut Self>) {
+        // XXX: we will not have the `data` field here if we failed the
+        // initialization, i.e.: if probe failed.
+        //
+        // We need to figure out with the community how to properly split the
+        // creation of a DRM device from the place where the data is set and
+        // from the place where it is registered to overcome this.
+        //
+        // The current solution, i.e.: `new_from_closure` is just a hack, and it
+        // shows its shortcomings here, for example.
+        //
+        // dev_dbg!(self.device.data().pdev.as_ref(), "Removed Tyr.\n");
+    }
+}
+
+const INFO: drm::driver::DriverInfo = drm::driver::DriverInfo {
+    major: 0,
+    minor: 0,
+    patchlevel: 0,
+    name: c_str!("tyr"),
+    desc: c_str!("ARM Mali CSF-based GPU driver"),
+};
+
+#[vtable]
+impl drm::driver::Driver for TyrDriver {
+    type Data = Arc<TyrData>;
+    type File = File;
+    type Object = crate::gem::DriverObject;
+
+    const INFO: drm::driver::DriverInfo = INFO;
+
+    // TODO: missing feature SYNC_OBJ
+
+    kernel::declare_drm_ioctls! {
+        (PANTHOR_DEV_QUERY, drm_panthor_dev_query, ioctl::RENDER_ALLOW, File::dev_query),
+        (PANTHOR_VM_CREATE, drm_panthor_vm_create, ioctl::RENDER_ALLOW, File::vm_create),
+        (PANTHOR_VM_DESTROY, drm_panthor_vm_destroy, ioctl::RENDER_ALLOW, File::vm_destroy),
+        (PANTHOR_VM_BIND, drm_panthor_vm_bind, ioctl::RENDER_ALLOW, File::vm_bind),
+        (PANTHOR_VM_GET_STATE, drm_panthor_vm_get_state, ioctl::RENDER_ALLOW, File::vm_get_state),
+        (PANTHOR_BO_CREATE, drm_panthor_bo_create, ioctl::RENDER_ALLOW, File::bo_create),
+        (PANTHOR_BO_MMAP_OFFSET, drm_panthor_bo_mmap_offset, ioctl::RENDER_ALLOW, File::bo_mmap_offset),
+        (PANTHOR_GROUP_CREATE, drm_panthor_group_create, ioctl::RENDER_ALLOW, File::group_create),
+        (PANTHOR_GROUP_DESTROY, drm_panthor_group_destroy, ioctl::RENDER_ALLOW, File::group_destroy),
+        (PANTHOR_GROUP_SUBMIT, drm_panthor_group_submit, ioctl::RENDER_ALLOW, File::group_submit),
+    }
+}
+
+#[pin_data]
+struct Clocks {
+    core: Clk,
+    stacks: Clk,
+    coregroup: Clk,
+}
+
+#[pin_data]
+struct Regulators {
+    mali: Regulator<regulator::Enabled>,
+    sram: Regulator<regulator::Enabled>,
+}
+
+pub(crate) trait TyrIrqTrait: Sync {
+    /// Reads the interrupt status register.
+    fn read_status(&self) -> u32;
+
+    /// Disable all device interrupts for the interrupt line.
+    ///
+    /// Needed so we can disable the top part while the threaded handler runs.
+    fn disable_all(&self);
+
+    /// Reenable the interrupts after the threaded handler has run.
+    fn reenable(&self);
+
+    /// Reads the raw interrupt status register.
+    fn read_raw_status(&self) -> u32;
+
+    /// Clears the interrupt status.
+    fn clear_status(&self, status: u32);
+
+    /// Returns the mask for the enabled interrupts.
+    fn mask(&self) -> u32;
+
+    /// Handles the interrupt in the threaded context.
+    fn handle(&self, tdev: &TyrDevice, status: u32);
+}
+
+pub(crate) struct TyrIrq<T: TyrIrqTrait> {
+    tdev: ARef<TyrDevice>,
+    irq: T,
+}
+
+impl<T: TyrIrqTrait + 'static> TyrIrq<T> {
+    pub(crate) fn request<'a>(
+        pdev: &'a platform::Device<device::Bound>,
+        tdev: ARef<TyrDevice>,
+        name: &'static CStr,
+        irq_type: T,
+    ) -> Result<impl PinInit<ThreadedRegistration<Self>, Error> + 'a> {
+        let handler = Self {
+            tdev,
+            irq: irq_type,
+        };
+
+        pdev.threaded_irq_by_name(name, kernel::irq::flags::SHARED, handler)
+    }
+}
+
+impl<T: TyrIrqTrait> ThreadedHandler for TyrIrq<T> {
+    fn handle_irq(&self) -> kernel::irq::request::ThreadedIrqReturn {
+        let int_stat = self.irq.read_status();
+
+        if int_stat == 0 {
+            return ThreadedIrqReturn::None;
+        }
+
+        self.irq.disable_all();
+        ThreadedIrqReturn::WakeThread
+    }
+
+    fn thread_fn(&self) -> kernel::irq::request::IrqReturn {
+        let mut ret = IrqReturn::None;
+
+        loop {
+            let int_stat = self.irq.read_raw_status() & self.irq.mask();
+
+            if int_stat == 0 {
+                break;
+            }
+
+            self.irq.clear_status(int_stat);
+            self.irq.handle(&self.tdev, int_stat);
+            ret = IrqReturn::Handled;
+        }
+
+        self.irq.reenable();
+        ret
+    }
+}
diff --git a/drivers/gpu/drm/tyr/file.rs b/drivers/gpu/drm/tyr/file.rs
new file mode 100644
index 0000000000..59f49958c5
--- /dev/null
+++ b/drivers/gpu/drm/tyr/file.rs
@@ -0,0 +1,412 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use core::ops::Deref;
+use core::ops::DerefMut;
+
+use kernel::alloc::flags::*;
+use kernel::drm;
+use kernel::drm::device::Device as DrmDevice;
+use kernel::drm::gem::BaseObject;
+use kernel::kvec;
+use kernel::prelude::*;
+use kernel::transmute::FromBytes;
+use kernel::types::ARef;
+use kernel::uaccess::UserSlice;
+use kernel::uapi;
+
+use crate::driver::TyrDevice;
+use crate::driver::TyrDriver;
+use crate::gem;
+use crate::mmu::vm;
+use crate::mmu::vm::pool::Pool;
+use crate::mmu::vm::VmLayout;
+use crate::mmu::vm::VmUserSize;
+use crate::sched::group;
+
+#[pin_data]
+pub(crate) struct File {
+    /// A pool storing our VMs for this particular context.
+    #[pin]
+    vm_pool: Pool,
+
+    group_pool: group::Pool,
+}
+
+/// Convenience type alias for our DRM `File` type
+pub(crate) type DrmFile = drm::file::File<File>;
+
+impl drm::file::DriverFile for File {
+    type Driver = TyrDriver;
+
+    fn open(dev: &DrmDevice<Self::Driver>) -> Result<Pin<KBox<Self>>> {
+        dev_dbg!(dev.as_ref(), "drm::device::Device::open\n");
+
+        KBox::try_pin_init(
+            try_pin_init!(Self {
+                vm_pool: Pool::create()?,
+                group_pool: group::Pool::create()?,
+            }),
+            GFP_KERNEL,
+        )
+    }
+}
+
+impl File {
+    pub(crate) fn dev_query(
+        tdev: &TyrDevice,
+        devquery: &mut uapi::drm_panthor_dev_query,
+        _file: &DrmFile,
+    ) -> Result<u32> {
+        if devquery.pointer == 0 {
+            match devquery.type_ {
+                uapi::drm_panthor_dev_query_type_DRM_PANTHOR_DEV_QUERY_GPU_INFO => {
+                    devquery.size = core::mem::size_of_val(&tdev.gpu_info) as u32;
+                    Ok(0)
+                }
+                _ => Err(EINVAL),
+            }
+        } else {
+            match devquery.type_ {
+                uapi::drm_panthor_dev_query_type_DRM_PANTHOR_DEV_QUERY_GPU_INFO => {
+                    let mut writer =
+                        UserSlice::new(devquery.pointer as usize, devquery.size as usize).writer();
+
+                    writer.write(&tdev.gpu_info)?;
+
+                    Ok(0)
+                }
+                _ => Err(EINVAL),
+            }
+        }
+    }
+
+    pub(crate) fn vm_create(
+        tdev: &TyrDevice,
+        vmcreate: &mut uapi::drm_panthor_vm_create,
+        file: &DrmFile,
+    ) -> Result<u32> {
+        let id = file.inner().vm_pool().create_vm(
+            &ARef::from(tdev),
+            VmLayout::from_user_sz(tdev, VmUserSize::Custom(vmcreate.user_va_range)),
+        )?;
+
+        vmcreate.id = id as u32;
+        Ok(0)
+    }
+
+    pub(crate) fn vm_destroy(
+        tdev: &TyrDevice,
+        vmdestroy: &mut uapi::drm_panthor_vm_destroy,
+        file: &DrmFile,
+    ) -> Result<u32> {
+        let iomem = tdev.iomem.clone();
+
+        file.inner()
+            .vm_pool()
+            .destroy_vm(vmdestroy.id as usize, iomem)?;
+        Ok(0)
+    }
+
+    pub(crate) fn vm_bind(
+        tdev: &TyrDevice,
+        vmbind: &mut uapi::drm_panthor_vm_bind,
+        file: &DrmFile,
+    ) -> Result<u32> {
+        if vmbind.flags & uapi::drm_panthor_vm_bind_flags_DRM_PANTHOR_VM_BIND_ASYNC != 0 {
+            dev_info!(tdev.as_ref(), "We do not support async VM_BIND yet");
+            return Err(ENOTSUPP);
+        }
+
+        if vmbind.ops.stride as usize != core::mem::size_of::<uapi::drm_panthor_vm_bind_op>() {
+            dev_info!(
+                tdev.as_ref(),
+                "We cannot graciously handle stride mismatches yet"
+            );
+            return Err(ENOTSUPP);
+        }
+
+        let stride = vmbind.ops.stride as usize;
+        let count = vmbind.ops.count as usize;
+
+        let mut reader = UserSlice::new(vmbind.ops.array as usize, stride).reader();
+        let iomem = tdev.iomem.clone();
+
+        for i in 0..count {
+            let res = {
+                let op: VmBindOp = reader.read()?;
+                let mask = uapi::drm_panthor_vm_bind_op_flags_DRM_PANTHOR_VM_BIND_OP_TYPE_MASK;
+
+                match op.0.flags as i32 & mask {
+                    uapi::drm_panthor_vm_bind_op_flags_DRM_PANTHOR_VM_BIND_OP_TYPE_MAP => {
+                        let bo = gem::lookup_handle(file, op.0.bo_handle)?;
+                        let range = op.0.va..op.0.va + op.0.size;
+
+                        let vm = file
+                            .inner()
+                            .vm_pool()
+                            .get_vm(vmbind.vm_id as usize)
+                            .ok_or(EINVAL)?;
+
+                        vm.lock().bind_gem(
+                            iomem.clone(),
+                            &bo.gem,
+                            op.0.bo_offset,
+                            range,
+                            vm::map_flags::Flags::try_from(op.0.flags & 0b111)?,
+                        )?;
+                    }
+
+                    uapi::drm_panthor_vm_bind_op_flags_DRM_PANTHOR_VM_BIND_OP_TYPE_UNMAP => {
+                        if op.0.bo_handle != 0 || op.0.bo_offset != 0 {
+                            return Err(EINVAL);
+                        }
+
+                        let range = op.0.va..op.0.va + op.0.size;
+
+                        let vm = file
+                            .inner()
+                            .vm_pool()
+                            .get_vm(vmbind.vm_id as usize)
+                            .ok_or(EINVAL)?;
+
+                        vm.lock().unmap_range(iomem.clone(), range)?;
+                    }
+                    _ => return Err(ENOTSUPP),
+                }
+
+                Ok(0)
+            };
+
+            if let Err(e) = res {
+                vmbind.ops.count = i as u32;
+                return Err(e);
+            }
+        }
+
+        Ok(0)
+    }
+
+    pub(crate) fn vm_get_state(
+        _tdev: &TyrDevice,
+        _vmgetstate: &mut uapi::drm_panthor_vm_get_state,
+        _file: &DrmFile,
+    ) -> Result<u32> {
+        Err(ENOTSUPP)
+    }
+
+    pub(crate) fn bo_create(
+        tdev: &TyrDevice,
+        bocreate: &mut uapi::drm_panthor_bo_create,
+        file: &DrmFile,
+    ) -> Result<u32> {
+        if bocreate.flags & !uapi::drm_panthor_bo_flags_DRM_PANTHOR_BO_NO_MMAP != 0 {
+            dev_err!(
+                tdev.as_ref(),
+                "bo_create: invalid flags {}\n",
+                bocreate.flags
+            );
+
+            return Err(EINVAL);
+        }
+
+        let bo = gem::new_object(tdev, bocreate.size as usize, bocreate.flags)?;
+
+        let handle = bo.gem.create_handle(file)?;
+        bocreate.handle = handle;
+        bocreate.size = bo.gem.size() as u64;
+
+        Ok(0)
+    }
+
+    pub(crate) fn bo_mmap_offset(
+        _tdev: &TyrDevice,
+        bommap: &mut uapi::drm_panthor_bo_mmap_offset,
+        file: &DrmFile,
+    ) -> Result<u32> {
+        let bo = gem::lookup_handle(file, bommap.handle)?;
+
+        bommap.offset = bo.gem.create_mmap_offset()?;
+
+        Ok(0)
+    }
+
+    pub(crate) fn vm_pool(self: Pin<&Self>) -> Pin<&Pool> {
+        // SAFETY: Field projection, we never move out of this field.
+        unsafe { self.map_unchecked(|f| &f.vm_pool) }
+    }
+
+    pub(crate) fn group_pool(self: Pin<&Self>) -> Pin<&group::Pool> {
+        // SAFETY: Field projection, we never move out of this field.
+        unsafe { self.map_unchecked(|f| &f.group_pool) }
+    }
+
+    pub(crate) fn group_create(
+        tdev: &TyrDevice,
+        groupcreate: &mut uapi::drm_panthor_group_create,
+        file: &DrmFile,
+    ) -> Result<u32> {
+        if groupcreate.queues.count == 0 {
+            return Err(EINVAL);
+        }
+
+        let mut reader = UserSlice::new(
+            groupcreate.queues.array as usize,
+            groupcreate.queues.stride as usize,
+        )
+        .reader();
+
+        let mut queue_args = kvec![];
+        for _ in 0..groupcreate.queues.count {
+            let queue: QueueCreate = reader.read()?;
+            queue_args.push(queue, GFP_KERNEL)?;
+        }
+
+        let handle = file
+            .inner()
+            .group_pool()
+            .create_group(tdev, groupcreate, file, queue_args)?;
+
+        groupcreate.group_handle = handle as u32;
+
+        Ok(0)
+    }
+
+    pub(crate) fn group_destroy(
+        _tdev: &TyrDevice,
+        groupdestroy: &mut uapi::drm_panthor_group_destroy,
+        file: &DrmFile,
+    ) -> Result<u32> {
+        file.inner()
+            .group_pool()
+            .destroy_group(groupdestroy.group_handle as usize)?;
+
+        Ok(0)
+    }
+
+    pub(crate) fn group_submit(
+        tdev: &TyrDevice,
+        groupsubmit: &mut uapi::drm_panthor_group_submit,
+        file: &DrmFile,
+    ) -> Result<u32> {
+        if groupsubmit.queue_submits.count == 0 {
+            return Err(EINVAL);
+        }
+
+        if groupsubmit.queue_submits.count > 1 {
+            pr_err!("We do not support multiple queue submits yet");
+            return Err(ENOTSUPP);
+        }
+
+        let mut reader = UserSlice::new(
+            groupsubmit.queue_submits.array as usize,
+            groupsubmit.queue_submits.stride as usize,
+        )
+        .reader();
+
+        let mut queue_submits = kvec![];
+        let mut syncs = kvec![];
+        for _ in 0..groupsubmit.queue_submits.count {
+            let queue: QueueSubmit = reader.read()?;
+
+            let mut sync_reader =
+                UserSlice::new(queue.syncs.array as usize, queue.syncs.stride as usize).reader();
+
+            for _ in 0..queue.syncs.count {
+                let sync: SyncOp = sync_reader.read()?;
+                if sync.flags & !uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_SIGNAL as u32
+                    != 0
+                {
+                    pr_err!("We only support DRM_PANTHOR_SYNC_OP_SIGNAL for now");
+                    return Err(ENOTSUPP);
+                }
+
+                syncs.push(sync, GFP_KERNEL)?;
+            }
+
+            queue_submits.push(queue, GFP_KERNEL)?;
+        }
+
+        let mut out_syncs = kvec![];
+        for sync in syncs.iter().filter(|sync| {
+            sync.flags & uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_SIGNAL as u32 != 0
+        }) {
+            out_syncs.push(
+                drm::syncobj::SyncObj::lookup_handle(file, sync.handle)?,
+                GFP_KERNEL,
+            )?;
+        }
+
+        let group = file
+            .inner()
+            .group_pool()
+            .group(groupsubmit.group_handle as usize)
+            .ok_or(EINVAL)?;
+
+        tdev.with_locked_scheduler(|sched| {
+            sched.bind0(tdev, group.clone())?;
+            sched.submit(kvec![], out_syncs, group, queue_submits)
+        })?;
+
+        Ok(0)
+    }
+}
+
+#[repr(transparent)]
+struct VmBindOp(uapi::drm_panthor_vm_bind_op);
+
+// XXX: we cannot implement this trait for the uapi type directly, hence the
+// wrapper.
+// SAFETY: this struct is safe to be transmuted from a byte slice.
+unsafe impl FromBytes for VmBindOp {}
+
+#[repr(transparent)]
+pub(crate) struct QueueCreate(pub uapi::drm_panthor_queue_create);
+
+// XXX: we cannot implement this trait for the uapi type directly, hence the
+// wrapper.
+// SAFETY: this struct is safe to be transmuted from a byte slice.
+unsafe impl FromBytes for QueueCreate {}
+
+#[repr(transparent)]
+pub(crate) struct QueueSubmit(pub uapi::drm_panthor_queue_submit);
+
+// XXX: we cannot implement this trait for the uapi type directly, hence the
+// wrapper.
+// SAFETY: this struct is safe to be transmuted from a byte slice.
+unsafe impl FromBytes for QueueSubmit {}
+
+impl Deref for QueueSubmit {
+    type Target = uapi::drm_panthor_queue_submit;
+
+    fn deref(&self) -> &Self::Target {
+        &self.0
+    }
+}
+
+impl DerefMut for QueueSubmit {
+    fn deref_mut(&mut self) -> &mut Self::Target {
+        &mut self.0
+    }
+}
+
+#[repr(transparent)]
+pub(crate) struct SyncOp(pub uapi::drm_panthor_sync_op);
+
+// XXX: we cannot implement this trait for the uapi type directly, hence the
+// wrapper.
+// SAFETY: this struct is safe to be transmuted from a byte slice.
+unsafe impl FromBytes for SyncOp {}
+
+impl Deref for SyncOp {
+    type Target = uapi::drm_panthor_sync_op;
+
+    fn deref(&self) -> &Self::Target {
+        &self.0
+    }
+}
+
+impl DerefMut for SyncOp {
+    fn deref_mut(&mut self) -> &mut Self::Target {
+        &mut self.0
+    }
+}
diff --git a/drivers/gpu/drm/tyr/flags.rs b/drivers/gpu/drm/tyr/flags.rs
new file mode 100644
index 0000000000..387b3f4394
--- /dev/null
+++ b/drivers/gpu/drm/tyr/flags.rs
@@ -0,0 +1,137 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! A general flags type adapted from the WIP work from Felipe Xavier.
+//!
+//! This will be replaced by his patch once it's ready.
+
+#[macro_export]
+/// Creates a new flags type.
+macro_rules! impl_flags {
+    ($flags:ident, $flag:ident, $ty:ty) => {
+        #[allow(missing_docs)]
+        #[repr(transparent)]
+        #[derive(Copy, Clone, Default, Debug, PartialEq, Eq)]
+        pub struct $flags($ty);
+
+        #[allow(missing_docs)]
+        #[derive(Copy, Clone, Debug, PartialEq, Eq)]
+        pub struct $flag($ty);
+
+        impl From<$flag> for $flags {
+            #[inline]
+            fn from(value: $flag) -> Self {
+                Self(value.0)
+            }
+        }
+
+        impl From<$flags> for $ty {
+            #[inline]
+            fn from(value: $flags) -> Self {
+                value.0
+            }
+        }
+
+        impl core::ops::BitOr for $flags {
+            type Output = Self;
+
+            #[inline]
+            fn bitor(self, rhs: Self) -> Self::Output {
+                Self(self.0 | rhs.0)
+            }
+        }
+
+        impl core::ops::BitOrAssign for $flags {
+            #[inline]
+            fn bitor_assign(&mut self, rhs: Self) {
+                *self = *self | rhs;
+            }
+        }
+
+        impl core::ops::BitAnd for $flags {
+            type Output = Self;
+
+            #[inline]
+            fn bitand(self, rhs: Self) -> Self::Output {
+                Self(self.0 & rhs.0)
+            }
+        }
+
+        impl core::ops::BitAndAssign for $flags {
+            #[inline]
+            fn bitand_assign(&mut self, rhs: Self) {
+                *self = *self & rhs;
+            }
+        }
+
+        impl core::ops::BitOr<$flag> for $flags {
+            type Output = Self;
+
+            #[inline]
+            fn bitor(self, rhs: $flag) -> Self::Output {
+                self | Self::from(rhs)
+            }
+        }
+
+        impl core::ops::BitOrAssign<$flag> for $flags {
+            #[inline]
+            fn bitor_assign(&mut self, rhs: $flag) {
+                *self = *self | rhs;
+            }
+        }
+
+        impl core::ops::BitAnd<$flag> for $flags {
+            type Output = Self;
+
+            #[inline]
+            fn bitand(self, rhs: $flag) -> Self::Output {
+                self & Self::from(rhs)
+            }
+        }
+
+        impl core::ops::BitAndAssign<$flag> for $flags {
+            #[inline]
+            fn bitand_assign(&mut self, rhs: $flag) {
+                *self = *self & rhs;
+            }
+        }
+
+        impl core::ops::BitXor for $flags {
+            type Output = Self;
+
+            #[inline]
+            fn bitxor(self, rhs: Self) -> Self::Output {
+                Self(self.0 ^ rhs.0)
+            }
+        }
+
+        impl core::ops::BitXorAssign for $flags {
+            #[inline]
+            fn bitxor_assign(&mut self, rhs: Self) {
+                *self = *self ^ rhs;
+            }
+        }
+
+        impl core::ops::Neg for $flags {
+            type Output = Self;
+
+            #[inline]
+            fn neg(self) -> Self::Output {
+                Self(!self.0)
+            }
+        }
+
+        impl $flags {
+            /// Returns an empty instance of <type> where no flags are set.
+            #[inline]
+            pub const fn empty() -> Self {
+                Self(0)
+            }
+
+            /// Checks if a specific flag is set.
+            #[inline]
+            pub fn contains(self, flag: $flag) -> bool {
+                (self.0 & flag.0) == flag.0
+            }
+        }
+    };
+}
diff --git a/drivers/gpu/drm/tyr/fw.rs b/drivers/gpu/drm/tyr/fw.rs
new file mode 100644
index 0000000000..1d24713f11
--- /dev/null
+++ b/drivers/gpu/drm/tyr/fw.rs
@@ -0,0 +1,369 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use global::GlobalInterface;
+use kernel::bindings::SZ_1G;
+use kernel::devres::Devres;
+use kernel::io::mem::IoMem;
+use kernel::new_mutex;
+use kernel::platform;
+use kernel::prelude::*;
+use kernel::sizes::SZ_8K;
+use kernel::sync::Arc;
+use kernel::sync::Mutex;
+use parse::Section;
+
+use crate::driver::TyrDevice;
+use crate::gem;
+use crate::gem::KernelVaPlacement;
+use crate::gem::ObjectRef;
+use crate::gpu::GpuInfo;
+use crate::mmu::vm::map_flags;
+use crate::mmu::vm::Vm;
+use crate::mmu::vm::VmLayout;
+use crate::mmu::Mmu;
+use crate::wait::Wait;
+use crate::wait::WaitResult;
+
+const CSF_MCU_SHARED_REGION_START: u32 = 0x04000000;
+const CSF_MCU_SHARED_REGION_SIZE: u32 = 0x04000000;
+
+pub(crate) mod global;
+pub(crate) mod irq;
+mod parse;
+
+/// A range into the shared section that is known to be valid.
+///
+/// This can be obtained via a call to [`Firmware::to_kmap_range(mcu_va, size)`].
+///
+/// # Invariants
+///
+/// `self.start..self.end` is a valid range into the shared section. This means
+/// that it can safely be dereferenced by the CPU.
+///
+pub(crate) struct SharedSectionRange {
+    shared_section: Arc<Mutex<Section>>,
+    start: usize,
+    end: usize,
+}
+
+impl SharedSectionRange {
+    fn len(&self) -> usize {
+        self.end - self.start
+    }
+
+    fn as_mut_ptr(&self) -> Result<*mut core::ffi::c_void> {
+        let mut shared_section = self.shared_section.lock();
+        let vmap = shared_section.mem.vmap()?;
+        let vmap = vmap.as_mut_ptr();
+
+        // SAFETY: safe by the type invariant.
+        let offset = unsafe { vmap.add(self.start) };
+
+        Ok(offset)
+    }
+
+    fn read<T>(&self) -> Result<T> {
+        if core::mem::size_of::<T>() > self.len() {
+            return Err(EINVAL);
+        }
+
+        let ptr = self.as_mut_ptr()?;
+
+        // SAFETY: we know that this pointer is aligned and valid for reads for
+        // at least size_of::<Self>() bytes.
+        Ok(unsafe { core::ptr::read_volatile(ptr as *const T) })
+    }
+
+    fn write<T>(&self, value: T) -> Result {
+        if core::mem::size_of::<T>() > self.len() {
+            return Err(EINVAL);
+        }
+
+        let ptr = self.as_mut_ptr()?;
+
+        // SAFETY: we know that this pointer is aligned and valid for writes for
+        // at least size_of::<Self>() bytes.
+        unsafe {
+            core::ptr::write_volatile(ptr as *mut T, value);
+        }
+
+        Ok(())
+    }
+}
+
+/// An offset into the shared section that is known to point to the request field.
+///
+/// It is more convenient to use this type than reading or writing the memory
+/// areas directly since it implements the XOR logic to handle the communication
+/// of requests and acknowledgements.
+pub(crate) struct RequestField {
+    req: SharedSectionRange,
+    ack: SharedSectionRange,
+}
+
+impl RequestField {
+    fn new(shared_section: &SharedSectionRange, req_offset: usize, ack_offset: usize) -> Self {
+        let req = SharedSectionRange {
+            shared_section: shared_section.shared_section.clone(),
+            start: shared_section.start + req_offset,
+            end: shared_section.start + req_offset + core::mem::size_of::<u32>(),
+        };
+
+        let ack = SharedSectionRange {
+            shared_section: shared_section.shared_section.clone(),
+            start: shared_section.start + ack_offset,
+            end: shared_section.start + ack_offset + core::mem::size_of::<u32>(),
+        };
+
+        Self { req, ack }
+    }
+
+    /// Toggle acknowledge bits to send an event to the FW
+    ///
+    /// The Host -> FW event/message passing was designed to be lockless, with each side of
+    /// the channel having its writeable section. Events are signaled as a difference between
+    /// the host and FW side in the req/ack registers (when a bit differs, there's an event
+    /// pending, when they are the same, nothing needs attention).
+    ///
+    /// This helper allows one to update the req register based on the current value of the
+    /// ack register managed by the FW. Toggling a specific bit will flag an event. In order
+    /// for events to be re-evaluated, the interface doorbell needs to be rung.
+    pub(crate) fn toggle_reqs(&self, reqs: u32) -> Result {
+        let cur_req_val = self.req.read::<u32>()?;
+        let ack_val = self.ack.read::<u32>()?;
+        let new_val = ((ack_val ^ reqs) & reqs) | (cur_req_val & !reqs);
+
+        self.req.write::<u32>(new_val)
+    }
+
+    /// Update bits to reflect a configuration change.
+    ///
+    /// Not all bits work in a toggle fashion. Some bits are used to configure the FW
+    /// and need to be set to 0 or 1. This function bypasses the toggle logic and
+    /// directly sets the bits in the req register.
+    pub(crate) fn update_reqs(&self, val: u32, reqs: u32) -> Result {
+        let cur_req_val = self.req.read::<u32>()?;
+        let new_val = (cur_req_val & !reqs) | (val & reqs);
+
+        self.req.write::<u32>(new_val)
+    }
+
+    /// Returns whether any requests are pending for `reqs`.
+    ///
+    /// Requests are pending when the value of the given bit in the req differs
+    /// from the one in ack.
+    pub(crate) fn pending_reqs(&self, reqs: u32) -> Result<bool> {
+        let cur_req_val = self.req.read::<u32>()? & reqs;
+        let cur_ack_val = self.ack.read::<u32>()? & reqs;
+
+        Ok((cur_req_val ^ cur_ack_val) != 0)
+    }
+
+    /// Waits for the given requests to be acknowledged.
+    ///
+    /// This will sleep for at most `timeout_ms` milliseconds.
+    pub(crate) fn wait_acks(&self, reqs: u32, events_wait: &Wait, timeout_ms: u32) -> Result {
+        events_wait.wait_interruptible_timeout(timeout_ms, |()| {
+            if !self.pending_reqs(reqs)? {
+                Ok(WaitResult::Ok)
+            } else {
+                Ok(WaitResult::Retry)
+            }
+        })
+    }
+}
+
+/// Our interface to the MCU.
+#[pin_data]
+pub(crate) struct Firmware {
+    #[pin]
+    /// The sections read from the firmware binary. These sections are loaded
+    /// into GPU memory via BOs.
+    sections: Mutex<KVec<Section>>,
+
+    /// The global FW interface.
+    #[pin]
+    global_iface: Mutex<GlobalInterface>,
+
+    /// The VM where we load the firmware into. This VM is always bound to AS0.
+    vm: Arc<Mutex<Vm>>,
+
+    /// A condvar representing a wait on a firmware event.
+    ///
+    /// We notify all waiters on every interrupt.
+    #[pin]
+    event_wait: Arc<Wait>,
+}
+
+impl Firmware {
+    pub(crate) fn init(
+        tdev: &TyrDevice,
+        pdev: &platform::Device,
+        gpu_info: &GpuInfo,
+        mmu: Pin<&Mutex<Mmu>>,
+        iomem: Arc<Devres<IoMem>>,
+        event_wait: Arc<Wait>,
+    ) -> Result<impl PinInit<Self>> {
+        let vm = {
+            let auto_kernel_va = CSF_MCU_SHARED_REGION_START as u64
+                ..CSF_MCU_SHARED_REGION_START as u64 + CSF_MCU_SHARED_REGION_SIZE as u64;
+
+            let mut mmu = mmu.lock();
+
+            // Create the FW VM. This will be used to communicate between the CPU
+            // and the MCU.
+            let vm = mmu.create_vm(
+                tdev,
+                pdev,
+                gpu_info,
+                true,
+                VmLayout {
+                    user: 0..0,
+                    kernel: 0..4 * SZ_1G as u64,
+                },
+                auto_kernel_va,
+            )?;
+
+            mmu.bind_vm(vm.clone(), gpu_info, &iomem)?;
+
+            vm
+        };
+
+        let (sections, shared_section) =
+            Self::read_sections(tdev, iomem.clone(), gpu_info, vm.clone())?;
+
+        let global_iface = GlobalInterface::new(shared_section, iomem.clone(), event_wait.clone())?;
+
+        Ok(pin_init!(Self {
+            sections <- new_mutex!(sections),
+            global_iface <- new_mutex!(global_iface),
+            vm,
+            event_wait,
+        }))
+    }
+
+    pub(crate) fn alloc_queue_mem(&self, tdev: &TyrDevice) -> Result<ObjectRef> {
+        let flags =
+            map_flags::Flags::from(map_flags::NOEXEC) | map_flags::Flags::from(map_flags::UNCACHED);
+        let va = KernelVaPlacement::Auto { size: SZ_8K };
+
+        gem::new_kernel_object(tdev, tdev.iomem.clone(), self.vm.clone(), va, flags)
+    }
+
+    pub(crate) fn alloc_suspend_buf(
+        &self,
+        tdev: &TyrDevice,
+        suspend_size: usize,
+    ) -> Result<ObjectRef> {
+        let flags = map_flags::Flags::from(map_flags::NOEXEC);
+        let va = KernelVaPlacement::Auto { size: suspend_size };
+
+        gem::new_kernel_object(tdev, tdev.iomem.clone(), self.vm.clone(), va, flags)
+    }
+
+    /// Provide access to the global interface, but as a closure so we can at
+    /// least try to reduce the scope of the lock in as much as possible.
+    pub(crate) fn with_locked_global_iface<F, R>(&self, f: F) -> Result<R>
+    where
+        F: FnOnce(&mut GlobalInterface) -> Result<R>,
+    {
+        let mut global_iface = self.global_iface.lock();
+        f(&mut global_iface)
+    }
+}
+
+macro_rules! impl_shared_section_read {
+    ($type:ty) => {
+        impl $type {
+            /// Reads the control interface from the given pointer.
+            ///
+            /// Note that the area pointed to by `ptr` is shared with the MCU, so we
+            /// cannot simply parse it or cast it to &Self.
+            ///
+            /// Merely taking a reference to it would be UB, as the MCU can change the
+            /// underlying memory at any time, as it is a core running its own code.
+            pub(super) fn read(range: &SharedSectionRange) -> Result<Self> {
+                // Make sure all writes took place before we read the memory.
+                kernel::sync::barrier::smp_mb();
+
+                let ptr = range.as_mut_ptr()?;
+                // SAFETY: we know that this pointer is aligned and valid for reads for
+                // at least size_of::<Self>() bytes.
+                Ok(unsafe { core::ptr::read_volatile(ptr as *const Self) })
+            }
+        }
+    };
+}
+pub(crate) use impl_shared_section_read;
+
+macro_rules! impl_shared_section_write {
+    ($type:ty) => {
+        impl $type {
+            /// Writes the control interface to the given pointer.
+            ///
+            /// Note that the area pointed to by `ptr` is shared with the MCU, so we
+            /// cannot simply parse it or cast it to &mut Self.
+            ///
+            /// Merely taking a reference to it would be UB, as the MCU can change the
+            /// underlying memory at any time, as it is a core running its own code.
+            pub(super) fn write(self, range: &mut SharedSectionRange) -> Result<()> {
+                // Make sure all writes took place before we update the memory.
+                kernel::sync::barrier::smp_mb();
+
+                let ptr = range.as_mut_ptr()?;
+                // SAFETY: we know that this pointer is aligned and valid for writes for
+                // at least size_of::<Self>() bytes.
+                unsafe {
+                    core::ptr::write_volatile(ptr as *mut Self, self);
+                }
+
+                Ok(())
+            }
+        }
+    };
+}
+pub(crate) use impl_shared_section_write;
+
+macro_rules! impl_shared_section_rw {
+    ($type:ty) => {
+        crate::fw::impl_shared_section_read!($type);
+        crate::fw::impl_shared_section_write!($type);
+    };
+}
+pub(crate) use impl_shared_section_rw;
+
+/// Standardizes the interface to the shared section entries.
+///
+/// This helps to ensure that the same names are used consistently across the
+/// different sections, and that no part of the implementation is forgotten.
+pub(crate) trait SharedSectionEntry {
+    /// The type of the area written by the CPU in order to set CSF control
+    /// parameters.
+    type Control;
+
+    /// The type of the area written by the CPU as input to CSF.
+    type Input;
+
+    /// The type of the area written by CSF.
+    type Output;
+
+    fn read_control(&self) -> Result<Self::Control>;
+    fn write_control(&mut self, control: Self::Control) -> Result;
+
+    fn read_input(&self) -> Result<Self::Input>;
+    fn write_input(&mut self, input: Self::Input) -> Result;
+
+    fn read_output(&self) -> Result<Self::Output>;
+
+    fn input_request(&self) -> Result<RequestField>;
+
+    fn doobell_request(&self) -> Result<RequestField> {
+        pr_err!("Doorbell request not supported for this interface");
+        Err(ENOTSUPP)
+    }
+
+    fn interrupt_ack(&self) -> Result<RequestField> {
+        pr_err!("Interrupt ack not supported for this interface");
+        Err(ENOTSUPP)
+    }
+}
diff --git a/drivers/gpu/drm/tyr/fw/global.rs b/drivers/gpu/drm/tyr/fw/global.rs
new file mode 100644
index 0000000000..500bd3dd4e
--- /dev/null
+++ b/drivers/gpu/drm/tyr/fw/global.rs
@@ -0,0 +1,550 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! Code to control the global interface of the CSF firmware.
+
+use core::ops::Deref;
+
+use csg::CommandStreamGroup;
+use kernel::bits::genmask_u32;
+use kernel::clk::Clk;
+use kernel::devres::Devres;
+use kernel::impl_has_delayed_work;
+use kernel::io;
+use kernel::io::mem::IoMem;
+use kernel::kvec;
+use kernel::new_mutex;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::sync::Mutex;
+use kernel::time;
+#[allow(unused)]
+use kernel::workqueue;
+use kernel::workqueue::WorkItem;
+
+use crate::driver::TyrData;
+use crate::driver::TyrDevice;
+use crate::fw::impl_shared_section_read;
+use crate::fw::impl_shared_section_rw;
+use crate::fw::RequestField;
+use crate::fw::SharedSectionEntry;
+use crate::gpu::GpuInfo;
+use crate::regs::Doorbell;
+use crate::regs::CSF_GLB_DOORBELL_ID;
+use crate::wait::Wait;
+
+use super::{Section, SharedSectionRange};
+
+pub(crate) mod cs;
+pub(crate) mod csg;
+
+#[allow(dead_code)]
+mod constants {
+    use kernel::bits::{bit_u32, genmask_u32};
+
+    pub(super) const GLB_TIMER_SOURCE_GPU_COUNTER: u32 = bit_u32(31);
+    pub(super) const PROGRESS_TIMEOUT_CYCLES: u32 = 5 * 500 * 1024 * 1024;
+    pub(super) const PROGRESS_TIMEOUT_SCALE_SHIFT: u32 = 10;
+    pub(super) const IDLE_HYSTERESIS_US: u32 = 800;
+    pub(super) const PWROFF_HYSTERESIS_US: u32 = 10000;
+    pub(super) const GLB_HALT: u32 = bit_u32(0);
+    pub(super) const GLB_CFG_PROGRESS_TIMER: u32 = bit_u32(1);
+    pub(super) const GLB_CFG_ALLOC_EN: u32 = bit_u32(2);
+    pub(super) const GLB_CFG_POWEROFF_TIMER: u32 = bit_u32(3);
+    pub(super) const GLB_PROTM_ENTER: u32 = bit_u32(4);
+    pub(super) const GLB_PERFCNT_EN: u32 = bit_u32(5);
+    pub(super) const GLB_PERFCNT_SAMPLE: u32 = bit_u32(6);
+    pub(super) const GLB_COUNTER_EN: u32 = bit_u32(7);
+    pub(super) const GLB_PING: u32 = bit_u32(8);
+    pub(super) const GLB_FWCFG_UPDATE: u32 = bit_u32(9);
+    pub(super) const GLB_IDLE_EN: u32 = bit_u32(10);
+    pub(super) const GLB_SLEEP: u32 = bit_u32(12);
+    pub(super) const GLB_INACTIVE_COMPUTE: u32 = bit_u32(20);
+    pub(super) const GLB_INACTIVE_FRAGMENT: u32 = bit_u32(21);
+    pub(super) const GLB_INACTIVE_TILER: u32 = bit_u32(22);
+    pub(super) const GLB_PROTM_EXIT: u32 = bit_u32(23);
+    pub(super) const GLB_PERFCNT_THRESHOLD: u32 = bit_u32(24);
+    pub(super) const GLB_PERFCNT_OVERFLOW: u32 = bit_u32(25);
+    pub(super) const GLB_IDLE: u32 = bit_u32(26);
+    pub(super) const GLB_DBG_CSF: u32 = bit_u32(30);
+    pub(super) const GLB_DBG_HOST: u32 = bit_u32(31);
+    pub(super) const GLB_REQ_MASK: u32 = genmask_u32(10, 0);
+    pub(super) const GLB_EVT_MASK: u32 = genmask_u32(26, 20);
+
+    pub(super) const PING_INTERVAL_MS: i64 = 12000;
+}
+
+use constants::*;
+
+fn glb_timer_val(val: u32) -> u32 {
+    val & genmask_u32(30, 0)
+}
+
+#[repr(transparent)]
+/// A value that is valid to pass for timeout fields in the global interface.
+struct TimeoutCycles(u32);
+
+impl TimeoutCycles {
+    fn from_micro(core_clk: &Clk, timeout_us: u32) -> Result<Self> {
+        let timer_rate = core_clk.rate().as_hz() as u64;
+
+        if timer_rate == 0 {
+            return Err(EINVAL);
+        }
+
+        let mut mod_cycles = (u64::from(timeout_us) * timer_rate).div_ceil(1000000 << 10);
+
+        if mod_cycles > glb_timer_val(u32::MAX).into() {
+            pr_err!("Invalid timeout computed\n");
+            mod_cycles = glb_timer_val(u32::MAX).into();
+        }
+
+        let mod_cycles = u32::try_from(mod_cycles)?;
+        Ok(Self(
+            glb_timer_val(mod_cycles) | GLB_TIMER_SOURCE_GPU_COUNTER,
+        ))
+    }
+}
+
+impl From<TimeoutCycles> for u32 {
+    fn from(value: TimeoutCycles) -> Self {
+        value.0
+    }
+}
+
+/// The global control interface.
+#[repr(C)]
+pub(crate) struct Control {
+    pub(crate) version: u32,
+    pub(crate) features: u32,
+    pub(crate) input_va: u32,
+    pub(crate) output_va: u32,
+    pub(crate) group_num: u32,
+    pub(crate) group_stride: u32,
+    pub(crate) perfcnt_size: u32,
+    pub(crate) instr_features: u32,
+}
+
+impl Control {
+    /// CSF major version.
+    pub(crate) fn version_major(&self) -> u32 {
+        self.version >> 24
+    }
+
+    /// CSF minor version.
+    pub(crate) fn version_minor(&self) -> u32 {
+        (self.version >> 16) & 0xff
+    }
+
+    /// CSF patch version.
+    pub(crate) fn version_patch(&self) -> u32 {
+        self.version & 0xffff
+    }
+}
+
+#[repr(C)]
+#[derive(Debug)]
+/// The input area for the global interface
+pub(crate) struct Input {
+    pub(crate) req: u32,
+    pub(crate) ack_irq_mask: u32,
+    pub(crate) doorbell_req: u32,
+    pub(crate) reserved1: u32,
+    pub(crate) progress_timer: u32,
+    pub(crate) poweroff_timer: u32,
+    pub(crate) core_en_mask: u32,
+    pub(crate) reserved2: u32,
+    pub(crate) perfcnt_as: u32,
+    pub(crate) perfcnt_base: u64,
+    pub(crate) perfcnt_extratct: u32,
+    pub(crate) reserved3: [u32; 3],
+    pub(crate) percnt_config: u32,
+    pub(crate) percnt_csg_select: u32,
+    pub(crate) perfcnt_fw_enable: u32,
+    pub(crate) perfcnt_csg_enable: u32,
+    pub(crate) perfcnt_csf_enable: u32,
+    pub(crate) perfcnt_shader_enable: u32,
+    pub(crate) perfcnt_tiler_enable: u32,
+    pub(crate) perfcnt_mmu_l2_enable: u32,
+    pub(crate) reserved4: [u32; 8],
+    pub(crate) idle_timer: u32,
+}
+
+#[repr(C)]
+#[derive(Debug)]
+/// The output area for the global interface
+pub(crate) struct Output {
+    pub(crate) ack: u32,
+    pub(crate) reserved1: u32,
+    pub(crate) doorbell_ack: u32,
+    pub(crate) reserved2: u32,
+    pub(crate) halt_status: u32,
+    pub(crate) perfcnt_status: u32,
+    pub(crate) perfcnt_insert: u32,
+}
+
+impl_shared_section_rw!(Control);
+impl_shared_section_rw!(Input);
+impl_shared_section_read!(Output);
+
+pub(crate) enum GlobalInterfaceState {
+    Disabled,
+    Enabled(EnabledGlobalInterface),
+}
+
+impl GlobalInterfaceState {
+    fn enabled(&self) -> Result<&EnabledGlobalInterface> {
+        match self {
+            GlobalInterfaceState::Enabled(enabled) => Ok(enabled),
+            GlobalInterfaceState::Disabled => Err(EINVAL),
+        }
+    }
+
+    fn enabled_mut(&mut self) -> Result<&mut EnabledGlobalInterface> {
+        match self {
+            GlobalInterfaceState::Enabled(enabled) => Ok(enabled),
+            GlobalInterfaceState::Disabled => Err(EINVAL),
+        }
+    }
+}
+
+struct EnabledGlobalInterface {
+    control_area: SharedSectionRange,
+    input_area: SharedSectionRange,
+    output_area: SharedSectionRange,
+
+    csgs: KVec<CommandStreamGroup>,
+}
+
+/// The global interface.
+pub(crate) struct GlobalInterface {
+    state: GlobalInterfaceState,
+
+    iomem: Arc<Devres<IoMem>>,
+
+    shared_section: Arc<Mutex<Section>>,
+
+    event_wait: Arc<Wait>,
+
+    /// Whether the MCU has booted.
+    pub(super) booted: bool,
+}
+
+impl GlobalInterface {
+    pub(super) fn new(
+        shared_section: Section,
+        iomem: Arc<Devres<IoMem>>,
+        req_wait: Arc<Wait>,
+    ) -> Result<Self> {
+        let shared_section = Arc::pin_init(new_mutex!(shared_section), GFP_KERNEL)?;
+
+        Ok(Self {
+            state: GlobalInterfaceState::Disabled,
+            iomem,
+            shared_section,
+            event_wait: req_wait,
+            booted: false,
+        })
+    }
+
+    pub(crate) fn enable(
+        &mut self,
+        tdev: &TyrDevice,
+        gpu_info: &GpuInfo,
+        core_clk: &Clk,
+    ) -> Result {
+        // This takes a mutex internally in clk_prepare().
+        let poweroff_timer = TimeoutCycles::from_micro(core_clk, PWROFF_HYSTERESIS_US)?.into();
+
+        let control_area = SharedSectionRange {
+            shared_section: self.shared_section.clone(),
+            start: 0,
+            end: core::mem::size_of::<Control>(),
+        };
+
+        let op = || Control::read(&control_area);
+        let cond = |control: &Control| -> bool { control.version != 0 };
+        let _ = io::poll::read_poll_timeout(
+            op,
+            cond,
+            time::Delta::from_millis(0),
+            Some(time::Delta::from_millis(200)),
+        );
+
+        let control = Control::read(&control_area)?;
+        if control.version == 0 {
+            pr_err!("MCU firmware version is 0. Firmware may have failed to boot\n");
+            return Err(EINVAL);
+        }
+
+        let mut input_area =
+            self.shared_range(control.input_va.into(), core::mem::size_of::<Input>())?;
+
+        let output_area =
+            self.shared_range(control.output_va.into(), core::mem::size_of::<Output>())?;
+
+        /// The start of the CSG control area for the first CSG.
+        const CSF_GROUP_CONTROL_OFFSET: u32 = 0x1000;
+
+        let mut csgs: KVec<CommandStreamGroup> = kvec![];
+        for csg_idx in 0..control.group_num {
+            let iface_offset = CSF_GROUP_CONTROL_OFFSET + (csg_idx * control.group_stride);
+
+            let csg = CommandStreamGroup::init(self, iface_offset, csg_idx as usize)?;
+
+            if let Some(first) = csgs.first() {
+                if !first.is_identical(&csg)? {
+                    pr_err!("Expecting identical CSG slots\n");
+                    return Err(EINVAL);
+                }
+            }
+
+            csgs.push(csg, GFP_KERNEL)?;
+        }
+
+        pr_info!(
+            "CSF FW using interface v.{}.{}.{}, Features {} Instrumentation features {}\n",
+            control.version_major(),
+            control.version_minor(),
+            control.version_patch(),
+            control.features,
+            control.instr_features
+        );
+
+        let mut input = Input::read(&input_area)?;
+
+        // Enable all shader cores.
+        input.core_en_mask = gpu_info.shader_present as u32;
+
+        // Setup timers.
+        input.poweroff_timer = poweroff_timer;
+        input.progress_timer = PROGRESS_TIMEOUT_CYCLES >> PROGRESS_TIMEOUT_SCALE_SHIFT;
+        input.idle_timer = IDLE_HYSTERESIS_US;
+
+        // Enable the interrupts we care about.
+        input.ack_irq_mask = GLB_CFG_ALLOC_EN
+            | GLB_PING
+            | GLB_CFG_POWEROFF_TIMER
+            | GLB_CFG_POWEROFF_TIMER
+            | GLB_IDLE_EN
+            | GLB_IDLE;
+
+        input.write(&mut input_area)?;
+
+        let req = RequestField::new(
+            &input_area,
+            core::mem::offset_of!(Input, req),
+            core::mem::offset_of!(Output, ack),
+        );
+        req.update_reqs(GLB_IDLE_EN, GLB_IDLE_EN)?;
+
+        let reqs = GLB_CFG_ALLOC_EN | GLB_CFG_POWEROFF_TIMER | GLB_CFG_PROGRESS_TIMER;
+        req.toggle_reqs(reqs)?;
+
+        self.ring_glb_doorbell()?;
+
+        let enabled = EnabledGlobalInterface {
+            control_area,
+            input_area,
+            output_area,
+            csgs,
+        };
+
+        self.state = GlobalInterfaceState::Enabled(enabled);
+        self.arm_watchdog(tdev)
+    }
+
+    /// Ring the global interface doorbell.
+    pub(crate) fn ring_glb_doorbell(&self) -> Result {
+        // Make sure that all previous writes are visible to the CSF before it
+        // can be awaken.
+        kernel::sync::barrier::smp_mb();
+        Doorbell::new(CSF_GLB_DOORBELL_ID).write(&self.iomem, 1)
+    }
+
+    pub(crate) fn csg(&mut self, csg_idx: usize) -> Option<&CommandStreamGroup> {
+        match &self.state {
+            GlobalInterfaceState::Disabled => None,
+            GlobalInterfaceState::Enabled(EnabledGlobalInterface { csgs, .. }) => csgs.get(csg_idx),
+        }
+    }
+
+    pub(crate) fn csg_mut(&mut self, csg_idx: usize) -> Option<&mut CommandStreamGroup> {
+        match &mut self.state {
+            GlobalInterfaceState::Disabled => None,
+            GlobalInterfaceState::Enabled(EnabledGlobalInterface { csgs, .. }) => {
+                csgs.get_mut(csg_idx)
+            }
+        }
+    }
+
+    pub(crate) fn arm_watchdog(&self, tdev: &TyrDevice) -> Result {
+        tdev.reset_wq
+            .enqueue_delayed::<_, 0>(tdev.deref().clone(), PING_INTERVAL_MS as usize)
+            .map_err(|_| EINVAL)
+    }
+
+    pub(crate) fn ping(&mut self) -> Result {
+        let glb_iface = match self.state {
+            GlobalInterfaceState::Enabled(ref enabled) => enabled,
+            GlobalInterfaceState::Disabled => {
+                pr_err!("Trying to ping CSF but the global interface is down\n");
+                return Ok(());
+            }
+        };
+
+        let req = RequestField::new(
+            &glb_iface.input_area,
+            core::mem::offset_of!(Input, req),
+            core::mem::offset_of!(Output, ack),
+        );
+
+        req.toggle_reqs(GLB_PING)?;
+
+        self.ring_glb_doorbell()?;
+
+        if let Err(ETIMEDOUT) = req.wait_acks(GLB_PING, &self.event_wait, 100) {
+            pr_err!("CSF has not responded to a ping request\n");
+            pr_err!("The firmware probably crashed\n");
+        }
+
+        Ok(())
+    }
+
+    /// Computes a range into the shared section for a given VA in the shared
+    /// area.
+    ///
+    /// The result is an offset that can be safely dereferenced by the CPU.
+    pub(super) fn shared_range(&mut self, mcu_va: u64, size: usize) -> Result<SharedSectionRange> {
+        let shared_mem_start = u64::from(self.shared_section.lock().va.start);
+        let shared_mem_end = u64::from(self.shared_section.lock().va.end);
+
+        if mcu_va < shared_mem_start || mcu_va >= shared_mem_end {
+            Err(EINVAL)
+        } else {
+            let offset = (mcu_va - shared_mem_start) as usize;
+            Ok(SharedSectionRange {
+                shared_section: self.shared_section.clone(),
+                start: offset,
+                end: offset + size,
+            })
+        }
+    }
+
+    /// Set the CSG state.
+    pub(crate) fn set_csg_state(&mut self, csg_idx: usize, state: csg::GroupState) -> Result {
+        let glb = self.state.enabled_mut()?;
+        let csg_iface = glb.csgs.get_mut(csg_idx).ok_or(EINVAL)?;
+
+        csg_iface.set_group_state(state)
+    }
+
+    /// Ring the CSG doorbell, thereby instructing CSF to process the requests
+    /// made on this CSG.
+    pub(crate) fn ring_csg_doorbell(&mut self, csg_idx: usize) -> Result {
+        self.doobell_request()?.toggle_reqs(1 << csg_idx)?;
+
+        self.ring_glb_doorbell()?;
+
+        let op = || self.read_output();
+        let cond = |output: &Output| -> bool { output.doorbell_ack == 1 };
+        let _ = io::poll::read_poll_timeout(
+            op,
+            cond,
+            time::Delta::from_millis(0),
+            Some(time::Delta::from_millis(100)),
+        );
+
+        Ok(())
+    }
+
+    fn shared_section_size(&self) -> usize {
+        let shared_section = self.shared_section.lock();
+        shared_section.mem.size()
+    }
+
+    /// Whether the firmware has booted or not.
+    pub(crate) fn booted(&self) -> bool {
+        self.booted
+    }
+}
+
+impl_has_delayed_work! {
+    impl HasDelayedWork<Self, 0> for TyrData {
+        self.ping_work
+    }
+}
+
+impl WorkItem<0> for TyrData {
+    type Pointer = Arc<Self>;
+
+    fn run(this: Self::Pointer) {
+        let res = this.fw.with_locked_global_iface(|glb| {
+            glb.ping()?;
+            this.reset_wq
+                .enqueue_delayed::<_, 0>(this.clone(), PING_INTERVAL_MS as usize)
+                .map_err(|_| EINVAL)
+        });
+
+        if let Err(err) = res {
+            pr_err!(
+                "Ping failed: {}, the firmware probably crashed\n",
+                err.to_errno()
+            );
+
+            if let Ok(mcu_status) = crate::regs::MCU_STATUS.read(&this.iomem) {
+                pr_err!("MCU_STATUS is {}\n", mcu_status);
+            }
+        }
+    }
+}
+
+impl SharedSectionEntry for GlobalInterface {
+    type Control = Control;
+    type Input = Input;
+    type Output = Output;
+
+    fn read_control(&self) -> Result<Self::Control> {
+        let glb = self.state.enabled()?;
+        Control::read(&glb.control_area)
+    }
+
+    fn write_control(&mut self, control: Self::Control) -> Result {
+        let glb = self.state.enabled_mut()?;
+        control.write(&mut glb.control_area)
+    }
+
+    fn read_input(&self) -> Result<Self::Input> {
+        let glb = self.state.enabled()?;
+        Input::read(&glb.input_area)
+    }
+
+    fn write_input(&mut self, input: Self::Input) -> Result {
+        let glb = self.state.enabled_mut()?;
+        input.write(&mut glb.input_area)
+    }
+
+    fn read_output(&self) -> Result<Self::Output> {
+        let glb = self.state.enabled()?;
+        Output::read(&glb.output_area)
+    }
+
+    fn input_request(&self) -> Result<RequestField> {
+        let glb = self.state.enabled()?;
+
+        Ok(RequestField::new(
+            &glb.input_area,
+            core::mem::offset_of!(Input, req),
+            core::mem::offset_of!(Output, ack),
+        ))
+    }
+
+    fn doobell_request(&self) -> Result<RequestField> {
+        let glb = self.state.enabled()?;
+
+        Ok(RequestField::new(
+            &glb.input_area,
+            core::mem::offset_of!(Input, doorbell_req),
+            core::mem::offset_of!(Output, doorbell_ack),
+        ))
+    }
+}
diff --git a/drivers/gpu/drm/tyr/fw/global/cs.rs b/drivers/gpu/drm/tyr/fw/global/cs.rs
new file mode 100644
index 0000000000..bfbbd86b9b
--- /dev/null
+++ b/drivers/gpu/drm/tyr/fw/global/cs.rs
@@ -0,0 +1,539 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! Command stream (CS) code.
+//!
+//! Represents a single hardware queue for command submission.
+//!
+//! Each CSG will expose a number of command streams that is firmware dependent.
+
+use kernel::bits::bit_u32;
+use kernel::bits::genmask_u32;
+use kernel::c_str;
+use kernel::prelude::*;
+
+use crate::fw::global::GlobalInterface;
+use crate::fw::impl_shared_section_read;
+use crate::fw::impl_shared_section_rw;
+use crate::fw::RequestField;
+use crate::fw::SharedSectionEntry;
+use crate::fw::SharedSectionRange;
+use constants::*;
+
+/// Used to decode command stream faults.
+pub(crate) const FAULT_EXCEPTION_MAP: &[(u32, &CStr)] = &[
+    (0x00, c_str!("OK")),
+    (0x05, c_str!("KABOOM")),
+    (0x0f, c_str!("CS_RESOURCE_TERMINATED")),
+    (0x48, c_str!("CS_BUS_FAULT")),
+    (0x4b, c_str!("CS_INHERIT_FAULT")),
+    (0x50, c_str!("INSTR_INVALID_PC")),
+    (0x51, c_str!("INSTR_INVALID_ENC")),
+    (0x55, c_str!("INSTR_BARRIER_FAULT")),
+    (0x58, c_str!("DATA_INVALID_FAULT")),
+    (0x59, c_str!("TILE_RANGE_FAULT")),
+    (0x5a, c_str!("ADDR_RANGE_FAULT")),
+    (0x5b, c_str!("IMPRECISE_FAULT")),
+    (0x69, c_str!("RESOURCE_EVICTION_TIMEOUT")),
+];
+
+pub(crate) fn fault_exception_name(code: u32) -> &'static CStr {
+    for &(exception_code, name) in FAULT_EXCEPTION_MAP {
+        if exception_code == code {
+            return name;
+        }
+    }
+    c_str!("UNKNOWN")
+}
+
+/// Used to decode command stream fatal errors.
+pub(crate) const FATAL_EXCEPTION_MAP: &[(u32, &CStr)] = &[
+    (0x00, c_str!("OK")),
+    (0x40, c_str!("CS_CONFIG_FAULT")),
+    (0x44, c_str!("CS_ENDPOINT_FAULT")),
+    (0x48, c_str!("CS_BUS_FAULT")),
+    (0x49, c_str!("CS_INVALID_INSTRUCTION")),
+    (0x4a, c_str!("CS_CALL_STACK_OVERFLOW")),
+    (0x68, c_str!("FIRMWARE_INTERNAL_ERROR")),
+];
+
+pub(crate) fn fatal_exception_name(code: u32) -> &'static CStr {
+    for &(exception_code, name) in FATAL_EXCEPTION_MAP {
+        if exception_code == code {
+            return name;
+        }
+    }
+    c_str!("UNKNOWN")
+}
+
+pub(crate) mod constants {
+    #![allow(dead_code)]
+    use kernel::bits::bit_u32;
+    use kernel::bits::genmask_u32;
+
+    const CS_STATE_STOP: u32 = 0;
+
+    const CS_STATE_START: u32 = 1;
+
+    pub(crate) const CS_EXTRACT_EVENT: u32 = bit_u32(4);
+
+    /// Enable idle events for sync/wait. If this is enabled, the CS is
+    /// considered idle when it is stalled due to a sync/wait dependency.
+    ///
+    /// This will trigger an IDLE event and also raise an interrupt if the IDLE
+    /// interrupt mask flag is also enabled.
+    pub(crate) const CS_IDLE_SYNC_WAIT: u32 = bit_u32(8);
+
+    /// Same as [`CS_IDLE_SYNC_WAIT`] but for protected mode.
+    ///
+    /// This will trigger an IDLE event and also raise an interrupt if the IDLE
+    /// interrupt mask flag is also enabled.
+    pub(crate) const CS_IDLE_PROTM_PENDING: u32 = bit_u32(9);
+
+    /// Enable idle events for empty ring buffers.
+    ///
+    /// Note that if this is enabled, command streams stalled because their ring
+    /// buffers are empty contribute to the IDLE event and interrupt.
+    ///
+    /// This will trigger an IDLE event and also raise an interrupt if the IDLE
+    /// interrupt mask flag is also enabled.
+    pub(crate) const CS_IDLE_EMPTY: u32 = bit_u32(10);
+
+    /// Enable idle events for resource requests, i.e.: the stream is considered
+    /// idle when it is waiting for the required resource requests to be
+    /// allocated to it.
+    ///
+    /// This will trigger an IDLE event and also raise an interrupt if the IDLE
+    /// interrupt mask flag is also enabled.
+    pub(crate) const CS_IDLE_RESOURCE_REQ: u32 = bit_u32(11);
+
+    /// Clear the tiler out-of-memory notification. This means that the CPU can
+    /// be notified again that the tiler has run out of memory.
+    ///
+    /// Note that this is updated when the global doorbell is written.
+    pub(crate) const CS_TILER_OOM: u32 = bit_u32(26);
+
+    /// Clear the protected mode pending notification. This means that the CPU c
+    /// an be notified again that the command stream is waiting for protected
+    /// mode.
+    ///
+    /// Note that this is updated when the global doorbell is written.
+    pub(crate) const CS_PROTM_PENDING: u32 = bit_u32(27);
+
+    /// Clear the fatal error notification. This means that the CPU can be
+    /// notified again that the command stream has encountered a non-recoverable
+    /// error.
+    ///
+    /// Note that this is updated when the global doorbell is written.
+    pub(crate) const CS_FATAL: u32 = bit_u32(30);
+
+    /// Clear the fault notification. This means that the CPU can be notified
+    /// again that the command stream has encountered a recoverable error.
+    ///
+    /// Note that this is updated when the global doorbell is written.
+    pub(crate) const CS_FAULT: u32 = bit_u32(31);
+
+    pub(crate) const CS_STATE_MASK: u32 = genmask_u32(2, 0);
+
+    pub(crate) const CS_REQ_MASK: u32 = CS_STATE_MASK
+        | CS_EXTRACT_EVENT
+        | CS_IDLE_SYNC_WAIT
+        | CS_IDLE_PROTM_PENDING
+        | CS_IDLE_EMPTY
+        | CS_IDLE_RESOURCE_REQ;
+
+    pub(crate) const CS_EVT_MASK: u32 = CS_TILER_OOM | CS_PROTM_PENDING | CS_FATAL | CS_FAULT;
+}
+
+pub(crate) struct CommandStream {
+    cs_id: usize,
+    csg_id: usize,
+
+    control_area: SharedSectionRange,
+    input_area: SharedSectionRange,
+    output_area: SharedSectionRange,
+
+    state: StreamState,
+}
+
+impl CommandStream {
+    pub(crate) fn init(
+        glb_iface: &mut GlobalInterface,
+        iface_offset: u32,
+        csg_id: usize,
+        cs_id: usize,
+    ) -> Result<Self> {
+        if iface_offset as usize + core::mem::size_of::<Self>()
+            >= glb_iface.shared_section.lock().mem.size()
+        {
+            pr_err!("CSG interface would overrun the shared section");
+            return Err(EINVAL);
+        }
+
+        let control_area = SharedSectionRange {
+            shared_section: glb_iface.shared_section.clone(),
+            start: iface_offset as usize,
+            end: core::mem::size_of::<Control>(),
+        };
+
+        let control = Control::read(&control_area)?;
+
+        let input_area =
+            glb_iface.shared_range(control.input_va.into(), core::mem::size_of::<Input>())?;
+
+        let output_area =
+            glb_iface.shared_range(control.output_va.into(), core::mem::size_of::<Output>())?;
+
+        Ok(CommandStream {
+            cs_id,
+            csg_id,
+            control_area,
+            input_area,
+            output_area,
+            state: StreamState::Stop,
+        })
+    }
+
+    pub(crate) fn set_state(&mut self, state: StreamState) -> Result {
+        self.input_request()?
+            .update_reqs(state as u32, CS_STATE_MASK)?;
+
+        self.state = state;
+        Ok(())
+    }
+
+    /// Decode a fatal error.
+    pub(crate) fn decode_fatal(&self) -> Result {
+        let output = self.read_output()?;
+        let exception_type = output.cs_fatal_exception_type();
+        let exception_data = output.cs_fatal_exception_data();
+        let exception_name = fatal_exception_name(exception_type);
+
+        pr_warn!(
+            "CSG slot: {} CS slot: {}\n\
+             CS_FATAL.EXCEPTION_TYPE: 0x{:x} ({})\n\
+             CS_FATAL.EXCEPTION_DATA: 0x{:x}\n\
+             CS_FATAL.FATAL_INFO: 0x{:x}\n",
+            self.csg_id,
+            self.cs_id,
+            exception_type,
+            exception_name.to_str().unwrap_or("UNKNOWN"),
+            exception_data,
+            output.fatal_info,
+        );
+
+        Ok(())
+    }
+
+    pub(crate) fn decode_fault(&self) -> Result {
+        let output = self.read_output()?;
+        let exception_type = output.cs_fault_exception_type();
+        let exception_data = output.cs_fault_exception_data();
+        let exception_name = fault_exception_name(exception_type);
+
+        pr_warn!(
+            "CSG slot: {} CS slot: {}\n\
+             CS_FAULT.EXCEPTION_TYPE: 0x{:x} ({})\n\
+             CS_FAULT.EXCEPTION_DATA: 0x{:x}\n\
+             CS_FAULT.FAULT_INFO: 0x{:x}\n",
+            self.csg_id,
+            self.cs_id,
+            exception_type,
+            exception_name.to_str().unwrap_or("UNKNOWN"),
+            exception_data,
+            output.fault_info,
+        );
+
+        Ok(())
+    }
+}
+
+impl SharedSectionEntry for CommandStream {
+    type Control = Control;
+    type Input = Input;
+    type Output = Output;
+
+    fn read_control(&self) -> Result<Self::Control> {
+        Control::read(&self.control_area)
+    }
+
+    fn write_control(&mut self, control: Self::Control) -> Result {
+        control.write(&mut self.control_area)
+    }
+
+    fn read_input(&self) -> Result<Self::Input> {
+        Input::read(&self.input_area)
+    }
+
+    fn write_input(&mut self, input: Self::Input) -> Result {
+        input.write(&mut self.input_area)
+    }
+
+    fn read_output(&self) -> Result<Self::Output> {
+        Output::read(&self.output_area)
+    }
+
+    fn input_request(&self) -> Result<RequestField> {
+        Ok(RequestField::new(
+            &self.input_area,
+            core::mem::offset_of!(Input, req),
+            core::mem::offset_of!(Output, ack),
+        ))
+    }
+}
+
+#[repr(C)]
+pub(crate) struct Control {
+    pub(crate) features: u32,
+    pub(crate) input_va: u32,
+    pub(crate) output_va: u32,
+}
+
+impl Control {
+    /// Returns the number of work registers available in the command stream.
+    pub(crate) fn work_regs(&self) -> u32 {
+        (self.features & genmask_u32(7, 0)) + 1
+    }
+
+    /// Returns the number of scoreboards available in the command stream.
+    pub(crate) fn scoreboards(&self) -> u32 {
+        (self.features & genmask_u32(15, 8)) >> 8
+    }
+
+    /// Whether this command stream supports compute workloads.
+    pub(crate) fn supports_compute(&self) -> bool {
+        self.features & bit_u32(16) != 0
+    }
+
+    /// Whether this command stream supports fragment workloads.
+    pub(crate) fn supports_fragment(&self) -> bool {
+        self.features & bit_u32(17) != 0
+    }
+
+    /// Whether this command stream supports tiler workloads.
+    pub(crate) fn supports_tiler(&self) -> bool {
+        self.features & bit_u32(18) != 0
+    }
+}
+
+#[repr(C)]
+pub(crate) struct Input {
+    pub(crate) req: u32,
+    pub(crate) config: u32,
+    pub(crate) reserved1: u32,
+    pub(crate) ack_irq_mask: u32,
+    pub(crate) ringbuf_base: u64,
+    pub(crate) ringbuf_size: u32,
+    pub(crate) reserved2: u32,
+    pub(crate) heap_start: u64,
+    pub(crate) heap_end: u64,
+    pub(crate) ringbuf_input: u64,
+    pub(crate) ringbuf_output: u64,
+    pub(crate) instr_config: u32,
+    pub(crate) instrbuf_size: u32,
+    pub(crate) instrbuf_base: u64,
+    pub(crate) instrbuf_offset_ptr: u64,
+}
+
+impl Input {
+    pub(crate) fn set_priority(&mut self, priority: u8) -> Result {
+        if priority >= 16 {
+            pr_err!("Invalid priority value: {}", priority);
+            return Err(EINVAL);
+        }
+
+        self.config |= u32::from(priority) & genmask_u32(3, 0);
+        Ok(())
+    }
+
+    pub(crate) fn set_doorbell_id(&mut self, doorbell_id: u32) -> Result {
+        if doorbell_id == 0 || doorbell_id > 63 {
+            pr_err!("Invalid doorbell value: {}", doorbell_id);
+            return Err(EINVAL);
+        }
+
+        self.config |= (doorbell_id << 8) & genmask_u32(15, 8);
+        Ok(())
+    }
+}
+
+#[repr(C)]
+pub(crate) struct Output {
+    pub(crate) ack: u32,
+    pub(crate) reserved1: [u32; 15],
+    pub(crate) status_cmd_ptr: u64,
+    pub(crate) status_wait: u32,
+    pub(crate) status_req_resource: u32,
+    pub(crate) status_wait_sync_ptr: u64,
+    pub(crate) status_wait_sync_value: u32,
+    pub(crate) status_scoreboards: u32,
+    pub(crate) status_blocked_reason: u32,
+    pub(crate) status_wait_sync_value_hi: u32,
+    pub(crate) reserved2: [u32; 6],
+    pub(crate) fault: u32,
+    pub(crate) fatal: u32,
+    pub(crate) fault_info: u64,
+    pub(crate) fatal_info: u64,
+    pub(crate) reserved3: [u32; 10],
+    pub(crate) heap_vt_start: u32,
+    pub(crate) heap_vt_end: u32,
+    pub(crate) reserved4: u32,
+    pub(crate) heap_frag_end: u32,
+    pub(crate) heap_address: u64,
+}
+
+impl Output {
+    pub(crate) fn cs_fault_exception_type(&self) -> u32 {
+        self.fault & genmask_u32(7, 0)
+    }
+
+    pub(crate) fn cs_fault_exception_data(&self) -> u32 {
+        self.fault >> 8 & genmask_u32(23, 0)
+    }
+
+    pub(crate) fn cs_fatal_exception_type(&self) -> u32 {
+        self.fatal & genmask_u32(7, 0)
+    }
+
+    pub(crate) fn cs_fatal_exception_data(&self) -> u32 {
+        self.fatal >> 8 & genmask_u32(23, 0)
+    }
+
+    pub(crate) fn status_wait(&self) -> Result<StatusWait> {
+        let status = self.status_wait;
+
+        let sb_mask = status & genmask_u32(15, 0);
+        let sb_source = (status & genmask_u32(19, 16)) >> 16;
+        let gt = (status & bit_u32(24)) != 0;
+        let progress_wait = (status & bit_u32(28)) != 0;
+        let protm_pend = (status & bit_u32(29)) != 0;
+        let sync64 = (status & bit_u32(30)) != 0;
+        let sync_wait = (status & bit_u32(31)) != 0;
+
+        Ok(StatusWait {
+            sb_mask,
+            sb_source,
+            gt,
+            progress_wait,
+            protm_pend,
+            sync64,
+            sync_wait,
+        })
+    }
+
+    pub(crate) fn blocked_reason(&self) -> Result<BlockedReason> {
+        let reason = self.status_blocked_reason & genmask_u32(3, 0);
+
+        let blocked_reason = match reason {
+            0 => BlockedReason::Unblocked,
+            1 => BlockedReason::SbWait,
+            2 => BlockedReason::ProgressWait,
+            3 => BlockedReason::SyncWait,
+            5 => BlockedReason::Deferred,
+            6 => BlockedReason::Resource,
+            7 => BlockedReason::Flush,
+            _ => return Err(EINVAL),
+        };
+
+        Ok(blocked_reason)
+    }
+}
+
+#[derive(Debug, Copy, Clone, PartialEq)]
+pub(crate) enum StreamState {
+    /// Stop the command stream. The execution of command stream instructions
+    /// stops and any job active runs to completion before the STOP request
+    /// completes (unless terminated at the CSG level).
+    Stop,
+    /// Initialize the command stream and start execution.
+    Start,
+}
+
+pub(crate) struct StatusWait {
+    /// Mask denoting which scoreboard entries are being waited on by this
+    /// command stream.
+    sb_mask: u32,
+
+    /// Source of the scoreboard wait status, if any.
+    sb_source: u32,
+
+    /// Whether the condition is a greater-than comparison.
+    gt: bool,
+
+    /// Whether the command stream is waiting for a PROGRESS_WAIT instruction.
+    progress_wait: bool,
+
+    /// Whether the command stream is waiting for protected mode execution.
+    protm_pend: bool,
+
+    /// Whether the sync object is 32 or 64 bits wide.
+    sync64: bool,
+
+    /// Whether the command stream is waiting for a SYNC_WAIT instruction.
+    sync_wait: bool,
+}
+
+pub(crate) enum BlockedReason {
+    /// The command stream is not blocked.
+    Unblocked = 0,
+
+    /// Blocked on scoreboards.
+    SbWait = 1,
+
+    /// Blocked on PROGRESS_WAIT instruction.
+    ProgressWait = 2,
+
+    /// Blocked on SYNC_WAIT32 or SYNC_WAIT64 instruction.
+    SyncWait = 3,
+
+    /// Awaiting storage for a deferred instruction.
+    Deferred = 5,
+
+    /// Waiting for resource allocation.
+    Resource = 6,
+
+    /// Waiting on the completion of a synchronous FLUSH_CACHE2 instruction.
+    Flush = 7,
+}
+
+impl_shared_section_rw!(Control);
+impl_shared_section_rw!(Input);
+impl_shared_section_read!(Output);
+
+/// The input interface for the ring buffer.
+///
+/// This area is only written by the CPU.
+///
+/// [`RingBufferInput::insert`] and [`RingBufferOutput::extract`] control the
+/// ends of the ring buffer; if both are identical, the buffer is considered
+/// empty.
+#[repr(C)]
+pub(crate) struct RingBufferInput {
+    /// Offset of the input point into the ring buffer.
+    ///
+    /// New instructions are appended to this offset by the CPU.
+    pub(crate) insert: u64,
+
+    /// Used to initialize the initial extract offset for the ring buffer.
+    pub(crate) extract_init: u64,
+}
+
+/// The output interface for the ring buffer.
+///
+/// This area is only written by CSF.
+///
+/// [`RingBufferInput::insert`] and [`RingBufferOutput::extract`] control the
+/// ends of the ring buffer; if both are identical, the buffer is considered
+/// empty.
+#[repr(C)]
+pub(crate) struct RingBufferOutput {
+    /// Offset of the extract point from the ring buffer.
+    ///
+    /// Ahead of this point, all instructions have been consumed by CSF, so this
+    /// provides an indication of free space for inserting new instructions in
+    /// the buffer. Locations in the ring buffer before this point can be
+    /// reused.
+    pub(crate) extract: u64,
+
+    /// Indicates whether the command stream is active on hardware.
+    pub(crate) active: u32,
+}
diff --git a/drivers/gpu/drm/tyr/fw/global/csg.rs b/drivers/gpu/drm/tyr/fw/global/csg.rs
new file mode 100644
index 0000000000..57251c2aed
--- /dev/null
+++ b/drivers/gpu/drm/tyr/fw/global/csg.rs
@@ -0,0 +1,360 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! Command stream group (CSG) code.
+//!
+//! A CSG is a group of command streams (CS) for a given context. They can be
+//! assigned to a single process at a time.
+//!
+//! A CSG exposes a given number of command streams, these are queues where
+//! actual work can be scheduled, provided that all resources for a given
+//! workload are available.
+//!
+//! The group is assigned an AS slot when it is activated. This address space is
+//! shared between all CS's in a CSG. Furthermore, all CS's in a group are
+//! suspended and resumed together, i.e.: as a single unit.
+//!
+//! A CSG is usually used to back a UMD queue abstraction, like a VkQueue, for
+//! example.
+//!
+//! Command stream groups are discovered using the global interface's control
+//! area.
+
+use kernel::bits::bit_u32;
+use kernel::kvec;
+use kernel::prelude::*;
+
+use crate::fw::global::cs::CommandStream;
+use crate::fw::impl_shared_section_read;
+use crate::fw::impl_shared_section_rw;
+use crate::fw::GlobalInterface;
+use crate::fw::RequestField;
+use crate::fw::SharedSectionEntry;
+use crate::fw::SharedSectionRange;
+use constants::*;
+
+/// Maximum number of command stream groups in all architectures so far.
+pub(crate) const MAX_CSGS: u32 = 31;
+
+pub(crate) mod constants {
+    #![allow(dead_code)]
+    use kernel::bits::bit_u32;
+    use kernel::bits::genmask_u32;
+
+    pub(crate) const CSG_STATE_MASK: u32 = genmask_u32(2, 0);
+    const CSG_STATE_TERMINATE: u32 = 0;
+    const CSG_STATE_START: u32 = 1;
+    const CSG_STATE_SUSPEND: u32 = 2;
+    const CSG_STATE_RESUME: u32 = 3;
+
+    pub(crate) const CSG_ENDPOINT_CONFIG: u32 = bit_u32(4);
+    pub(crate) const CSG_STATUS_UPDATE: u32 = bit_u32(5);
+
+    pub(crate) const CSG_SYNC_UPDATE: u32 = bit_u32(28);
+    pub(crate) const CSG_IDLE: u32 = bit_u32(29);
+    pub(crate) const CSG_DOORBELL: u32 = bit_u32(30);
+    pub(crate) const CSG_PROGRESS_TIMER_EVENT: u32 = bit_u32(31);
+
+    pub(crate) const CSG_REQ_MASK: u32 = CSG_STATE_MASK | CSG_ENDPOINT_CONFIG | CSG_STATUS_UPDATE;
+    pub(crate) const CSG_EVT_MASK: u32 = CSG_SYNC_UPDATE | CSG_IDLE | CSG_PROGRESS_TIMER_EVENT;
+
+    pub(crate) const fn csg_ep_req_compute(x: u32) -> u32 {
+        x & genmask_u32(7, 0)
+    }
+
+    pub(crate) const fn csg_ep_req_fragment(x: u32) -> u32 {
+        (x << 8) & genmask_u32(15, 8)
+    }
+
+    pub(crate) const fn csg_ep_req_tiler(x: u32) -> u32 {
+        (x << 16) & genmask_u32(19, 16)
+    }
+
+    pub(crate) const CSG_EP_REQ_EXCL_COMPUTE: u32 = bit_u32(20);
+    pub(crate) const CSG_EP_REQ_EXCL_FRAGMENT: u32 = bit_u32(21);
+
+    pub(crate) const fn csg_ep_req_priority(x: u32) -> u32 {
+        (x << 28) & genmask_u32(31, 28)
+    }
+
+    pub(crate) const CSG_EP_REQ_PRIORITY_MASK: u32 = genmask_u32(31, 28);
+}
+
+pub(crate) struct CommandStreamGroup {
+    csg_id: usize,
+
+    control_area: SharedSectionRange,
+    input_area: SharedSectionRange,
+    output_area: SharedSectionRange,
+
+    streams: KVec<CommandStream>,
+    state: GroupState,
+}
+
+impl CommandStreamGroup {
+    pub(crate) fn init(
+        glb_iface: &mut GlobalInterface,
+        iface_offset: u32,
+        csg_id: usize,
+    ) -> Result<Self> {
+        if iface_offset as usize + core::mem::size_of::<Self>() >= glb_iface.shared_section_size() {
+            pr_err!("CSG interface would overrun the shared section");
+            return Err(EINVAL);
+        }
+
+        let control_area = SharedSectionRange {
+            shared_section: glb_iface.shared_section.clone(),
+            start: iface_offset as usize,
+            end: core::mem::size_of::<Control>(),
+        };
+
+        let control = Control::read(&control_area)?;
+
+        let input_area =
+            glb_iface.shared_range(control.input_va.into(), core::mem::size_of::<Input>())?;
+
+        let output_area =
+            glb_iface.shared_range(control.output_va.into(), core::mem::size_of::<Output>())?;
+
+        const CSF_STREAM_CONTROL_OFFSET: u32 = 0x40;
+        let mut streams: KVec<CommandStream> = kvec![];
+
+        for cs_idx in 0..control.stream_num {
+            let iface_offset = iface_offset + CSF_STREAM_CONTROL_OFFSET + (cs_idx * control.stride);
+            let cs = CommandStream::init(glb_iface, iface_offset, csg_id, cs_idx as usize)?;
+
+            if let Some(first) = streams.first() {
+                let control = cs.read_control()?;
+                let first_control = first.read_control()?;
+
+                if control.features != first_control.features {
+                    pr_err!("Expecting identical CS slots in a group\n");
+                    return Err(EINVAL);
+                }
+            }
+
+            streams.push(cs, GFP_KERNEL)?;
+        }
+
+        Ok(CommandStreamGroup {
+            csg_id,
+            control_area,
+            input_area,
+            output_area,
+            streams,
+            state: GroupState::Terminate,
+        })
+    }
+
+    pub(super) fn set_group_state(&mut self, state: GroupState) -> Result<()> {
+        let req = self.input_request()?;
+        req.update_reqs(state as u32, CSG_STATE_MASK)?;
+
+        if let GroupState::Start = state {
+            req.toggle_reqs(constants::CSG_ENDPOINT_CONFIG)?;
+        }
+
+        self.state = state;
+        Ok(())
+    }
+
+    pub(super) fn is_identical(&self, other: &CommandStreamGroup) -> Result<bool> {
+        let a = self.read_control()?;
+        let b = other.read_control()?;
+
+        if a.features != b.features {
+            return Ok(false);
+        }
+        if a.suspend_size != b.suspend_size {
+            return Ok(false);
+        }
+        if a.protm_suspend_size != b.protm_suspend_size {
+            return Ok(false);
+        }
+        if a.stream_num != b.stream_num {
+            return Ok(false);
+        }
+
+        Ok(true)
+    }
+
+    /// Returns the stream of index `idx` in this command stream group.
+    pub(crate) fn cs(&self, idx: usize) -> Option<&CommandStream> {
+        self.streams.get(idx)
+    }
+
+    /// Returns the stream of index `idx` in this command stream group.
+    pub(crate) fn cs_mut(&mut self, idx: usize) -> Option<&mut CommandStream> {
+        self.streams.get_mut(idx)
+    }
+
+    pub(crate) fn csg_id(&self) -> usize {
+        self.csg_id
+    }
+}
+
+impl SharedSectionEntry for CommandStreamGroup {
+    type Control = Control;
+    type Input = Input;
+    type Output = Output;
+
+    fn read_control(&self) -> Result<Self::Control> {
+        Control::read(&self.control_area)
+    }
+
+    fn write_control(&mut self, control: Self::Control) -> Result {
+        control.write(&mut self.control_area)
+    }
+
+    fn read_input(&self) -> Result<Self::Input> {
+        Input::read(&self.input_area)
+    }
+
+    fn write_input(&mut self, input: Self::Input) -> Result {
+        input.write(&mut self.input_area)
+    }
+
+    fn read_output(&self) -> Result<Self::Output> {
+        Output::read(&self.output_area)
+    }
+
+    fn input_request(&self) -> Result<RequestField> {
+        Ok(RequestField::new(
+            &self.input_area,
+            core::mem::offset_of!(Input, req),
+            core::mem::offset_of!(Output, ack),
+        ))
+    }
+
+    fn doobell_request(&self) -> Result<RequestField> {
+        Ok(RequestField::new(
+            &self.input_area,
+            core::mem::offset_of!(Input, doorbell_req),
+            core::mem::offset_of!(Output, doorbell_ack),
+        ))
+    }
+
+    fn interrupt_ack(&self) -> Result<RequestField> {
+        // Note that the order is reversed, because the roles are switched: this
+        // is the CPU answering to CSF.
+        Ok(RequestField::new(
+            &self.input_area,
+            core::mem::offset_of!(Input, irq_ack),
+            core::mem::offset_of!(Output, irq_req),
+        ))
+    }
+}
+
+#[repr(C)]
+pub(crate) struct Control {
+    pub(crate) features: u32,
+    pub(crate) input_va: u32,
+    pub(crate) output_va: u32,
+    pub(crate) suspend_size: u32,
+    pub(crate) protm_suspend_size: u32,
+    pub(crate) stream_num: u32,
+    pub(crate) stride: u32,
+}
+
+#[repr(C)]
+pub(crate) struct Input {
+    pub(crate) req: u32,
+    pub(crate) ack_irq_mask: u32,
+    pub(crate) doorbell_req: u32,
+    pub(crate) irq_ack: u32,
+    pub(crate) reserved1: [u32; 4],
+    pub(crate) allow_compute: u64,
+    pub(crate) allow_fragment: u64,
+    pub(crate) allow_other: u32,
+    pub(crate) csg_ep_req: u32,
+    pub(crate) reserved2: [u32; 2],
+    pub(crate) suspend_buf: u64,
+    pub(crate) protm_suspend_buf: u64,
+    pub(crate) csg_config: u32,
+    pub(crate) reserved3: u32,
+}
+
+impl Input {
+    pub(crate) fn set_endpoint_req(
+        &mut self,
+        compute: u32,
+        fragment: u32,
+        tiler: u32,
+        priority: Priority,
+    ) {
+        self.csg_ep_req = constants::csg_ep_req_compute(compute)
+            | constants::csg_ep_req_fragment(fragment)
+            | constants::csg_ep_req_tiler(tiler)
+            | constants::csg_ep_req_priority(priority as u32);
+    }
+}
+
+#[repr(C)]
+pub(crate) struct Output {
+    pub(crate) ack: u32,
+    pub(crate) reserved1: u32,
+    pub(crate) doorbell_ack: u32,
+    pub(crate) irq_req: u32,
+    pub(crate) status_ep_current: u32,
+    pub(crate) status_ep_req: u32,
+    pub(crate) status_state: u32,
+    pub(crate) resource_dep: u32,
+}
+
+impl Output {
+    pub(crate) fn is_idle(&self) -> bool {
+        self.status_state & bit_u32(0) != 0
+    }
+}
+
+impl_shared_section_rw!(Control);
+impl_shared_section_rw!(Input);
+impl_shared_section_read!(Output);
+
+#[derive(Copy, Clone, Debug, PartialEq)]
+pub(crate) enum GroupState {
+    Terminate,
+    Start,
+    Suspend,
+    Resume,
+}
+
+/// Represents the priority levels for a Command Stream Group (CSG).
+#[derive(Debug, Copy, Clone, PartialEq, Eq)]
+pub(crate) enum Priority {
+    /// Low priority group.
+    Low = 0,
+
+    /// Medium priority group.
+    Medium = 1,
+
+    /// High priority group.
+    High = 2,
+
+    /// Real-time priority group.
+    ///
+    /// Real-time priority allows preempting the scheduling of other
+    /// non-real-time groups. When such a group becomes executable,
+    /// it will evict the group with the lowest non-real-time priority
+    /// if there's no free group slot available.
+    RealTime = 3,
+}
+
+impl Priority {
+    pub(crate) const fn num_priorities() -> usize {
+        4
+    }
+}
+
+impl TryFrom<u8> for Priority {
+    type Error = Error;
+
+    fn try_from(value: u8) -> Result<Self> {
+        match value {
+            0 => Ok(Priority::Low),
+            1 => Ok(Priority::Medium),
+            2 => Ok(Priority::High),
+            3 => Ok(Priority::RealTime),
+            _ => Err(EINVAL),
+        }
+    }
+}
diff --git a/drivers/gpu/drm/tyr/fw/irq.rs b/drivers/gpu/drm/tyr/fw/irq.rs
new file mode 100644
index 0000000000..eaeb0fcb45
--- /dev/null
+++ b/drivers/gpu/drm/tyr/fw/irq.rs
@@ -0,0 +1,88 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! The IRQ handling for the Job IRQs.
+//!
+//! The Job IRQ controls our interactions with the MCU.
+
+use kernel::c_str;
+use kernel::devres::Devres;
+use kernel::io::mem::IoMem;
+use kernel::irq::ThreadedRegistration;
+use kernel::platform;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::types::ARef;
+
+use crate::driver::TyrDevice;
+use crate::driver::TyrIrq;
+use crate::driver::TyrIrqTrait;
+use crate::regs;
+use crate::wait::Wait;
+
+pub(crate) struct JobIrq {
+    iomem: Arc<Devres<IoMem>>,
+    event_wait: Arc<Wait>,
+    boot_wait: Arc<Wait>,
+}
+
+pub(crate) fn job_irq_init<'a>(
+    tdev: ARef<TyrDevice>,
+    pdev: &'a platform::Device<kernel::device::Bound>,
+    iomem: Arc<Devres<IoMem>>,
+    event_wait: Arc<Wait>,
+    boot_wait: Arc<Wait>,
+) -> Result<impl PinInit<ThreadedRegistration<TyrIrq<JobIrq>>, Error> + 'a> {
+    crate::regs::JOB_INT_MASK.write(&iomem, u32::MAX)?;
+
+    let irq_type = JobIrq {
+        iomem: iomem.clone(),
+        event_wait,
+        boot_wait,
+    };
+
+    TyrIrq::request(pdev, tdev, c_str!("job"), irq_type)
+}
+
+impl TyrIrqTrait for JobIrq {
+    fn read_status(&self) -> u32 {
+        regs::JOB_INT_STAT.read(&self.iomem).unwrap_or_default()
+    }
+
+    fn disable_all(&self) {
+        let _ = regs::JOB_INT_MASK.write(&self.iomem, 0);
+    }
+
+    fn reenable(&self) {
+        let _ = regs::JOB_INT_MASK.write(&self.iomem, self.mask());
+    }
+
+    fn read_raw_status(&self) -> u32 {
+        regs::JOB_INT_RAWSTAT.read(&self.iomem).unwrap_or_default()
+    }
+
+    fn clear_status(&self, status: u32) {
+        let _ = regs::JOB_INT_CLEAR.write(&self.iomem, status);
+    }
+
+    fn mask(&self) -> u32 {
+        u32::MAX // for now.
+    }
+
+    fn handle(&self, tdev: &TyrDevice, status: u32) {
+        self.event_wait.notify_all();
+
+        let _ = tdev.fw.with_locked_global_iface(|glb| {
+            if status & regs::JOB_INT_GLOBAL_IF != 0 && !glb.booted {
+                glb.booted = true;
+            }
+            Ok(())
+        });
+
+        self.boot_wait.notify_all();
+
+        let _ = tdev.with_locked_scheduler(|sched| {
+            sched.set_events(tdev, status);
+            Ok(())
+        });
+    }
+}
diff --git a/drivers/gpu/drm/tyr/fw/parse.rs b/drivers/gpu/drm/tyr/fw/parse.rs
new file mode 100644
index 0000000000..59603dd219
--- /dev/null
+++ b/drivers/gpu/drm/tyr/fw/parse.rs
@@ -0,0 +1,554 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! Code to parse the firmware binary.
+
+use core::ops::Range;
+
+use cursor::Cursor;
+use kernel::alloc::KVec;
+use kernel::bits::bit_u32;
+use kernel::c_str;
+use kernel::devres::Devres;
+use kernel::fmt;
+use kernel::io::mem::IoMem;
+use kernel::prelude::*;
+use kernel::str::CString;
+use kernel::sync::Arc;
+use kernel::sync::Mutex;
+
+use crate::driver::TyrDevice;
+use crate::fw::Firmware;
+use crate::fw::CSF_MCU_SHARED_REGION_START;
+use crate::gem;
+use crate::gem::KernelVaPlacement;
+use crate::gpu::GpuId;
+use crate::gpu::GpuInfo;
+use crate::mmu::vm;
+use crate::mmu::vm::Vm;
+
+mod cursor;
+
+const FW_BINARY_MAGIC: u32 = 0xc3f13a6e;
+const FW_BINARY_MAJOR_MAX: u8 = 0;
+
+mod flags {
+    use kernel::bits::bit_u32;
+    use kernel::bits::genmask_u32;
+    use kernel::prelude::*;
+
+    use crate::impl_flags;
+
+    impl_flags!(Flags, Flag, u32);
+
+    const CACHE_MODE_MASK: Flags = Flags(genmask_u32(4, 3));
+
+    impl Flags {
+        pub(crate) fn cache_mode(&self) -> Flags {
+            *self & CACHE_MODE_MASK
+        }
+    }
+
+    impl TryFrom<u32> for Flags {
+        type Error = Error;
+
+        fn try_from(value: u32) -> Result<Self, Self::Error> {
+            if value & valid_flags().0 != value {
+                Err(EINVAL)
+            } else {
+                Ok(Self(value))
+            }
+        }
+    }
+
+    pub(crate) fn valid_flags() -> Flags {
+        Flags::from(READ)
+            | Flags::from(WRITE)
+            | Flags::from(EXEC)
+            | CACHE_MODE_MASK
+            | Flags::from(PROT)
+            | Flags::from(SHARED)
+            | Flags::from(ZERO)
+    }
+
+    pub(crate) const READ: Flag = Flag(bit_u32(0));
+    pub(crate) const WRITE: Flag = Flag(bit_u32(1));
+    pub(crate) const EXEC: Flag = Flag(bit_u32(2));
+    pub(crate) const CACHE_MODE_NONE: Flag = Flag(0 << 3);
+    pub(crate) const CACHE_MODE_CACHED: Flag = Flag(1 << 3);
+    pub(crate) const CACHE_MODE_UNCACHED_COHERENT: Flag = Flag(2 << 3);
+    pub(crate) const CACHE_MODE_CACHED_COHERENT: Flag = Flag(3 << 3);
+    pub(crate) const PROT: Flag = Flag(bit_u32(5));
+    pub(crate) const SHARED: Flag = Flag(bit_u32(30));
+    pub(crate) const ZERO: Flag = Flag(bit_u32(31));
+}
+
+struct BuildInfoHeader(Range<u32>);
+
+/// A parsed section of the firmware binary.
+pub(crate) struct Section {
+    /// Flags for this section.
+    flags: flags::Flags,
+
+    /// The name of the section in the binary, if any.
+    name: Option<CString>,
+
+    /// The raw parsed data for reset purposes.
+    data: KVec<u8>,
+
+    /// The BO that this section was loaded into.
+    pub(super) mem: gem::ObjectRef,
+
+    /// The VA range for this section.
+    ///
+    /// The MCU expects the firmware to be loaded at a specific addresses.
+    pub(super) va: Range<u32>,
+
+    /// The flags used to map this section.
+    vm_map_flags: vm::map_flags::Flags,
+}
+
+/// The firmware header.
+struct BinaryHeader {
+    /// Magic value to check binary validity.
+    magic: u32,
+
+    /// Minor FW version.
+    minor: u8,
+
+    /// Major FW version.
+    major: u8,
+
+    /// Padding. Must be set to zero.
+    _padding1: u16,
+
+    /// FW Version hash
+    version_hash: u32,
+
+    /// Padding. Must be set to zero.
+    _padding2: u32,
+
+    /// FW binary size
+    size: u32,
+}
+
+impl BinaryHeader {
+    fn new(tdev: &TyrDevice, cursor: &mut Cursor<'_>) -> Result<Self> {
+        let magic = cursor.read_u32(tdev)?;
+        if magic != FW_BINARY_MAGIC {
+            dev_err!(tdev.as_ref(), "Invalid firmware magic");
+            return Err(EINVAL);
+        }
+
+        let minor = cursor.read_u8(tdev)?;
+        let major = cursor.read_u8(tdev)?;
+        let padding1 = cursor.read_u16(tdev)?;
+        let version_hash = cursor.read_u32(tdev)?;
+        let padding2 = cursor.read_u32(tdev)?;
+        let size = cursor.read_u32(tdev)?;
+
+        if padding1 != 0 || padding2 != 0 {
+            dev_err!(
+                tdev.as_ref(),
+                "Invalid firmware file: header padding is not zero"
+            );
+            return Err(EINVAL);
+        }
+
+        Ok(Self {
+            magic,
+            minor,
+            major,
+            _padding1: padding1,
+            version_hash,
+            _padding2: padding2,
+            size,
+        })
+    }
+}
+
+#[derive(Clone, Copy, Debug)]
+enum BinaryEntryType {
+    /// Host <-> FW interface.
+    Iface = 0,
+    /// FW config.
+    Config = 1,
+    /// Unit tests.
+    FutfTest = 2,
+    /// Trace buffer interface.
+    TraceBuffer = 3,
+    /// Timeline metadata interface,
+    TimelineMetadata = 4,
+    /// Metadata about how the FW binary was built
+    BuildInfoMetadata = 6,
+}
+
+impl TryFrom<u8> for BinaryEntryType {
+    type Error = Error;
+
+    fn try_from(value: u8) -> Result<Self, Self::Error> {
+        match value {
+            0 => Ok(BinaryEntryType::Iface),
+            1 => Ok(BinaryEntryType::Config),
+            2 => Ok(BinaryEntryType::FutfTest),
+            3 => Ok(BinaryEntryType::TraceBuffer),
+            4 => Ok(BinaryEntryType::TimelineMetadata),
+            6 => Ok(BinaryEntryType::BuildInfoMetadata),
+            _ => Err(EINVAL),
+        }
+    }
+}
+
+#[derive(Debug)]
+struct BinarySectionEntryHeader {
+    /// Section flags
+    flags: flags::Flags,
+    /// MCU virtual range to map this binary section to.
+    va: Range<u32>,
+    /// References the data in the FW binary.
+    data: Range<u32>,
+}
+
+impl BinarySectionEntryHeader {
+    fn new(tdev: &TyrDevice, cursor: &mut Cursor<'_>) -> Result<Self> {
+        let flags = cursor.read_u32(tdev)?;
+        let flags = flags::Flags::try_from(flags)?;
+
+        let va_start = cursor.read_u32(tdev)?;
+        let va_end = cursor.read_u32(tdev)?;
+
+        let va = va_start..va_end;
+
+        if va.is_empty() {
+            dev_err!(
+                tdev.as_ref(),
+                "Invalid firmware file: empty VA range at pos {}\n",
+                cursor.pos(),
+            );
+            return Err(EINVAL);
+        }
+
+        let data_start = cursor.read_u32(tdev)?;
+        let data_end = cursor.read_u32(tdev)?;
+        let data = data_start..data_end;
+
+        Ok(Self { flags, va, data })
+    }
+}
+
+struct BinaryEntryHeader(u32);
+
+impl BinaryEntryHeader {
+    /// The entry type.
+    fn entry_ty(&self) -> Result<BinaryEntryType> {
+        let v = (self.0 & 0xff) as u8;
+        BinaryEntryType::try_from(v)
+    }
+
+    /// Whether this entry is optional.
+    fn optional(&self) -> bool {
+        self.0 & bit_u32(31) != 0
+    }
+
+    /// The size of the entry.
+    fn size(&self) -> u32 {
+        self.0 >> 8 & 0xff
+    }
+}
+
+struct BinaryEntrySection {
+    hdr: BinaryEntryHeader,
+    inner: Option<Section>,
+}
+
+impl Firmware {
+    /// Parses the firmware sections from the binary.
+    pub(super) fn read_sections(
+        tdev: &TyrDevice,
+        iomem: Arc<Devres<IoMem>>,
+        gpu_info: &GpuInfo,
+        vm: Arc<Mutex<Vm>>,
+    ) -> Result<(KVec<Section>, Section)> {
+        let gpu_id = GpuId::from(gpu_info.gpu_id);
+
+        let fw_path = CString::try_from_fmt(fmt!(
+            "arm/mali/arch{}.{}/mali_csffw.bin",
+            gpu_id.arch_major,
+            gpu_id.arch_minor
+        ))?;
+
+        let fw = kernel::firmware::Firmware::request(&fw_path, tdev.as_ref())?;
+
+        let mut cursor = Cursor::new(fw.data());
+
+        dev_err!(
+            tdev.as_ref(),
+            "Requested {} bytes of firmware successfully\n",
+            fw.data().len()
+        );
+        let fw_bin_hdr = match BinaryHeader::new(tdev, &mut cursor) {
+            Ok(fw_bin_hdr) => fw_bin_hdr,
+            Err(e) => {
+                dev_err!(tdev.as_ref(), "Invalid firmware file: {}", e.to_errno());
+                return Err(e);
+            }
+        };
+
+        if fw_bin_hdr.magic != FW_BINARY_MAGIC {
+            dev_err!(tdev.as_ref(), "Invalid firmware magic");
+            return Err(EINVAL);
+        }
+
+        if fw_bin_hdr.major > FW_BINARY_MAJOR_MAX {
+            dev_err!(
+                tdev.as_ref(),
+                "Unsupported firmware binary version: {}.{}",
+                fw_bin_hdr.major,
+                fw_bin_hdr.minor
+            );
+            return Err(EINVAL);
+        }
+
+        if fw_bin_hdr.size > cursor.len() as u32 {
+            dev_err!(tdev.as_ref(), "Firmware image is truncated");
+            return Err(EINVAL);
+        }
+
+        let mut sections = Vec::new();
+        let mut shared_section = None;
+
+        while (cursor.pos() as u32) < fw_bin_hdr.size {
+            match Self::read_entry(&mut cursor, tdev, iomem.clone(), &fw, vm.clone())? {
+                section => {
+                    cursor.advance((section.hdr.size() - 4) as usize)?;
+
+                    match section.inner {
+                        Some(section) => {
+                            // TODO: refactor this.
+                            if section.flags.contains(flags::SHARED) {
+                                shared_section = Some(section);
+                            } else {
+                                sections.push(section, GFP_KERNEL)?
+                            }
+                        }
+                        None => continue,
+                    }
+                }
+            }
+        }
+
+        let shared_section = shared_section.ok_or_else(|| {
+            dev_err!(tdev.as_ref(), "No shared section found in firmware");
+            EINVAL
+        })?;
+
+        Ok((sections, shared_section))
+    }
+
+    fn read_entry(
+        cursor: &mut Cursor<'_>,
+        tdev: &TyrDevice,
+        iomem: Arc<Devres<IoMem>>,
+        fw: &kernel::firmware::Firmware,
+        vm: Arc<Mutex<Vm>>,
+    ) -> Result<BinaryEntrySection> {
+        let section = BinaryEntrySection {
+            hdr: BinaryEntryHeader(cursor.read_u32(tdev)?),
+            inner: None,
+        };
+
+        let section_size = section.hdr.size() as usize - core::mem::size_of::<BinaryEntryHeader>();
+
+        let entry_ty = match section.hdr.entry_ty() {
+            Ok(entry_ty) => entry_ty,
+            Err(e) => {
+                if section.hdr.optional() {
+                    dev_info!(
+                        tdev.as_ref(),
+                        "Skipping unknown optional firmware entry type: {}",
+                        e.to_errno()
+                    );
+                    return Ok(section);
+                } else {
+                    dev_err!(
+                        tdev.as_ref(),
+                        "Invalid firmware entry type: {}",
+                        e.to_errno()
+                    );
+                    return Err(EINVAL);
+                }
+            }
+        };
+
+        if cursor.pos() % core::mem::size_of::<u32>() != 0 {
+            dev_err!(
+                tdev.as_ref(),
+                "Invalid firmware file: entry not aligned to 4 bytes at pos {}\n",
+                cursor.pos()
+            );
+            return Err(EINVAL);
+        }
+
+        let mut entry_cursor = cursor.view(cursor.pos()..cursor.pos() + section_size)?;
+
+        match entry_ty {
+            BinaryEntryType::Iface => Ok(BinaryEntrySection {
+                hdr: section.hdr,
+                inner: Self::read_section(tdev, iomem, &mut entry_cursor, fw, vm.clone())?,
+            }),
+
+            BinaryEntryType::BuildInfoMetadata => {
+                // TODO: Read build metadata
+                Ok(section)
+            }
+
+            BinaryEntryType::Config
+            | BinaryEntryType::FutfTest
+            | BinaryEntryType::TraceBuffer
+            | BinaryEntryType::TimelineMetadata => Ok(section),
+
+            _ => {
+                if !section.hdr.optional() {
+                    dev_info!(
+                        tdev.as_ref(),
+                        "Unsupported non-optional entry type: {}",
+                        entry_ty as u32
+                    );
+
+                    Err(EINVAL)
+                } else {
+                    dev_info!(
+                        tdev.as_ref(),
+                        "Skipping unsupported firmware entry type: {}",
+                        entry_ty as u32
+                    );
+
+                    Ok(section)
+                }
+            }
+        }
+    }
+
+    fn read_section(
+        tdev: &TyrDevice,
+        iomem: Arc<Devres<IoMem>>,
+        cursor: &mut Cursor<'_>,
+        fw: &kernel::firmware::Firmware,
+        vm: Arc<Mutex<Vm>>,
+    ) -> Result<Option<Section>> {
+        let hdr = BinarySectionEntryHeader::new(tdev, cursor)?;
+
+        if hdr.flags.contains(flags::PROT) {
+            dev_warn!(
+                tdev.as_ref(),
+                "Firmware protected mode entry not supported, ignoring"
+            );
+            return Ok(None);
+        }
+
+        if hdr.va.start == CSF_MCU_SHARED_REGION_START && !hdr.flags.contains(flags::SHARED) {
+            dev_err!(
+                tdev.as_ref(),
+                "Interface at 0x{:x} must be shared",
+                CSF_MCU_SHARED_REGION_START
+            );
+            return Err(EINVAL);
+        }
+
+        let name_len = cursor.len() - cursor.pos();
+        let name_bytes = cursor.read(tdev, name_len)?;
+
+        let mut name = KVec::with_capacity(name_bytes.len() + 1, GFP_KERNEL)?;
+        name.extend_from_slice(name_bytes, GFP_KERNEL)?;
+        name.push(0, GFP_KERNEL)?;
+
+        let name = CStr::from_bytes_with_nul(&name)
+            .ok()
+            .and_then(|name| CString::try_from(name).ok());
+
+        let fw = fw.data();
+        let section_start = hdr.data.start as usize;
+        let section_end = hdr.data.end as usize;
+
+        let mut data = KVec::new();
+        data.extend_from_slice(&fw[section_start..section_end], GFP_KERNEL)?;
+
+        let bo_len = (hdr.va.end - hdr.va.start) as usize;
+
+        let cache_mode = hdr.flags.cache_mode();
+
+        let mut vm_map_flags = vm::map_flags::Flags::empty();
+
+        if !hdr.flags.contains(flags::WRITE) {
+            vm_map_flags |= vm::map_flags::READONLY;
+        }
+        if !hdr.flags.contains(flags::EXEC) {
+            vm_map_flags |= vm::map_flags::NOEXEC;
+        }
+        if cache_mode != flags::CACHE_MODE_CACHED.into() {
+            vm_map_flags |= vm::map_flags::UNCACHED;
+        }
+
+        let mut mem = gem::new_kernel_object(
+            tdev,
+            iomem,
+            vm,
+            KernelVaPlacement::At(hdr.va.start as u64..hdr.va.end as u64),
+            vm_map_flags,
+        )?;
+
+        let vmap = mem.vmap()?;
+        let vmap = vmap.as_mut_slice();
+
+        vmap[0..data.len()].copy_from_slice(&data);
+
+        if hdr.flags.contains(flags::ZERO) {
+            vmap[data.len()..].fill(0);
+        }
+
+        dev_info!(
+            tdev.as_ref(),
+            "Copied firmware data to BO {:p} of size {} with flags {}\n",
+            &mem.gem,
+            bo_len,
+            vm_map_flags
+        );
+
+        Ok(Some(Section {
+            flags: hdr.flags,
+            name,
+            data,
+            mem,
+            va: hdr.va,
+            vm_map_flags,
+        }))
+    }
+
+    fn read_build_info(cursor: &mut Cursor<'_>, tdev: &TyrDevice) -> Result<()> {
+        let meta_start = cursor.read_u32(tdev)? as usize;
+        let meta_end = cursor.read_u32(tdev)? as usize;
+
+        let expected_hdr = b"git_sha: ";
+        let hdr = cursor.read(tdev, expected_hdr.len())?;
+
+        if hdr != expected_hdr {
+            dev_warn!(tdev.as_ref(), "Firmware's git sha is missing\n");
+            return Ok(());
+        }
+
+        let sz = meta_end - meta_start - expected_hdr.len();
+        let sha = cursor.read(tdev, sz)?;
+        if sha[sha.len()] != 0 {
+            dev_warn!(tdev.as_ref(), "Firmware's git sha is not NULL terminated\n");
+            return Ok(()); // Don't treat as fatal
+        }
+
+        let sha = CStr::from_bytes_with_nul(sha).unwrap_or(c_str!(""));
+        dev_info!(
+            tdev.as_ref(),
+            "Firmware git sha: {}\n",
+            sha.to_str().unwrap()
+        );
+
+        Ok(())
+    }
+}
diff --git a/drivers/gpu/drm/tyr/fw/parse/cursor.rs b/drivers/gpu/drm/tyr/fw/parse/cursor.rs
new file mode 100644
index 0000000000..dcb94f81cf
--- /dev/null
+++ b/drivers/gpu/drm/tyr/fw/parse/cursor.rs
@@ -0,0 +1,91 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! A bare-bones std::io::Cursor<[u8]> clone to keep track of the current
+//! position in the firmware binary.
+
+use core::ops::Range;
+
+use kernel::prelude::*;
+
+use crate::driver::TyrDevice;
+
+pub(crate) struct Cursor<'a> {
+    data: &'a [u8],
+    pos: usize,
+}
+
+impl<'a> Cursor<'a> {
+    pub(crate) fn new(data: &'a [u8]) -> Self {
+        Self { data, pos: 0 }
+    }
+
+    pub(super) fn len(&self) -> usize {
+        self.data.len()
+    }
+
+    pub(super) fn pos(&self) -> usize {
+        self.pos
+    }
+
+    pub(super) fn advance(&mut self, nbytes: usize) -> Result {
+        if self.pos + nbytes > self.data.len() {
+            return Err(EINVAL);
+        }
+
+        self.pos += nbytes;
+        Ok(())
+    }
+
+    /// Returns a view into the cursor's data.
+    ///
+    /// This spawns a new cursor, leaving the current cursor unchanged.
+    pub(super) fn view(&self, range: Range<usize>) -> Result<Cursor<'_>> {
+        if range.start < self.pos || range.end > self.data.len() {
+            pr_err!(
+                "Invalid cursor range {:?} for data of length {}",
+                range,
+                self.data.len()
+            );
+
+            Err(EINVAL)
+        } else {
+            Ok(Self {
+                data: &self.data[range],
+                pos: 0,
+            })
+        }
+    }
+
+    pub(super) fn read(&mut self, tdev: &TyrDevice, nbytes: usize) -> Result<&[u8]> {
+        let start = self.pos;
+        let end = start + nbytes;
+
+        if end > self.data.len() {
+            dev_err!(
+                tdev.as_ref(),
+                "Invalid firmware file: read of size {} at position {} is out of bounds",
+                nbytes,
+                start,
+            );
+            return Err(EINVAL);
+        }
+
+        self.pos += nbytes;
+        Ok(&self.data[start..end])
+    }
+
+    pub(super) fn read_u8(&mut self, tdev: &TyrDevice) -> Result<u8> {
+        let bytes = self.read(tdev, 1)?;
+        Ok(bytes[0])
+    }
+
+    pub(super) fn read_u16(&mut self, tdev: &TyrDevice) -> Result<u16> {
+        let bytes = self.read(tdev, 2)?;
+        Ok(u16::from_le_bytes(bytes.try_into().unwrap()))
+    }
+
+    pub(super) fn read_u32(&mut self, tdev: &TyrDevice) -> Result<u32> {
+        let bytes = self.read(tdev, 4)?;
+        Ok(u32::from_le_bytes(bytes.try_into().unwrap()))
+    }
+}
diff --git a/drivers/gpu/drm/tyr/gem.rs b/drivers/gpu/drm/tyr/gem.rs
new file mode 100644
index 0000000000..8388914b03
--- /dev/null
+++ b/drivers/gpu/drm/tyr/gem.rs
@@ -0,0 +1,229 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use core::ops::Range;
+
+use crate::driver::TyrDevice;
+use crate::driver::TyrDriver;
+use crate::file::DrmFile;
+use crate::mmu::vm;
+use crate::mmu::vm::Vm;
+use kernel::devres::Devres;
+use kernel::drm::gem::shmem;
+use kernel::drm::gem::BaseObject;
+use kernel::drm::gem::{self};
+use kernel::drm::mm;
+use kernel::io::mem::IoMem;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::sync::Mutex;
+use kernel::types::ARef;
+
+/// GEM Object inner driver data
+#[pin_data]
+pub(crate) struct DriverObject {
+    /// Whether this is a kernel or user BO.
+    ty: ObjectType,
+
+    /// The flags received at BO creation time.
+    flags: u32,
+}
+
+enum ObjectType {
+    Kernel {
+        // Kernel objects have their VA managed by the MM allocator. This node
+        // represents the allocation.
+        node: mm::Node<(), ()>,
+    },
+
+    User,
+}
+
+/// Type alias for the GEM object type for this driver.
+pub(crate) type Object = gem::shmem::Object<DriverObject>;
+
+pub struct GemArgs {
+    ty: ObjectType,
+    flags: u32,
+}
+
+#[vtable]
+impl gem::BaseDriverObject for DriverObject {
+    type Driver = TyrDriver;
+    type Object = gem::shmem::Object<Self>;
+    type Args = GemArgs;
+
+    fn new(dev: &TyrDevice, _size: usize, args: Self::Args) -> impl PinInit<Self, Error> {
+        dev_dbg!(dev.as_ref(), "DriverObject::new\n");
+        DriverObject {
+            ty: args.ty,
+            flags: args.flags,
+        }
+    }
+}
+
+// impl gem::shmem::DriverObject for DriverObject {
+//     type Driver = TyrDriver;
+// }
+
+/// A shared reference to a GEM object for this driver.
+pub(crate) struct ObjectRef {
+    /// The underlying GEM object reference
+    pub(crate) gem: ARef<shmem::Object<DriverObject>>,
+
+    /// The kernel-side VMap of this object, if any.
+    vmap: Option<shmem::VMap<DriverObject>>,
+}
+
+impl ObjectRef {
+    /// Create a new wrapper for a raw GEM object reference.
+    pub(crate) fn new(gem: ARef<shmem::Object<DriverObject>>) -> ObjectRef {
+        ObjectRef { gem, vmap: None }
+    }
+
+    /// Return the `VMap` for this object, creating it if necessary.
+    pub(crate) fn vmap(&mut self) -> Result<&mut shmem::VMap<DriverObject>> {
+        if self.vmap.is_none() {
+            self.vmap = Some(self.gem.vmap()?);
+        }
+        Ok(self.vmap.as_mut().unwrap())
+    }
+
+    /// Returns the size of an object in bytes
+    pub(crate) fn size(&self) -> usize {
+        self.gem.size()
+    }
+
+    /// Returns the range occupied by this object in the kernel VA space, if
+    /// any.
+    pub(crate) fn kernel_va(&self) -> Option<Range<u64>> {
+        match &self.gem.ty {
+            ObjectType::Kernel { node } => Some(node.start()..node.start() + node.size()),
+            ObjectType::User => None,
+        }
+    }
+}
+
+type ObjectConfig<'a> = shmem::ObjectConfig<'a, DriverObject>;
+
+/// Create a new DRM GEM object.
+pub(crate) fn new_object(dev: &TyrDevice, size: usize, flags: u32) -> Result<ObjectRef> {
+    let aligned_size = size.next_multiple_of(1 << 12);
+
+    if size == 0 || size > aligned_size {
+        return Err(EINVAL);
+    }
+
+    let gem = Object::new(
+        dev,
+        aligned_size,
+        ObjectConfig {
+            map_wc: true,
+            parent_resv_obj: None,
+        },
+        GemArgs {
+            ty: ObjectType::User,
+            flags,
+        },
+    )?;
+
+    // TODO: This is really bad but at this point seems to be the only way:
+    // to be refactored
+    // SAFETY: We are the only owners at this point
+    let mut obj = ARef::<kernel::drm::gem::shmem::Object<DriverObject>>::into_raw(gem);
+    unsafe { obj.as_mut().flags = flags };
+
+    let gem = unsafe { ARef::<kernel::drm::gem::shmem::Object<DriverObject>>::from_raw(obj) };
+
+    Ok(ObjectRef::new(gem))
+}
+
+/// Look up a GEM object handle for a `File` and return an `ObjectRef` for it.
+pub(crate) fn lookup_handle(file: &DrmFile, handle: u32) -> Result<ObjectRef> {
+    Ok(ObjectRef::new(shmem::Object::lookup_handle(file, handle)?))
+}
+
+/// Create a new kernel-owned GEM object.
+pub(crate) fn new_kernel_object(
+    tdev: &TyrDevice,
+    iomem: Arc<Devres<IoMem>>,
+    vm: Arc<Mutex<Vm>>,
+    mut va: KernelVaPlacement,
+    flags: vm::map_flags::Flags,
+) -> Result<ObjectRef> {
+    va.align()?;
+    let sz = va.size();
+    let node = vm.lock().alloc_kernel_range(va)?;
+    let range = node.start()..node.start() + node.size();
+
+    let gem = Object::new(
+        tdev,
+        sz,
+        ObjectConfig {
+            map_wc: true,
+            parent_resv_obj: None,
+        },
+        GemArgs {
+            ty: ObjectType::Kernel { node },
+            flags: 0,
+        },
+    )?;
+
+    vm.lock().bind_gem(iomem, &gem, 0, range, flags)?;
+
+    Ok(ObjectRef::new(gem))
+}
+
+/// Creates a dummy GEM object to serve as the root of a GPUVM.
+pub(crate) fn new_dummy_object(tdev: &TyrDevice) -> Result<ObjectRef> {
+    let gem = Object::new(
+        tdev,
+        4096,
+        ObjectConfig {
+            map_wc: true,
+            parent_resv_obj: None,
+        },
+        GemArgs {
+            ty: ObjectType::User,
+            flags: 0,
+        },
+    )?;
+
+    Ok(ObjectRef::new(gem))
+}
+
+/// Controls the VA range assigned to a kernel-owned GEM object.
+pub(crate) enum KernelVaPlacement {
+    /// Automatically place this object in a free spot in the kernel VA range.
+    Auto { size: usize },
+    /// Place this object at a given address.
+    At(Range<u64>),
+}
+
+impl KernelVaPlacement {
+    pub(crate) fn size(&self) -> usize {
+        match self {
+            KernelVaPlacement::Auto { size } => *size,
+            KernelVaPlacement::At(range) => (range.end - range.start) as usize,
+        }
+    }
+
+    pub(crate) fn align(&mut self) -> Result {
+        match self {
+            KernelVaPlacement::Auto { size } => {
+                *size = size.next_multiple_of(1 << 12);
+            }
+            KernelVaPlacement::At(range) => {
+                if range.start % (1 << 12) != 0 {
+                    pr_err!(
+                        "Invalid range for kernel VA placement: {:#x}..{:#x}",
+                        range.start,
+                        range.end
+                    );
+                    return Err(EINVAL);
+                }
+            }
+        }
+
+        Ok(())
+    }
+}
diff --git a/drivers/gpu/drm/tyr/gpu.rs b/drivers/gpu/drm/tyr/gpu.rs
new file mode 100644
index 0000000000..10157074de
--- /dev/null
+++ b/drivers/gpu/drm/tyr/gpu.rs
@@ -0,0 +1,212 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use crate::regs::*;
+use kernel::bits;
+use kernel::bits::genmask_u32;
+use kernel::devres::Devres;
+use kernel::io;
+use kernel::io::mem::IoMem;
+use kernel::platform;
+use kernel::prelude::*;
+use kernel::time;
+use kernel::transmute::AsBytes;
+
+pub(crate) mod irq;
+
+#[repr(C)]
+// This can be queried by userspace to get information about the GPU.
+pub(crate) struct GpuInfo {
+    pub(crate) gpu_id: u32,
+    pub(crate) csf_id: u32,
+    pub(crate) gpu_rev: u32,
+    pub(crate) core_features: u32,
+    pub(crate) l2_features: u32,
+    pub(crate) tiler_features: u32,
+    pub(crate) mem_features: u32,
+    pub(crate) mmu_features: u32,
+    pub(crate) thread_features: u32,
+    pub(crate) max_threads: u32,
+    pub(crate) thread_max_workgroup_size: u32,
+    pub(crate) thread_max_barrier_size: u32,
+    pub(crate) coherency_features: u32,
+    pub(crate) texture_features: [u32; 4],
+    pub(crate) as_present: u32,
+    pub(crate) shader_present: u64,
+    pub(crate) tiler_present: u64,
+    pub(crate) l2_present: u64,
+}
+
+impl GpuInfo {
+    pub(crate) fn new(iomem: &Devres<IoMem>) -> Result<Self> {
+        let gpu_id = GPU_ID.read(iomem)?;
+        let csf_id = GPU_CSF_ID.read(iomem)?;
+        let gpu_rev = GPU_REVID.read(iomem)?;
+        let core_features = GPU_CORE_FEATURES.read(iomem)?;
+        let l2_features = GPU_L2_FEATURES.read(iomem)?;
+        let tiler_features = GPU_TILER_FEATURES.read(iomem)?;
+        let mem_features = GPU_MEM_FEATURES.read(iomem)?;
+        let mmu_features = GPU_MMU_FEATURES.read(iomem)?;
+        let thread_features = GPU_THREAD_FEATURES.read(iomem)?;
+        let max_threads = GPU_THREAD_MAX_THREADS.read(iomem)?;
+        let thread_max_workgroup_size = GPU_THREAD_MAX_WORKGROUP_SIZE.read(iomem)?;
+        let thread_max_barrier_size = GPU_THREAD_MAX_BARRIER_SIZE.read(iomem)?;
+        let coherency_features = GPU_COHERENCY_FEATURES.read(iomem)?;
+
+        let texture_features = GPU_TEXTURE_FEATURES0.read(iomem)?;
+
+        let as_present = GPU_AS_PRESENT.read(iomem)?;
+
+        let shader_present = GPU_SHADER_PRESENT_LO.read(iomem)? as u64;
+        let shader_present = shader_present | (GPU_SHADER_PRESENT_HI.read(iomem)? as u64) << 32;
+
+        let tiler_present = GPU_TILER_PRESENT_LO.read(iomem)? as u64;
+        let tiler_present = tiler_present | (GPU_TILER_PRESENT_HI.read(iomem)? as u64) << 32;
+
+        let l2_present = GPU_L2_PRESENT_LO.read(iomem)? as u64;
+        let l2_present = l2_present | (GPU_L2_PRESENT_HI.read(iomem)? as u64) << 32;
+
+        Ok(Self {
+            gpu_id,
+            csf_id,
+            gpu_rev,
+            core_features,
+            l2_features,
+            tiler_features,
+            mem_features,
+            mmu_features,
+            thread_features,
+            max_threads,
+            thread_max_workgroup_size,
+            thread_max_barrier_size,
+            coherency_features,
+            texture_features: [texture_features, 0, 0, 0],
+            as_present,
+            shader_present,
+            tiler_present,
+            l2_present,
+        })
+    }
+
+    pub(crate) fn log(&self, pdev: &platform::Device) {
+        let major = (self.gpu_id >> 16) & 0xff;
+        let minor = (self.gpu_id >> 8) & 0xff;
+        let status = self.gpu_id & 0xff;
+
+        let model_name = if let Some(model) = GPU_MODELS
+            .iter()
+            .find(|&f| f.major == major && f.minor == minor)
+        {
+            model.name
+        } else {
+            "unknown"
+        };
+
+        dev_info!(
+            pdev.as_ref(),
+            "mali-{} id 0x{:x} major 0x{:x} minor 0x{:x} status 0x{:x}",
+            model_name,
+            self.gpu_id >> 16,
+            major,
+            minor,
+            status
+        );
+
+        dev_info!(
+            pdev.as_ref(),
+            "Features: L2:{:#x} Tiler:{:#x} Mem:{:#x} MMU:{:#x} AS:{:#x}",
+            self.l2_features,
+            self.tiler_features,
+            self.mem_features,
+            self.mmu_features,
+            self.as_present
+        );
+
+        dev_info!(
+            pdev.as_ref(),
+            "shader_present=0x{:016x} l2_present=0x{:016x} tiler_present=0x{:016x}",
+            self.shader_present,
+            self.l2_present,
+            self.tiler_present
+        );
+    }
+
+    pub(crate) fn va_bits(&self) -> u32 {
+        self.mmu_features & bits::genmask_u32(7, 0)
+    }
+
+    pub(crate) fn pa_bits(&self) -> u32 {
+        (self.mmu_features >> 8) & bits::genmask_u32(7, 0)
+    }
+}
+
+// SAFETY:
+//
+// This type is the same type exposed by Panthor's uAPI. As it's declared as
+// #repr(C), we can be sure that the layout is the same. Therefore, it is safe
+// to expose this to userspace.
+unsafe impl AsBytes for GpuInfo {}
+
+struct GpuModels {
+    name: &'static str,
+    major: u32,
+    minor: u32,
+}
+
+const GPU_MODELS: [GpuModels; 1] = [GpuModels {
+    name: "g610",
+    major: 10,
+    minor: 7,
+}];
+
+#[allow(dead_code)]
+pub(crate) struct GpuId {
+    pub(crate) arch_major: u32,
+    pub(crate) arch_minor: u32,
+    pub(crate) arch_rev: u32,
+    pub(crate) prod_major: u32,
+    pub(crate) ver_major: u32,
+    pub(crate) ver_minor: u32,
+    pub(crate) ver_status: u32,
+}
+
+impl From<u32> for GpuId {
+    fn from(value: u32) -> Self {
+        GpuId {
+            arch_major: (value & genmask_u32(31, 28)) >> 28,
+            arch_minor: (value & genmask_u32(27, 24)) >> 24,
+            arch_rev: (value & genmask_u32(23, 20)) >> 20,
+            prod_major: (value & genmask_u32(19, 16)) >> 16,
+            ver_major: (value & genmask_u32(15, 12)) >> 12,
+            ver_minor: (value & genmask_u32(11, 4)) >> 4,
+            ver_status: value & genmask_u32(3, 0),
+        }
+    }
+}
+
+/// Powers on the l2 block.
+pub(crate) fn l2_power_on(iomem: &Devres<IoMem>) -> Result<()> {
+    let op = || L2_PWRTRANS_LO.read(iomem);
+
+    let cond = |pwr_trans: &u32| *pwr_trans == 0;
+
+    let _ = io::poll::read_poll_timeout(
+        op,
+        cond,
+        time::Delta::from_millis(100),
+        Some(time::Delta::from_millis(200)),
+    )?;
+
+    L2_PWRON_LO.write(iomem, 1)?;
+
+    let op = || L2_READY_LO.read(iomem);
+    let cond = |l2_ready: &u32| *l2_ready == 1;
+
+    let _ = io::poll::read_poll_timeout(
+        op,
+        cond,
+        time::Delta::from_millis(100),
+        Some(time::Delta::from_millis(200)),
+    )?;
+
+    Ok(())
+}
diff --git a/drivers/gpu/drm/tyr/gpu/irq.rs b/drivers/gpu/drm/tyr/gpu/irq.rs
new file mode 100644
index 0000000000..14a3ee9d11
--- /dev/null
+++ b/drivers/gpu/drm/tyr/gpu/irq.rs
@@ -0,0 +1,80 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! GPU IRQ handler.
+
+use kernel::c_str;
+use kernel::devres::Devres;
+use kernel::io::mem::IoMem;
+use kernel::irq::ThreadedRegistration;
+use kernel::platform;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::types::ARef;
+
+use crate::driver::TyrDevice;
+use crate::driver::TyrIrq;
+use crate::driver::TyrIrqTrait;
+use crate::regs;
+use crate::wait::Wait;
+
+pub(crate) struct GpuIrq {
+    iomem: Arc<Devres<IoMem>>,
+    power_on_wait: Arc<Wait<bool>>,
+}
+
+pub(crate) fn gpu_irq_init<'a>(
+    tdev: ARef<TyrDevice>,
+    pdev: &'a platform::Device<kernel::device::Bound>,
+    iomem: Arc<Devres<IoMem>>,
+    power_on_wait: Arc<Wait<bool>>,
+) -> Result<impl PinInit<ThreadedRegistration<TyrIrq<GpuIrq>>, Error> + 'a> {
+    crate::regs::GPU_INT_MASK.write(&iomem, u32::MAX)?;
+
+    let irq_type = GpuIrq {
+        iomem: iomem.clone(),
+        power_on_wait,
+    };
+
+    TyrIrq::request(pdev, tdev, c_str!("gpu"), irq_type)
+}
+
+impl TyrIrqTrait for GpuIrq {
+    fn read_status(&self) -> u32 {
+        regs::GPU_INT_STAT.read(&self.iomem).unwrap_or_default()
+    }
+
+    fn disable_all(&self) {
+        let _ = regs::GPU_INT_MASK.write(&self.iomem, 0);
+    }
+
+    fn reenable(&self) {
+        let _ = regs::GPU_INT_MASK.write(&self.iomem, self.mask());
+    }
+
+    fn read_raw_status(&self) -> u32 {
+        regs::GPU_INT_RAWSTAT.read(&self.iomem).unwrap_or_default()
+    }
+
+    fn clear_status(&self, status: u32) {
+        let _ = regs::GPU_INT_CLEAR.write(&self.iomem, status);
+    }
+
+    fn mask(&self) -> u32 {
+        u32::MAX
+    }
+
+    fn handle(&self, _: &TyrDevice, status: u32) {
+        if status
+            == regs::GPU_INT_RAWSTAT_RESET_COMPLETED
+                | regs::GPU_INT_RAWSTAT_POWER_CHANGED_SINGLE
+                | regs::GPU_INT_RAWSTAT_POWER_CHANGED_ALL
+        {
+            let _ = self.power_on_wait.with_locked_data(|powered_on| {
+                *powered_on = true;
+                Ok(())
+            });
+
+            self.power_on_wait.notify_all();
+        }
+    }
+}
diff --git a/drivers/gpu/drm/tyr/mmu.rs b/drivers/gpu/drm/tyr/mmu.rs
new file mode 100644
index 0000000000..601facca50
--- /dev/null
+++ b/drivers/gpu/drm/tyr/mmu.rs
@@ -0,0 +1,212 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use core::ops::Range;
+
+use as_lock::AsLockToken;
+use faults::decode_faults;
+use kernel::devres::Devres;
+use kernel::io;
+use kernel::io::mem::IoMem;
+use kernel::io_pgtable;
+use kernel::new_mutex;
+use kernel::platform;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::sync::Mutex;
+use kernel::time::Delta;
+use kernel::types::ForeignOwnable;
+use vm::Vm;
+use vm::VmLayout;
+
+use crate::driver::TyrDevice;
+use crate::gpu::GpuInfo;
+use crate::regs::*;
+
+mod as_lock;
+mod faults;
+pub(crate) mod irq;
+pub(crate) mod vm;
+
+pub(crate) struct Mmu {
+    /// List containing all VMs.
+    vms: KVec<Arc<Mutex<Vm>>>,
+    /// Tracks which of the 32 AS slots are free.
+    free_slots: usize,
+    // slot_allocator: Arc<Mutex<SlotAllocator>>,
+}
+
+impl Mmu {
+    pub(crate) fn new() -> Result<Self> {
+        Ok(Self {
+            vms: KVec::new(),
+            // slot_allocator: Arc::pin_init(
+            //     new_mutex!(SlotAllocator {
+            //         free_mask: u32::MAX,
+            //     }),
+            //     GFP_KERNEL,
+            // )?,
+            free_slots: usize::MAX & !1,
+        })
+    }
+
+    pub(crate) fn create_vm(
+        &mut self,
+        tdev: &TyrDevice,
+        pdev: &platform::Device,
+        gpu_info: &GpuInfo,
+        for_mcu: bool,
+        layout: VmLayout,
+        auto_kernel_va: Range<u64>,
+        /* coherent: bool, */
+    ) -> Result<Arc<Mutex<Vm>>> {
+        let vm = Vm::create(tdev, pdev, for_mcu, gpu_info, layout, auto_kernel_va)?;
+
+        let vm = Arc::pin_init(new_mutex!(vm), GFP_KERNEL)?;
+        self.vms.push(vm.clone(), GFP_KERNEL)?;
+        Ok(vm)
+    }
+
+    fn flush_range(iomem: &Devres<IoMem>, as_nr: usize, range: Range<u64>) -> Result {
+        Self::do_as_command(iomem, as_nr, AS_COMMAND_FLUSH_PT, range)
+    }
+
+    fn allocate_as(&mut self) -> Result<usize> {
+        let slot = self.free_slots.trailing_zeros();
+        if slot == 32 {
+            return Err(EBUSY);
+        }
+
+        self.free_slots |= 1 << slot;
+        Ok(slot as usize)
+    }
+
+    fn wait_ready(iomem: &Devres<IoMem>, as_nr: usize) -> Result {
+        let op = || as_status(as_nr)?.read(iomem);
+        let cond = |status: &u32| -> bool { *status & AS_STATUS_ACTIVE == 0 };
+        let _ = io::poll::read_poll_timeout(
+            op,
+            cond,
+            Delta::from_millis(0),
+            Some(Delta::from_micros(10000)),
+        )?;
+
+        Ok(())
+    }
+
+    /// TODO: The code to manage AS slots is still TODO.
+    #[allow(unused)]
+    fn free_as(&mut self, as_nr: usize) {
+        self.free_slots &= !(1 << as_nr);
+    }
+
+    fn do_as_command(
+        iomem: &Devres<IoMem>,
+        as_nr: usize,
+        command: u32,
+        region: Range<u64>,
+    ) -> Result {
+        if command == AS_COMMAND_UNLOCK {
+            as_command(as_nr)?.write(iomem, command)?;
+        } else {
+            let _lock = AsLockToken::lock_region(iomem, as_nr, region)?;
+            Self::wait_ready(iomem, as_nr)?;
+            as_command(as_nr)?.write(iomem, command)?;
+            Self::wait_ready(iomem, as_nr)?;
+        }
+
+        Ok(())
+    }
+
+    pub(crate) fn bind_vm(
+        &mut self,
+        vm: Arc<Mutex<Vm>>,
+        gpu_info: &GpuInfo,
+        iomem: &Devres<IoMem>,
+    ) -> Result {
+        let mut vm = vm.lock();
+        let va_bits = gpu_info.va_bits();
+
+        // stack_pin_init!(let local_guard = new_mutex!(()));
+        // let locked_vm = vm.gpuvm.lock(&mut local_guard.lock());
+
+        let transtab = vm.gpuvm.page_table.cfg().ttbr;
+        let transcfg = AS_TRANSCFG_PTW_MEMATTR_WB
+            | AS_TRANSCFG_PTW_RA
+            | AS_TRANSCFG_ADRMODE_AARCH64_4K
+            | as_transcfg_ina_bits((55 - va_bits).into());
+
+        let memattr = vm.memattr;
+        let as_nr = if vm.for_mcu { 0 } else { self.allocate_as()? };
+
+        Self::enable_as(iomem, as_nr, transtab, transcfg, memattr)?;
+
+        vm.address_space = Some(as_nr);
+        Ok(())
+    }
+
+    fn enable_as(
+        iomem: &Devres<IoMem>,
+        as_nr: usize,
+        transtab: u64,
+        transcfg: u64,
+        memattr: u64,
+    ) -> Result {
+        let active = as_status(as_nr)?.read(iomem)? & AS_STATUS_ACTIVE != 0;
+        if active {
+            return Err(EBUSY);
+        }
+
+        Self::do_as_command(iomem, as_nr, AS_COMMAND_FLUSH_MEM, 0..u64::MAX)?;
+
+        let transtab_lo = (transtab & 0xffffffff) as u32;
+        let transtab_hi = (transtab >> 32) as u32;
+
+        let transcfg_lo = (transcfg & 0xffffffff) as u32;
+        let transcfg_hi = (transcfg >> 32) as u32;
+
+        let memattr_lo = (memattr & 0xffffffff) as u32;
+        let memattr_hi = (memattr >> 32) as u32;
+
+        as_transtab_lo(as_nr)?.write(iomem, transtab_lo)?;
+        as_transtab_hi(as_nr)?.write(iomem, transtab_hi)?;
+
+        as_transcfg_lo(as_nr)?.write(iomem, transcfg_lo)?;
+        as_transcfg_hi(as_nr)?.write(iomem, transcfg_hi)?;
+
+        as_memattr_lo(as_nr)?.write(iomem, memattr_lo)?;
+        as_memattr_hi(as_nr)?.write(iomem, memattr_hi)?;
+
+        as_command(as_nr)?.write(iomem, AS_COMMAND_UPDATE)?;
+
+        let op = || as_status(as_nr)?.read(iomem);
+        let cond = |status: &u32| -> bool { *status & AS_STATUS_ACTIVE == 0 };
+        let _ = io::poll::read_poll_timeout(
+            op,
+            cond,
+            Delta::from_millis(0),
+            Some(Delta::from_micros(200)),
+        )?;
+
+        Ok(())
+    }
+}
+
+/* dummy TLB ops, the real TLB flush happens in panthor_vm_flush_range() */
+impl io_pgtable::FlushOps for Mmu {
+    type Data = ();
+
+    fn tlb_flush_all(_data: <Self::Data as ForeignOwnable>::Borrowed<'_>) {}
+    fn tlb_flush_walk(
+        _data: <Self::Data as ForeignOwnable>::Borrowed<'_>,
+        _iova: usize,
+        _size: usize,
+        _granule: usize,
+    ) {
+    }
+    fn tlb_add_page(
+        _data: <Self::Data as ForeignOwnable>::Borrowed<'_>,
+        _iova: usize,
+        _granule: usize,
+    ) {
+    }
+}
diff --git a/drivers/gpu/drm/tyr/mmu/as_lock.rs b/drivers/gpu/drm/tyr/mmu/as_lock.rs
new file mode 100644
index 0000000000..dd3ed25374
--- /dev/null
+++ b/drivers/gpu/drm/tyr/mmu/as_lock.rs
@@ -0,0 +1,89 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! Address space locking.
+
+use core::ops::Range;
+
+use kernel::bits::genmask_u64;
+use kernel::devres::Devres;
+use kernel::io::mem::IoMem;
+use kernel::prelude::*;
+
+use crate::mmu::Mmu;
+use crate::regs::*;
+
+/// A token type that represents a lock on a region of a given address space.
+pub(super) struct AsLockToken<'a> {
+    iomem: &'a Devres<IoMem>,
+    as_nr: usize,
+}
+
+impl<'a> AsLockToken<'a> {
+    /// Lock a `region` of `as_nr`.
+    pub(super) fn lock_region(
+        iomem: &'a Devres<IoMem>,
+        as_nr: usize,
+        region: Range<u64>,
+    ) -> Result<Self> {
+        if region.end - region.start == 0 {
+            return Err(EINVAL);
+        }
+
+        // The locked region is a naturally aligned power of 2 block encoded as
+        // log2 minus(1).
+        //
+        // Calculate the desired start/end and look for the highest bit which
+        // differs. The smallest naturally aligned block must include this bit
+        // change, the desired region starts with this bit (and subsequent bits)
+        // zeroed and ends with the bit (and subsequent bits) set to one.
+        let region_width = core::cmp::max(
+            (region.start ^ (region.end - 1)).leading_zeros() as u8,
+            64 - AS_LOCK_REGION_MIN_SIZE.trailing_zeros() as u8,
+        ) - 1;
+
+        // Mask off the low bits of region.start, which would be ignored by the
+        // hardware anyways.
+        let region_start = region.start & genmask_u64(63, region_width as u32);
+
+        let region = (region_width as u64) | region_start;
+
+        let region_lo = (region & 0xffffffff) as u32;
+        let region_hi = (region >> 32) as u32;
+
+        // Lock the region that needs to be updated.
+        as_lockaddr_lo(as_nr)?.write(iomem, region_lo)?;
+        as_lockaddr_hi(as_nr)?.write(iomem, region_hi)?;
+        as_command(as_nr)?.write(iomem, AS_COMMAND_LOCK)?;
+
+        Ok(Self { iomem, as_nr })
+    }
+}
+
+impl Drop for AsLockToken<'_> {
+    fn drop(&mut self) {
+        let as_cmd = as_command(self.as_nr);
+        match as_cmd {
+            Ok(as_cmd) => {
+                if let Err(err) = Mmu::wait_ready(self.iomem, self.as_nr) {
+                    pr_err!("MMU is busy for AS{}: {:?}\n", self.as_nr, err);
+                    return;
+                }
+                if let Err(err) = as_cmd.write(self.iomem, AS_COMMAND_FLUSH_PT) {
+                    pr_err!(
+                        "Failed to flush page tables for AS{}: {:?}\n",
+                        self.as_nr,
+                        err
+                    );
+                    return;
+                }
+                if let Err(err) = Mmu::wait_ready(self.iomem, self.as_nr) {
+                    pr_err!("MMU is busy for AS{}: {:?}\n", self.as_nr, err);
+                }
+            }
+
+            Err(err) => {
+                pr_err!("Failed to unlock AS{}: {:?}\n", self.as_nr, err);
+            }
+        }
+    }
+}
diff --git a/drivers/gpu/drm/tyr/mmu/faults.rs b/drivers/gpu/drm/tyr/mmu/faults.rs
new file mode 100644
index 0000000000..a9509648b3
--- /dev/null
+++ b/drivers/gpu/drm/tyr/mmu/faults.rs
@@ -0,0 +1,126 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! Fault reporting.
+
+use crate::regs::*;
+use kernel::c_str;
+use kernel::devres::Devres;
+use kernel::io::mem::IoMem;
+use kernel::prelude::*;
+use kernel::str::CStr;
+
+pub(crate) const EXCEPTION_MAP: &[(u32, &CStr)] = &[
+    (0x00, c_str!("OK")),
+    (0x04, c_str!("TERMINATED")),
+    (0x05, c_str!("KABOOM")),
+    (0x06, c_str!("EUREKA")),
+    (0x08, c_str!("ACTIVE")),
+    (0x0f, c_str!("CS_RES_TERM")),
+    (0x3f, c_str!("MAX_NON_FAULT")),
+    (0x40, c_str!("CS_CONFIG_FAULT")),
+    (0x41, c_str!("CS_UNRECOVERABLE")),
+    (0x44, c_str!("CS_ENDPOINT_FAULT")),
+    (0x48, c_str!("CS_BUS_FAULT")),
+    (0x49, c_str!("CS_INSTR_INVALID")),
+    (0x4a, c_str!("CS_CALL_STACK_OVERFLOW")),
+    (0x4b, c_str!("CS_INHERIT_FAULT")),
+    (0x50, c_str!("INSTR_INVALID_PC")),
+    (0x51, c_str!("INSTR_INVALID_ENC")),
+    (0x55, c_str!("INSTR_BARRIER_FAULT")),
+    (0x58, c_str!("DATA_INVALID_FAULT")),
+    (0x59, c_str!("TILE_RANGE_FAULT")),
+    (0x5a, c_str!("ADDR_RANGE_FAULT")),
+    (0x5b, c_str!("IMPRECISE_FAULT")),
+    (0x60, c_str!("OOM")),
+    (0x68, c_str!("CSF_FW_INTERNAL_ERROR")),
+    (0x69, c_str!("CSF_RES_EVICTION_TIMEOUT")),
+    (0x80, c_str!("GPU_BUS_FAULT")),
+    (0x88, c_str!("GPU_SHAREABILITY_FAULT")),
+    (0x89, c_str!("SYS_SHAREABILITY_FAULT")),
+    (0x8a, c_str!("GPU_CACHEABILITY_FAULT")),
+    (0xc0, c_str!("TRANSLATION_FAULT_0")),
+    (0xc1, c_str!("TRANSLATION_FAULT_1")),
+    (0xc2, c_str!("TRANSLATION_FAULT_2")),
+    (0xc3, c_str!("TRANSLATION_FAULT_3")),
+    (0xc4, c_str!("TRANSLATION_FAULT_4")),
+    (0xc8, c_str!("PERM_FAULT_0")),
+    (0xc9, c_str!("PERM_FAULT_1")),
+    (0xca, c_str!("PERM_FAULT_2")),
+    (0xcb, c_str!("PERM_FAULT_3")),
+    (0xd9, c_str!("ACCESS_FLAG_1")),
+    (0xda, c_str!("ACCESS_FLAG_2")),
+    (0xdb, c_str!("ACCESS_FLAG_3")),
+    (0xe0, c_str!("ADDR_SIZE_FAULT_IN")),
+    (0xe4, c_str!("ADDR_SIZE_FAULT_OUT0")),
+    (0xe5, c_str!("ADDR_SIZE_FAULT_OUT1")),
+    (0xe6, c_str!("ADDR_SIZE_FAULT_OUT2")),
+    (0xe7, c_str!("ADDR_SIZE_FAULT_OUT3")),
+    (0xe8, c_str!("MEM_ATTR_FAULT_0")),
+    (0xe9, c_str!("MEM_ATTR_FAULT_1")),
+    (0xea, c_str!("MEM_ATTR_FAULT_2")),
+    (0xeb, c_str!("MEM_ATTR_FAULT_3")),
+];
+
+pub(crate) fn get_exception_name(code: u32) -> &'static CStr {
+    for &(exception_code, name) in EXCEPTION_MAP {
+        if exception_code == code {
+            return name;
+        }
+    }
+    c_str!("UNKNOWN")
+}
+
+pub(crate) fn access_type_name(fault_status: u32) -> &'static str {
+    match fault_status & AS_FAULTSTATUS_ACCESS_TYPE_MASK {
+        AS_FAULTSTATUS_ACCESS_TYPE_ATOMIC => "ATOMIC",
+        AS_FAULTSTATUS_ACCESS_TYPE_READ => "READ",
+        AS_FAULTSTATUS_ACCESS_TYPE_WRITE => "WRITE",
+        AS_FAULTSTATUS_ACCESS_TYPE_EX => "EXECUTE",
+        _ => "UNKNOWN",
+    }
+}
+
+/// Decodes a MMU fault, printing a message to the kernel log.
+pub(super) fn decode_faults(mut status: u32, iomem: &Devres<IoMem>) -> Result {
+    while status != 0 {
+        let as_index = (status | (status >> 16)).trailing_zeros();
+        let mask = kernel::bits::bit_u32(as_index);
+
+        let mut addr: u64;
+
+        let fault_status: u32 = as_faultstatus(as_index as usize).unwrap().read(iomem)?;
+        addr = as_faultaddress_lo(as_index as usize).unwrap().read(iomem)? as u64;
+        addr |= (as_faultaddress_hi(as_index as usize).unwrap().read(iomem)? as u64) << 32;
+
+        let exception_type: u32 = fault_status & 0xff;
+        let access_type: u32 = (fault_status >> 8) & 0x3;
+        let source_id: u32 = fault_status >> 16;
+
+        pr_err!(
+            "Unhandled Page fault in AS{} at VA 0x{:016X}\n\
+                raw fault status: 0x{:X}\n\
+                decoded fault status: {}\n\
+                exception type 0x{:X}: {}\n\
+                access type 0x{:X}: {}\n\
+                source id 0x{:X}\n",
+            as_index,
+            addr,
+            fault_status,
+            if fault_status & (1 << 10) != 0 {
+                "DECODER FAULT"
+            } else {
+                "SLAVE FAULT"
+            },
+            exception_type,
+            get_exception_name(exception_type),
+            access_type,
+            access_type_name(fault_status),
+            source_id
+        );
+
+        // Update status to process the next fault
+        status &= !mask;
+    }
+
+    Ok(())
+}
diff --git a/drivers/gpu/drm/tyr/mmu/irq.rs b/drivers/gpu/drm/tyr/mmu/irq.rs
new file mode 100644
index 0000000000..cd741f8a62
--- /dev/null
+++ b/drivers/gpu/drm/tyr/mmu/irq.rs
@@ -0,0 +1,70 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! MMU IRQ handler.
+//!
+//! The interrupts return, among many other things, information about faulting
+//! addresses.
+
+use kernel::c_str;
+use kernel::devres::Devres;
+use kernel::io::mem::IoMem;
+use kernel::irq::ThreadedRegistration;
+use kernel::platform;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::types::ARef;
+
+use crate::driver::TyrDevice;
+use crate::driver::TyrIrq;
+use crate::driver::TyrIrqTrait;
+use crate::mmu::decode_faults;
+use crate::regs;
+
+pub(crate) struct MmuIrq {
+    iomem: Arc<Devres<IoMem>>,
+}
+
+pub(crate) fn mmu_irq_init<'a>(
+    tdev: ARef<TyrDevice>,
+    pdev: &'a platform::Device<kernel::device::Bound>,
+    iomem: Arc<Devres<IoMem>>,
+) -> Result<impl PinInit<ThreadedRegistration<TyrIrq<MmuIrq>>, Error> + 'a> {
+    crate::regs::MMU_INT_MASK.write(&iomem, u32::MAX)?;
+
+    let irq_type = MmuIrq {
+        iomem: iomem.clone(),
+    };
+
+    TyrIrq::request(pdev, tdev, c_str!("mmu"), irq_type)
+}
+
+impl TyrIrqTrait for MmuIrq {
+    fn read_status(&self) -> u32 {
+        regs::MMU_INT_STAT.read(&self.iomem).unwrap_or_default()
+    }
+
+    fn disable_all(&self) {
+        let _ = regs::MMU_INT_MASK.write(&self.iomem, 0);
+    }
+
+    fn reenable(&self) {
+        let _ = regs::MMU_INT_MASK.write(&self.iomem, self.mask());
+    }
+
+    fn read_raw_status(&self) -> u32 {
+        regs::MMU_INT_RAWSTAT.read(&self.iomem).unwrap_or_default()
+    }
+
+    fn clear_status(&self, status: u32) {
+        let _ = regs::MMU_INT_CLEAR.write(&self.iomem, status);
+    }
+
+    fn mask(&self) -> u32 {
+        u32::MAX // for now.
+    }
+
+    fn handle(&self, _: &TyrDevice, status: u32) {
+        let status = status & kernel::bits::genmask_u32(15, 0);
+        let _ = decode_faults(status, &self.iomem);
+    }
+}
diff --git a/drivers/gpu/drm/tyr/mmu/slot_allocator.rs b/drivers/gpu/drm/tyr/mmu/slot_allocator.rs
new file mode 100644
index 0000000000..8ba5f8cd3d
--- /dev/null
+++ b/drivers/gpu/drm/tyr/mmu/slot_allocator.rs
@@ -0,0 +1,59 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! All VMs have to be placed on a physical slot to become active. This file
+//! implements an allocator to track which slots are active, and later to evict
+//! the least recently used one if needed.
+//!
+//! Implementing this allocator is a TODO. For now, we just return EBUSY when
+//! all slots are taken, and slots are never freed once inactive.
+
+// /// Alocates HW AS slots, which represent a physical slot where a VM can be
+// /// placed in.
+// ///
+// /// Panthor keeps a LRU list for the purposes of evicting VMs when a slot is
+// /// requested but no one is free. We defer this to a future implementation.
+// ///
+// /// Note that this is still TODO: this type doesn't yet track any VMs.
+// struct SlotAllocator {
+//     /// How many slots are free.
+//     free_mask: u32,
+// }
+
+// impl SlotAllocator {
+//     fn alloc_slot(allocator: Arc<Mutex<Self>>, vm: &mut Vm) {
+//         let mut alloc = allocator.lock();
+//         let slot = alloc.free_mask.trailing_zeros();
+
+//         if slot < 32 {
+//             alloc.free_mask |= 1 << slot;
+//             let slot_allocation = SlotAllocation {
+//                 allocator: allocator.clone(),
+//                 slot: slot as u8,
+//             };
+//             vm.binding = Some(slot_allocation);
+//         }
+//     }
+
+//     fn free_slot(vm: &mut Vm) {
+//         vm.binding = None;
+//     }
+// }
+
+// /// Represents a slot allocation.
+// ///
+// /// This type returns the slot to the allocator once it is dropped.
+// ///
+// ///
+// /// Note that this is still TODO: this type doesn't yet track any VMs.
+// struct SlotAllocation {
+//     /// The allocator that allocated this slot.
+//     allocator: Arc<Mutex<SlotAllocator>>,
+//     /// The actual slot value.
+//     slot: u8,
+// }
+
+// impl Drop for SlotAllocation {
+//     fn drop(&mut self) {
+//         self.allocator.lock().free_mask &= !(1 << self.slot);
+//     }
+// }
diff --git a/drivers/gpu/drm/tyr/mmu/vm.rs b/drivers/gpu/drm/tyr/mmu/vm.rs
new file mode 100644
index 0000000000..b6c96ae354
--- /dev/null
+++ b/drivers/gpu/drm/tyr/mmu/vm.rs
@@ -0,0 +1,380 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! VM management.
+//!
+//! VMs represent a given address space. It provides memory isolation and the
+//! illusion of owning the entire VA range, much like CPU virtual memory.
+//!
+//! VMs can be placed into a hardware slots (i.e.: AS slots), which will make
+//! them active. The number of AS's is limited, and one VM must evict an inactive
+//! one if all slots are taken. In Panthor, this is implemented by keeping a LRU
+//! list, but this is currently not implemented here.
+//!
+//! A VM is assigned an AS by means of a VM_BIND call if the request operation
+//! is OP_MAP.
+//!
+//! If there is no unactive VM to evict, the call to VM_BIND should fail with
+//! EBUSY, but note that slot management is somewhat WIP for now, as we have no
+//! tests for that yet.
+//!
+//! AS0 is special, in the sense that it's the slot used by the firmware's VM.
+//! No other VM can occupy AS0 at any time.
+
+use core::ops::Range;
+
+use gpuvm::LockedVm;
+use gpuvm::StepContext;
+use kernel::bindings::SZ_2M;
+use kernel::c_str;
+use kernel::devres::Devres;
+use kernel::drm::gem::shmem;
+use kernel::drm::gpuvm::ExecToken;
+use kernel::drm::mm;
+use kernel::io::mem::IoMem;
+use kernel::io_pgtable::ARM64LPAES1;
+use kernel::io_pgtable::{self};
+use kernel::platform;
+use kernel::prelude::*;
+use kernel::sizes::SZ_4K;
+use kernel::sync::Arc;
+use kernel::types::ARef;
+
+use crate::driver::TyrDevice;
+use crate::gem;
+use crate::gem::DriverObject;
+use crate::gem::KernelVaPlacement;
+use crate::gpu::GpuInfo;
+use crate::mmu::Mmu;
+use crate::regs;
+
+mod gpuvm;
+pub(crate) mod map_flags;
+pub(crate) mod pool;
+
+// TODO: we need *all* of these in kernel::bindings.
+const SZ_4G: u64 = 4 * kernel::bindings::SZ_1G as u64;
+
+pub(crate) struct Vm {
+    /// A dummy object to serve as GPUVM's root. We need ownership of this.
+    _dummy_obj: ARef<shmem::Object<DriverObject>>,
+
+    pub(super) gpuvm: ARef<kernel::drm::gpuvm::GpuVm<LockedVm>>,
+
+    /// The AS to which this VM is bound, if any.
+    pub(super) address_space: Option<usize>,
+
+    // binding: Option<SlotAllocation>,
+    /// The memory attributes for this VM.
+    pub(super) memattr: u64,
+
+    /// The layout describing how the VM is split between user and kernel space.
+    _layout: VmLayout,
+
+    /// Whether this is the MCU VM.
+    pub(super) for_mcu: bool,
+
+    /// The range to automatically allocate kernel VAs from, if requested.
+    auto_kernel_va: Range<u64>,
+
+    /// Whether this VM was destroyed by userspace.
+    ///
+    /// Destroyed VMs are unmapped and cannot be the target of map operations
+    /// anymore.
+    pub(super) destroyed: bool,
+}
+
+impl Vm {
+    pub(super) fn create(
+        tdev: &TyrDevice,
+        pdev: &platform::Device,
+        for_mcu: bool,
+        gpu_info: &GpuInfo,
+        layout: VmLayout,
+        auto_kernel_va: Range<u64>,
+    ) -> Result<Self> {
+        // We should ideally not allocate memory for this, but there is no way
+        // to create dummy GPUVM GEM objects for now.
+        //
+        // This is being discussed on Zulip. For now we have to waste 4k on
+        // this.
+        let dummy_obj = gem::new_dummy_object(tdev)?;
+
+        let va_bits = gpu_info.va_bits();
+        let pa_bits = gpu_info.pa_bits();
+
+        pr_info!(
+            "Creating VM with VA bits: {}, PA bits: {}\n",
+            va_bits,
+            pa_bits
+        );
+
+        let full_va_range = 1u64 << va_bits;
+
+        let va_range = if for_mcu { 0..SZ_4G } else { 0..full_va_range };
+
+        let kernel_mm = mm::Allocator::new(
+            layout.kernel.start,
+            layout.kernel.end - layout.kernel.start,
+            (),
+        )?;
+
+        let page_table = ARM64LPAES1::new(
+            pdev.as_ref(),
+            io_pgtable::Config {
+                pgsize_bitmap: SZ_4K | SZ_2M as usize,
+                ias: va_bits as usize,
+                oas: pa_bits as usize,
+                coherent_walk: false,
+                quirks: 0,
+            },
+            (),
+        )?;
+
+        let memattr = mair_to_memattr(page_table.cfg().mair);
+
+        Ok(Vm {
+            _dummy_obj: dummy_obj.gem.clone(),
+            gpuvm: kernel::drm::gpuvm::GpuVm::new(
+                c_str!("Tyr::GpuVm"),
+                tdev,
+                &*(dummy_obj.gem),
+                va_range.clone(),
+                0..0,
+                LockedVm::new(page_table, kernel_mm),
+            )?,
+            // binding: None,
+            address_space: None,
+            memattr,
+            _layout: layout,
+            for_mcu,
+            auto_kernel_va,
+            destroyed: false,
+        })
+    }
+
+    /// Allocs a kernel range using the MM allocator.
+    ///
+    /// Kernel VAs are used for the FW, for synchronization objects, ring
+    /// buffers and other kernel-only data structures.
+    pub(crate) fn alloc_kernel_range(&mut self, va: KernelVaPlacement) -> Result<mm::Node<(), ()>> {
+        // stack_pin_init!(let local_guard = new_mutex!(()));
+        // let mut locked_vm = self.gpuvm.lock(&mut local_guard.lock());
+
+        match va {
+            KernelVaPlacement::Auto { size } => unsafe { self.gpuvm.as_inner_mut() }
+                .kernel_mm
+                .insert_node_in_range(
+                    (),
+                    size as u64,
+                    4096,
+                    0,
+                    self.auto_kernel_va.start,
+                    self.auto_kernel_va.end,
+                    mm::InsertMode::Best,
+                ),
+            KernelVaPlacement::At(va) => unsafe { self.gpuvm.as_inner_mut() }
+                .kernel_mm
+                .reserve_node((), va.start, va.end - va.start, 0),
+        }
+    }
+
+    /// Binds a GEM object to the VM, starting at `bo_offset`.
+    ///
+    /// `va_range` controls where in the VA space the BO will be mapped to.
+    pub(crate) fn bind_gem(
+        &mut self,
+        iomem: Arc<Devres<IoMem>>,
+        bo: &gem::Object,
+        bo_offset: u64,
+        va_range: Range<u64>,
+        vm_map_flags: map_flags::Flags,
+    ) -> Result {
+        // XXX: do not rearrange this or it will deadlock.
+        //
+        // Sadly, `inner` will lock the reservation for `bo`, and we need
+        // `inner` to produce `vm_bo`.
+        //
+        // In the natural drop order, the `ARef` for `vm_bo` will attempt to
+        // lock the reservation to decrement the refcount, but it's already
+        // locked by the call that produced `inner`.
+        //
+        // We can prove the above by just enabling lockdep.
+        //
+        // This means that it's trivially easy to deadlock when obtain_bo() is
+        // called if the drop order is not inverted. A solution to this will
+        // probably be beyond the scope of this driver. This problem also
+        // apparently predates Rust4Linux, from what I could gather.
+        //
+        // Here we just move `vm_bo` into `ctx`, to make sure it gets dropped
+        // after `inner`, on top of it also being needed in the `step_map`
+        // callback.
+        //
+        // Note that sg_table() will also lock the reservation, so it too needs
+        // to come before `inner`.
+        let mut ctx: StepContext = StepContext {
+            iomem,
+            vm_bo: None,
+            vm_map_flags: Some(vm_map_flags),
+            vm_as_nr: self.address_space,
+            preallocated_vas: StepContext::preallocate_vas()?,
+        };
+
+        // Things get tricky/nasty here as obtaining the gpuvm inner data requires
+        // a guard. Even though access is already protected by VM lock,
+        // this one cannot be used really, so ... there it is
+        // ... untill smth better pops up
+        // stack_pin_init!(let local_guard = new_mutex!(()));
+        // let mut locked_vm = self.gpuvm.lock(&mut local_guard.lock());
+
+        let vm_bo = self.gpuvm.obtain_bo(bo)?;
+
+        ctx.vm_bo = Some(vm_bo);
+        unsafe { self.gpuvm.as_inner_mut() }.map(&mut ctx, bo, va_range, bo_offset)
+    }
+
+    /// Unmap a given VA range.
+    pub(crate) fn unmap_range(&mut self, iomem: Arc<Devres<IoMem>>, range: Range<u64>) -> Result {
+        // stack_pin_init!(let local_guard = new_mutex!(()));
+        // let mut locked_vm = self.gpuvm.lock(&mut local_guard.lock());
+
+        let mut ctx = StepContext {
+            iomem,
+            vm_bo: None,
+            vm_map_flags: None,
+            vm_as_nr: None,
+            preallocated_vas: StepContext::preallocate_vas()?,
+        };
+
+        unsafe { self.gpuvm.as_inner_mut() }.unmap(&mut ctx, range)
+    }
+
+    /// Flush L2 caches for the entirety of a VM's AS.
+    pub(crate) fn flush(&self, tdev: &ARef<TyrDevice>) -> Result {
+        let iomem = &tdev.iomem;
+
+        let as_nr = self.address_space.ok_or(EINVAL)?;
+        let range = self.gpuvm.va_range();
+        Mmu::flush_range(iomem, as_nr, range)
+    }
+
+    /// Unmap the whole VM.
+    pub(crate) fn unmap_all(&mut self, iomem: Arc<Devres<IoMem>>) -> Result {
+        let range = self.gpuvm.va_range();
+
+        self.unmap_range(iomem, range)?;
+        self.address_space = None;
+
+        Ok(())
+    }
+
+    pub(crate) fn address_space(&self) -> Option<usize> {
+        self.address_space
+    }
+
+    /// Prepare our objecs, reserving a total of `num_slots` fence slots.
+    pub(crate) fn with_prepared_vm(
+        &self,
+        num_slots: u32,
+        f: impl FnOnce(PreparedVm<'_>) -> Result,
+    ) -> Result {
+        let exec_token = self.gpuvm.prepare(num_slots)?;
+        let prepared_vm = PreparedVm {
+            exec_token,
+            num_slots,
+        };
+
+        f(prepared_vm)
+    }
+}
+
+/// Indicates that all the reservations are locked for the objects in a given
+/// VM, and that `num_slots` have been reserved for fences.
+pub(crate) struct PreparedVm<'a> {
+    exec_token: ExecToken<'a, LockedVm>,
+    pub(crate) num_slots: u32,
+}
+
+/// 256M of every VM is reserved for kernel objects by default, i.e.: heap
+/// chunks, heapcontext, ring buffers, kernel synchronization objects and etc.
+///
+/// The user VA space always start at 0x0, and the kernel VA space is always
+/// placed after the user VA range.
+const MIN_KERNEL_VA_SIZE: u64 = 0x10000000;
+
+pub(crate) struct VmLayout {
+    /// Section reserved for user objects.
+    pub(crate) user: Range<u64>,
+
+    /// Section reserved for kernel objects.
+    pub(crate) kernel: Range<u64>,
+}
+
+impl VmLayout {
+    /// Automatically manages a layout given the a `VmSize`
+    pub(crate) fn from_user_sz(tdev: &TyrDevice, user_sz: VmUserSize) -> Self {
+        let va_bits = tdev.gpu_info.va_bits();
+        let max_va_range = 1u64 << va_bits;
+
+        let user;
+        let kernel;
+
+        match user_sz {
+            VmUserSize::Auto | VmUserSize::Custom(0) => {
+                user = 0..max_va_range - MIN_KERNEL_VA_SIZE;
+                kernel = user.end..user.end + MIN_KERNEL_VA_SIZE;
+            }
+            VmUserSize::Custom(user_sz) => {
+                let user_sz = core::cmp::min(user_sz, max_va_range - MIN_KERNEL_VA_SIZE);
+                user = 0..user_sz;
+                kernel = user_sz..user_sz + MIN_KERNEL_VA_SIZE;
+            }
+        }
+
+        Self { user, kernel }
+    }
+}
+
+/// Controls the size of the user VA space.
+pub(crate) enum VmUserSize {
+    /// Lets the kernel decide the user/kernel split.
+    Auto,
+    /// Sets the user VA space to a custom size. Things will crash if not enough
+    /// is left for kernel objects.
+    Custom(u64),
+}
+
+fn as_memattr_aarch64_inner_alloc_expl(inner: bool, outer: bool) -> u8 {
+    ((inner as u8) << 1) | (outer as u8)
+}
+
+fn mair_to_memattr(mair: u64) -> u64 {
+    let mut memattr: u64 = 0;
+
+    for i in 0..8 {
+        let in_attr = (mair >> (8 * i)) as u8;
+        let outer = in_attr >> 4;
+        let inner = in_attr & 0xf;
+
+        // For caching to be enabled, inner and outer caching policy
+        // have to be both write-back, if one of them is write-through
+        // or non-cacheable, we just choose non-cacheable. Device
+        // memory is also translated to non-cacheable.
+        let out_attr = if (outer & 3 == 0) || (outer & 4 == 0) || (inner & 4 == 0) {
+            regs::AS_MEMATTR_AARCH64_INNER_OUTER_NC
+                | regs::AS_MEMATTR_AARCH64_SH_MIDGARD_INNER
+                | as_memattr_aarch64_inner_alloc_expl(false, false) as u32
+        } else {
+            // Use SH_CPU_INNER mode so SH_IS, which is used when
+            // IOMMU_CACHE is set, actually maps to the standard
+            // definition of inner-shareable and not Mali's
+            // internal-shareable mode.
+            regs::AS_MEMATTR_AARCH64_INNER_OUTER_WB
+                | regs::AS_MEMATTR_AARCH64_SH_CPU_INNER
+                | as_memattr_aarch64_inner_alloc_expl(inner & 1 != 0, inner & 2 != 0) as u32
+        };
+
+        memattr |= (out_attr as u64) << (8 * i);
+    }
+
+    memattr
+}
diff --git a/drivers/gpu/drm/tyr/mmu/vm/gpuvm.rs b/drivers/gpu/drm/tyr/mmu/vm/gpuvm.rs
new file mode 100644
index 0000000000..38818db245
--- /dev/null
+++ b/drivers/gpu/drm/tyr/mmu/vm/gpuvm.rs
@@ -0,0 +1,323 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! The GPUVM driver implementation.
+//!
+//! GPUVM will split a given sm_map/sm_unmap request into a series of map, unmap
+//! and remap operations in order to manage the VA range.
+//!
+//! This file contains the driver-specific implementation, which includes the
+//! map, unmap and remap driver callbacks.
+
+use core::ops::Range;
+
+use kernel::devres::Devres;
+use kernel::drm::gpuvm::DriverGpuVa;
+use kernel::drm::gpuvm::{self};
+use kernel::drm::mm;
+use kernel::io::mem::IoMem;
+use kernel::io_pgtable::IoPageTable;
+use kernel::io_pgtable::ARM64LPAES1;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::types::ARef;
+
+use crate::driver;
+use crate::mmu::vm;
+use crate::mmu::Mmu;
+
+/// A convenience so that we do not have to spell this whole thing out every
+/// time.
+type PinnedVa = Pin<KBox<gpuvm::GpuVa<LockedVm>>>;
+
+/// A context that is passed throughout the map/unmap/remap steps.
+pub(in crate::mmu) struct StepContext {
+    /// The Vm <=> BO connection,
+    pub(super) vm_bo: Option<ARef<gpuvm::GpuVmBo<LockedVm>>>,
+
+    /// The used when mapping the VM that we are doing the steps on.
+    pub(super) vm_map_flags: Option<vm::map_flags::Flags>,
+
+    /// The address space number for the VM we are executing the operations on.
+    pub(super) vm_as_nr: Option<usize>,
+
+    /// We may need to access the MMIO region when performing the steps.
+    pub(super) iomem: Arc<Devres<IoMem>>,
+
+    /// This handles the remap case.
+    ///
+    /// Partial unmap requests or map requests overlapping existing mappings
+    /// will trigger a remap call, which needs to register up to three VA
+    /// objects (one for the new mapping, and two for the previous and next
+    /// mappings).
+    pub(super) preallocated_vas: [Option<PinnedVa>; 3],
+}
+
+impl StepContext {
+    /// Finds one of our pre-allocated VAs.
+    ///
+    /// It is a logic error to call this more than three times for a given
+    /// StepContext.
+    fn preallocated_va(&mut self) -> Result<PinnedVa> {
+        self.preallocated_vas
+            .iter_mut()
+            .find_map(|f| f.take())
+            .ok_or(EINVAL)
+    }
+
+    pub(super) fn preallocate_vas() -> Result<[Option<PinnedVa>; 3]> {
+        Ok([
+            Some(gpuvm::GpuVa::<LockedVm>::new(pin_init::zeroed())?),
+            Some(gpuvm::GpuVa::<LockedVm>::new(pin_init::zeroed())?),
+            Some(gpuvm::GpuVa::<LockedVm>::new(pin_init::zeroed())?),
+        ])
+    }
+}
+
+pub(crate) struct GpuVa {/* TODO */}
+unsafe impl pin_init::Zeroable for GpuVa {}
+
+impl DriverGpuVa for GpuVa {}
+
+/// A state that can only be accessed when the GPUVM is locked.
+pub(in crate::mmu) struct LockedVm {
+    /// The page table for this VM.
+    pub(in crate::mmu) page_table: ARM64LPAES1<Mmu>,
+    /// The allocator keeping track of what ranges are in use for the kernel VA
+    /// range.
+    pub(super) kernel_mm: mm::Allocator<(), ()>,
+}
+
+impl LockedVm {
+    pub(super) fn new(
+        page_table: ARM64LPAES1<Mmu>,
+        kernel_mm: mm::Allocator<(), ()>,
+    ) -> impl Init<Self> {
+        init!(LockedVm {
+            page_table,
+            kernel_mm,
+        })
+    }
+
+    fn unmap_pages(
+        &mut self,
+        iomem: &Devres<IoMem>,
+        as_nr: Option<usize>,
+        iova: Range<u64>,
+    ) -> Result {
+        let mut total_unmapped = 0;
+        let size = iova.end - iova.start;
+
+        while total_unmapped < size {
+            let pgsize = 4096;
+            let pgcount = (size - total_unmapped).div_ceil(pgsize);
+
+            let unmapped_sz =
+                self.page_table
+                    .unmap_pages(iova.start as usize, pgsize as usize, pgcount as usize);
+
+            if unmapped_sz as u64 != pgsize * pgcount {
+                let range = iova.start..iova.start + total_unmapped + unmapped_sz as u64;
+
+                pr_err!(
+                    "AS ({:#?}): failed to unmap range {:#x} - {:#x}, unmapped only {:#x} bytes\n",
+                    as_nr,
+                    iova.start,
+                    iova.start + size,
+                    unmapped_sz,
+                );
+
+                if let Some(as_nr) = as_nr {
+                    Mmu::flush_range(iomem, as_nr, range)?;
+                }
+
+                return Err(EINVAL);
+            }
+
+            pr_info!(
+                "AS ({:#?}): unmapped {} bytes, iova: {:#x}, pgsize: {}, pgcount: {}, len: {}\n",
+                as_nr,
+                unmapped_sz,
+                iova.start,
+                pgsize,
+                pgcount,
+                size
+            );
+
+            total_unmapped += unmapped_sz as u64;
+        }
+
+        if let Some(as_nr) = as_nr {
+            Mmu::flush_range(iomem, as_nr, iova)?;
+        }
+
+        Ok(())
+    }
+}
+
+impl gpuvm::DriverGpuVm for LockedVm {
+    type Driver = driver::TyrDriver;
+    type GpuVmBo = VmBo;
+    type StepContext = StepContext;
+
+    type GpuVa = GpuVa;
+
+    fn step_map(
+        gpuvm: &mut gpuvm::GpuVm<Self>,
+        op: &mut gpuvm::OpMap<Self>,
+        ctx: &mut Self::StepContext,
+    ) -> Result {
+        // This is the mapping algorithm from Asahi.
+
+        let mut iova = op.addr();
+        let mut left = op.length();
+        let mut offset = op.gem_offset();
+        let gpuva = ctx.preallocated_va()?;
+
+        let vm_bo = ctx.vm_bo.as_ref().ok_or(EINVAL)?;
+        let sgt = vm_bo.gem().sg_table();
+        let prot = ctx.vm_map_flags.ok_or(EINVAL)?.to_prot();
+
+        pr_info!("mapping {} bytes, iova: {:#x}, prot {}\n", left, iova, prot);
+
+        for range in sgt
+            .as_ref()
+            .expect("SGT should be set before step_map")
+            .iter()
+        {
+            let mut addr = range.dma_address();
+            let mut len = u64::from(range.dma_len());
+
+            if left == 0 {
+                break;
+            }
+
+            if offset > 0 {
+                let skip = len.min(offset);
+                addr += skip;
+                len -= skip;
+                offset -= skip;
+            }
+
+            if len == 0 {
+                continue;
+            }
+
+            assert!(offset == 0);
+
+            len = len.min(left);
+
+            let pgsize = 4096;
+            let pgcount = len.div_ceil(pgsize);
+
+            let _ = gpuvm.page_table.map_pages(
+                iova as usize,
+                addr as usize,
+                pgsize as usize,
+                pgcount as usize,
+                prot,
+            )?;
+
+            left -= len;
+            iova += len;
+        }
+
+        gpuvm.insert_va(op, gpuva).map_err(|_| EINVAL)?;
+        gpuvm.find_va(op.range(), |gpuvm, gpuva| {
+            let gpuva = gpuva.ok_or(EINVAL)?;
+            gpuvm.link_va(gpuva, ctx.vm_bo.as_ref().expect("step_map with no BO"))?;
+            Ok(())
+        })?;
+
+        Ok(())
+    }
+
+    fn step_unmap(
+        gpuvm: &mut gpuvm::GpuVm<Self>,
+        op: &mut gpuvm::OpUnMap<Self>,
+        ctx: &mut Self::StepContext,
+    ) -> Result {
+        // This is always set by drm_gpuvm.c:op_unmap_cb(), not sure why it's an
+        // Option.
+        //
+        // XXX: discuss this with everybody else
+        let va = op.va().expect("This is always set by GPUVM");
+        let iova = va.range();
+
+        gpuvm.unmap_pages(&ctx.iomem, ctx.vm_as_nr, iova)?;
+
+        gpuvm.find_va(va.range(), |gpuvm, gpuva| {
+            let removed = gpuvm.remove_va(gpuva.unwrap()).map_err(|_| EINVAL)?;
+            gpuvm.unlink_va(&removed);
+            Ok(())
+        })?;
+
+        Ok(())
+    }
+
+    fn step_remap(
+        gpuvm: &mut gpuvm::GpuVm<Self>,
+        op: &mut gpuvm::OpReMap<Self>,
+        _vm_bo: &gpuvm::GpuVmBo<Self>,
+        ctx: &mut Self::StepContext,
+    ) -> Result {
+        let prev_va = ctx.preallocated_va()?;
+        let next_va = ctx.preallocated_va()?;
+        let vm_bo = ctx.vm_bo.as_ref().ok_or(EINVAL)?;
+
+        let va = op.unmap().va().ok_or(EINVAL)?;
+        let orig_addr = va.addr();
+        let orig_range: u64 = va.length();
+
+        // Only unmap the hole between prev/next, if they exist
+        let unmap_start = if let Some(op) = op.prev_map() {
+            op.addr() + op.length()
+        } else {
+            orig_addr
+        };
+
+        let unmap_end = if let Some(op) = op.next_map() {
+            op.addr()
+        } else {
+            orig_addr + orig_range
+        };
+
+        let unmap_range = unmap_start..unmap_end;
+
+        gpuvm.unmap_pages(&ctx.iomem, ctx.vm_as_nr, unmap_range)?;
+        gpuvm.find_va(op.unmap().va().unwrap().range(), |gpuvm, gpuva| {
+            let removed_va = gpuvm.remove_va(gpuva.unwrap()).map_err(|_| EINVAL)?;
+            gpuvm.unlink_va(&removed_va);
+            Ok(())
+        })?;
+
+        if let Some(prev_op) = op.prev_map() {
+            gpuvm.insert_va(prev_op, prev_va).map_err(|_| EINVAL)?;
+            gpuvm.find_va(prev_op.range(), |gpuvm, gpuva| {
+                let gpuva = gpuva.ok_or(EINVAL)?;
+                gpuvm.link_va(gpuva, vm_bo)?;
+                Ok(())
+            })?;
+        }
+
+        if let Some(next_op) = op.next_map() {
+            gpuvm.insert_va(next_op, next_va).map_err(|_| EINVAL)?;
+            gpuvm.find_va(next_op.range(), |gpuvm, gpuva| {
+                let gpuva = gpuva.ok_or(EINVAL)?;
+                gpuvm.link_va(gpuva, vm_bo)?;
+                Ok(())
+            })?;
+        }
+
+        Ok(())
+    }
+}
+
+/// Data associated with a VM <=> BO pairing
+#[pin_data]
+pub(in crate::mmu) struct VmBo {}
+
+impl gpuvm::DriverGpuVmBo for VmBo {
+    fn new() -> impl PinInit<Self> {
+        pin_init!(VmBo {})
+    }
+}
diff --git a/drivers/gpu/drm/tyr/mmu/vm/map_flags.rs b/drivers/gpu/drm/tyr/mmu/vm/map_flags.rs
new file mode 100644
index 0000000000..f9619293b1
--- /dev/null
+++ b/drivers/gpu/drm/tyr/mmu/vm/map_flags.rs
@@ -0,0 +1,66 @@
+use kernel::bits::bit_u32;
+use kernel::io_pgtable;
+use kernel::prelude::*;
+
+use crate::impl_flags;
+
+impl_flags!(Flags, Flag, u32);
+
+impl Flags {
+    /// Convert the flags to `io_pgtable::prot`.
+    pub(super) fn to_prot(&self) -> u32 {
+        let mut prot = 0;
+
+        if self.contains(READONLY) {
+            prot |= io_pgtable::prot::READ;
+        } else {
+            prot |= io_pgtable::prot::READ | io_pgtable::prot::WRITE;
+        }
+
+        if self.contains(NOEXEC) {
+            prot |= io_pgtable::prot::NOEXEC;
+        }
+
+        if !self.contains(UNCACHED) {
+            prot |= io_pgtable::prot::CACHE;
+        }
+
+        prot
+    }
+}
+
+pub(crate) const READONLY: Flag = Flag(bit_u32(1));
+pub(crate) const NOEXEC: Flag = Flag(bit_u32(2));
+pub(crate) const UNCACHED: Flag = Flag(bit_u32(3));
+
+impl core::fmt::Display for Flags {
+    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
+        if self.contains(READONLY) {
+            write!(f, "| READONLY")?;
+        }
+        if self.contains(NOEXEC) {
+            write!(f, " | NOEXEC")?;
+        }
+
+        if self.contains(UNCACHED) {
+            write!(f, " | UNCACHED")?;
+        }
+
+        Ok(())
+    }
+}
+
+impl TryFrom<u32> for Flags {
+    type Error = Error;
+
+    fn try_from(value: u32) -> core::result::Result<Self, Self::Error> {
+        let valid = Flags::from(READONLY) | Flags::from(NOEXEC) | Flags::from(UNCACHED);
+
+        if value & !valid.0 != 0 {
+            pr_err!("Invalid VM map flags: {:#x}\n", value);
+            Err(EINVAL)
+        } else {
+            Ok(Self(value << 1))
+        }
+    }
+}
diff --git a/drivers/gpu/drm/tyr/mmu/vm/pool.rs b/drivers/gpu/drm/tyr/mmu/vm/pool.rs
new file mode 100644
index 0000000000..cbb09e1ff1
--- /dev/null
+++ b/drivers/gpu/drm/tyr/mmu/vm/pool.rs
@@ -0,0 +1,80 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! VMs created by userspace are placed in a pool so they can be find by other
+//! VM ioctls like VM_BIND or VM_DESTROY.
+
+use core::sync::atomic::AtomicUsize;
+
+use kernel::devres::Devres;
+use kernel::io::mem::IoMem;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::sync::Mutex;
+use kernel::types::ARef;
+use kernel::xarray;
+use kernel::xarray::XArray;
+
+use crate::driver::TyrDevice;
+use crate::mmu::vm::Vm;
+use crate::mmu::vm::VmLayout;
+
+/// The pool for user VMs.
+pub(crate) struct Pool {
+    xa: Pin<KBox<XArray<Arc<Mutex<Vm>>>>>,
+    free_index: AtomicUsize,
+}
+
+impl Pool {
+    pub(crate) fn create() -> Result<Self> {
+        let xa = KBox::pin_init(XArray::new(xarray::AllocKind::Alloc1), GFP_KERNEL)?;
+
+        Ok(Self {
+            xa,
+            free_index: AtomicUsize::new(1),
+        })
+    }
+
+    pub(crate) fn create_vm(&self, tdev: &ARef<TyrDevice>, layout: VmLayout) -> Result<usize> {
+        let auto_kernel_va = layout.kernel.clone();
+
+        let vm = {
+            tdev.with_locked_mmu(|mmu| {
+                mmu.create_vm(
+                    tdev,
+                    &tdev.pdev,
+                    &tdev.gpu_info,
+                    false,
+                    layout,
+                    auto_kernel_va,
+                )
+            })
+        }?;
+
+        let index = self
+            .free_index
+            .fetch_add(1, core::sync::atomic::Ordering::Relaxed);
+
+        let xa = self.xa.as_ref();
+        let mut guard = xa.lock();
+        guard.store(index, vm, GFP_KERNEL).map_err(|_| EINVAL)?;
+
+        Ok(index)
+    }
+
+    pub(crate) fn get_vm(self: Pin<&Self>, index: usize) -> Option<Arc<Mutex<Vm>>> {
+        let xa = self.xa.as_ref();
+        let guard = xa.lock();
+        let vm = guard.get(index)?;
+        Some(vm.into())
+    }
+
+    pub(crate) fn destroy_vm(self: Pin<&Self>, index: usize, iomem: Arc<Devres<IoMem>>) -> Result {
+        let xa = self.xa.as_ref();
+        let mut guard = xa.lock();
+        let vm = guard.remove(index).ok_or(EINVAL)?;
+
+        let mut vm = vm.lock();
+        vm.destroyed = true;
+        vm.unmap_all(iomem)
+    }
+}
diff --git a/drivers/gpu/drm/tyr/regs.rs b/drivers/gpu/drm/tyr/regs.rs
new file mode 100644
index 0000000000..db36cfd030
--- /dev/null
+++ b/drivers/gpu/drm/tyr/regs.rs
@@ -0,0 +1,252 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+#![allow(dead_code)]
+
+use kernel::bits::bit_u64;
+use kernel::devres::Devres;
+use kernel::io::mem::IoMem;
+use kernel::{bits::bit_u32, prelude::*};
+
+/// Represents a register in the Register Set
+pub(crate) struct Register<const OFFSET: usize>;
+
+impl<const OFFSET: usize> Register<OFFSET> {
+    #[inline]
+    pub(crate) fn read(&self, iomem: &Devres<IoMem>) -> Result<u32> {
+        (*iomem).try_access().ok_or(ENODEV)?.try_read32(OFFSET)
+    }
+
+    #[inline]
+    pub(crate) fn write(&self, iomem: &Devres<IoMem>, value: u32) -> Result<()> {
+        (*iomem)
+            .try_access()
+            .ok_or(ENODEV)?
+            .try_write32(value, OFFSET)
+    }
+}
+
+pub(crate) const GPU_ID: Register<0x0> = Register;
+pub(crate) const GPU_L2_FEATURES: Register<0x4> = Register;
+pub(crate) const GPU_CORE_FEATURES: Register<0x8> = Register;
+pub(crate) const GPU_CSF_ID: Register<0x1c> = Register;
+pub(crate) const GPU_REVID: Register<0x280> = Register;
+pub(crate) const GPU_TILER_FEATURES: Register<0xc> = Register;
+pub(crate) const GPU_MEM_FEATURES: Register<0x10> = Register;
+pub(crate) const GPU_MMU_FEATURES: Register<0x14> = Register;
+pub(crate) const GPU_AS_PRESENT: Register<0x18> = Register;
+pub(crate) const GPU_INT_RAWSTAT: Register<0x20> = Register;
+
+pub(crate) const GPU_INT_RAWSTAT_FAULT: u32 = bit_u32(0);
+pub(crate) const GPU_INT_RAWSTAT_PROTECTED_FAULT: u32 = bit_u32(1);
+pub(crate) const GPU_INT_RAWSTAT_RESET_COMPLETED: u32 = bit_u32(8);
+pub(crate) const GPU_INT_RAWSTAT_POWER_CHANGED_SINGLE: u32 = bit_u32(9);
+pub(crate) const GPU_INT_RAWSTAT_POWER_CHANGED_ALL: u32 = bit_u32(10);
+pub(crate) const GPU_INT_RAWSTAT_CLEAN_CACHES_COMPLETED: u32 = bit_u32(17);
+pub(crate) const GPU_INT_RAWSTAT_DOORBELL_STATUS: u32 = bit_u32(18);
+pub(crate) const GPU_INT_RAWSTAT_MCU_STATUS: u32 = bit_u32(19);
+
+pub(crate) const GPU_INT_CLEAR: Register<0x24> = Register;
+pub(crate) const GPU_INT_MASK: Register<0x28> = Register;
+pub(crate) const GPU_INT_STAT: Register<0x2c> = Register;
+pub(crate) const GPU_CMD: Register<0x30> = Register;
+pub(crate) const GPU_THREAD_FEATURES: Register<0xac> = Register;
+pub(crate) const GPU_THREAD_MAX_THREADS: Register<0xa0> = Register;
+pub(crate) const GPU_THREAD_MAX_WORKGROUP_SIZE: Register<0xa4> = Register;
+pub(crate) const GPU_THREAD_MAX_BARRIER_SIZE: Register<0xa8> = Register;
+pub(crate) const GPU_TEXTURE_FEATURES0: Register<0xb0> = Register;
+pub(crate) const GPU_SHADER_PRESENT_LO: Register<0x100> = Register;
+pub(crate) const GPU_SHADER_PRESENT_HI: Register<0x104> = Register;
+pub(crate) const GPU_TILER_PRESENT_LO: Register<0x110> = Register;
+pub(crate) const GPU_TILER_PRESENT_HI: Register<0x114> = Register;
+pub(crate) const GPU_L2_PRESENT_LO: Register<0x120> = Register;
+pub(crate) const GPU_L2_PRESENT_HI: Register<0x124> = Register;
+pub(crate) const L2_READY_LO: Register<0x160> = Register;
+pub(crate) const L2_READY_HI: Register<0x164> = Register;
+pub(crate) const L2_PWRON_LO: Register<0x1a0> = Register;
+pub(crate) const L2_PWRON_HI: Register<0x1a4> = Register;
+pub(crate) const L2_PWRTRANS_LO: Register<0x220> = Register;
+pub(crate) const L2_PWRTRANS_HI: Register<0x204> = Register;
+pub(crate) const L2_PWRACTIVE_LO: Register<0x260> = Register;
+pub(crate) const L2_PWRACTIVE_HI: Register<0x264> = Register;
+
+pub(crate) const MCU_CONTROL: Register<0x700> = Register;
+pub(crate) const MCU_CONTROL_ENABLE: u32 = 1;
+pub(crate) const MCU_CONTROL_AUTO: u32 = 2;
+pub(crate) const MCU_CONTROL_DISABLE: u32 = 0;
+
+pub(crate) const MCU_STATUS: Register<0x704> = Register;
+pub(crate) const MCU_STATUS_DISABLED: u32 = 0;
+pub(crate) const MCU_STATUS_ENABLED: u32 = 1;
+pub(crate) const MCU_STATUS_HALT: u32 = 2;
+pub(crate) const MCU_STATUS_FATAL: u32 = 3;
+
+pub(crate) const GPU_COHERENCY_FEATURES: Register<0x300> = Register;
+
+pub(crate) const JOB_INT_RAWSTAT: Register<0x1000> = Register;
+pub(crate) const JOB_INT_CLEAR: Register<0x1004> = Register;
+pub(crate) const JOB_INT_MASK: Register<0x1008> = Register;
+pub(crate) const JOB_INT_STAT: Register<0x100c> = Register;
+
+pub(crate) const JOB_INT_GLOBAL_IF: u32 = bit_u32(31);
+
+pub(crate) const MMU_INT_RAWSTAT: Register<0x2000> = Register;
+pub(crate) const MMU_INT_CLEAR: Register<0x2004> = Register;
+pub(crate) const MMU_INT_MASK: Register<0x2008> = Register;
+pub(crate) const MMU_INT_STAT: Register<0x200c> = Register;
+
+pub(crate) const AS_TRANSCFG_ADRMODE_UNMAPPED: u64 = bit_u64(0);
+pub(crate) const AS_TRANSCFG_ADRMODE_IDENTITY: u64 = bit_u64(1);
+pub(crate) const AS_TRANSCFG_ADRMODE_AARCH64_4K: u64 = bit_u64(2) | bit_u64(1);
+pub(crate) const AS_TRANSCFG_ADRMODE_AARCH64_64K: u64 = bit_u64(3);
+pub(crate) const fn as_transcfg_ina_bits(x: u64) -> u64 {
+    x << 6
+}
+pub(crate) const fn as_transcfg_outa_bits(x: u64) -> u64 {
+    x << 14
+}
+pub(crate) const AS_TRANSCFG_SL_CONCAT: u64 = bit_u64(22);
+pub(crate) const AS_TRANSCFG_PTW_MEMATTR_NC: u64 = bit_u64(24);
+pub(crate) const AS_TRANSCFG_PTW_MEMATTR_WB: u64 = bit_u64(25);
+pub(crate) const AS_TRANSCFG_PTW_SH_NS: u64 = 0 << 28;
+pub(crate) const AS_TRANSCFG_PTW_SH_OS: u64 = bit_u64(29);
+pub(crate) const AS_TRANSCFG_PTW_SH_IS: u64 = bit_u64(29) | bit_u64(28);
+pub(crate) const AS_TRANSCFG_PTW_RA: u64 = bit_u64(30);
+pub(crate) const AS_TRANSCFG_DISABLE_HIER_AP: u64 = bit_u64(33);
+pub(crate) const AS_TRANSCFG_DISABLE_AF_FAULT: u64 = bit_u64(34);
+pub(crate) const AS_TRANSCFG_WXN: u64 = bit_u64(35);
+
+pub(crate) const MMU_BASE: usize = 0x2400;
+pub(crate) const MMU_AS_SHIFT: usize = 6;
+
+const fn mmu_as(as_nr: usize) -> usize {
+    MMU_BASE + (as_nr << MMU_AS_SHIFT)
+}
+
+pub(crate) struct AsRegister(usize);
+
+impl AsRegister {
+    fn new(as_nr: usize, offset: usize) -> Result<Self> {
+        if as_nr >= 32 {
+            Err(EINVAL)
+        } else {
+            Ok(AsRegister(mmu_as(as_nr) + offset))
+        }
+    }
+
+    #[inline]
+    pub(crate) fn read(&self, iomem: &Devres<IoMem>) -> Result<u32> {
+        (*iomem).try_access().ok_or(ENODEV)?.try_read32(self.0)
+    }
+
+    #[inline]
+    pub(crate) fn write(&self, iomem: &Devres<IoMem>, value: u32) -> Result<()> {
+        (*iomem)
+            .try_access()
+            .ok_or(ENODEV)?
+            .try_write32(value, self.0)
+    }
+}
+
+pub(crate) fn as_transtab_lo(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x0)
+}
+
+pub(crate) fn as_transtab_hi(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x4)
+}
+
+pub(crate) fn as_memattr_lo(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x8)
+}
+
+pub(crate) fn as_memattr_hi(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0xc)
+}
+
+pub(crate) fn as_lockaddr_lo(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x10)
+}
+
+pub(crate) fn as_lockaddr_hi(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x14)
+}
+
+pub(crate) fn as_command(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x18)
+}
+
+pub(crate) fn as_faultstatus(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x1c)
+}
+
+pub(crate) const AS_FAULTSTATUS_ACCESS_TYPE_MASK: u32 = 0x3 << 8;
+pub(crate) const AS_FAULTSTATUS_ACCESS_TYPE_ATOMIC: u32 = 0x0 << 8;
+pub(crate) const AS_FAULTSTATUS_ACCESS_TYPE_EX: u32 = 0x1 << 8;
+pub(crate) const AS_FAULTSTATUS_ACCESS_TYPE_READ: u32 = 0x2 << 8;
+pub(crate) const AS_FAULTSTATUS_ACCESS_TYPE_WRITE: u32 = 0x3 << 8;
+
+pub(crate) fn as_faultaddress_lo(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x20)
+}
+
+pub(crate) fn as_faultaddress_hi(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x24)
+}
+
+pub(crate) const AS_COMMAND_NOP: u32 = 0;
+pub(crate) const AS_COMMAND_UPDATE: u32 = 1;
+pub(crate) const AS_COMMAND_LOCK: u32 = 2;
+pub(crate) const AS_COMMAND_UNLOCK: u32 = 3;
+pub(crate) const AS_COMMAND_FLUSH_PT: u32 = 4;
+pub(crate) const AS_COMMAND_FLUSH_MEM: u32 = 5;
+
+pub(crate) fn as_status(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x28)
+}
+
+pub(crate) const AS_STATUS_ACTIVE: u32 = bit_u32(0);
+
+pub(crate) fn as_transcfg_lo(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x30)
+}
+pub(crate) fn as_transcfg_hi(as_nr: usize) -> Result<AsRegister> {
+    AsRegister::new(as_nr, 0x34)
+}
+
+pub(crate) const AS_LOCK_REGION_MIN_SIZE: u32 = bit_u32(15);
+
+pub(crate) const AS_MEMATTR_AARCH64_INNER_ALLOC_IMPL: u32 = 2 << 2;
+
+pub(crate) fn as_memattr_aarch64_inner_alloc_expl(w: bool, r: bool) -> u32 {
+    (3 << 2) | ((w as u32) << 0) | ((r as u32) << 1)
+}
+pub(crate) const AS_MEMATTR_AARCH64_SH_MIDGARD_INNER: u32 = 0 << 4;
+pub(crate) const AS_MEMATTR_AARCH64_SH_CPU_INNER: u32 = 1 << 4;
+pub(crate) const AS_MEMATTR_AARCH64_SH_CPU_INNER_SHADER_COH: u32 = 2 << 4;
+pub(crate) const AS_MEMATTR_AARCH64_SHARED: u32 = 0 << 6;
+pub(crate) const AS_MEMATTR_AARCH64_INNER_OUTER_NC: u32 = 1 << 6;
+pub(crate) const AS_MEMATTR_AARCH64_INNER_OUTER_WB: u32 = 2 << 6;
+pub(crate) const AS_MEMATTR_AARCH64_FAULT: u32 = 3 << 6;
+
+pub(crate) struct Doorbell(usize);
+
+impl Doorbell {
+    pub(crate) fn new(doorbell_id: usize) -> Self {
+        Doorbell(0x80000 + (doorbell_id * 0x10000))
+    }
+
+    #[inline]
+    pub(crate) fn read(&self, iomem: &Devres<IoMem>) -> Result<u32> {
+        (*iomem).try_access().ok_or(ENODEV)?.try_read32(self.0)
+    }
+
+    #[inline]
+    pub(crate) fn write(&self, iomem: &Devres<IoMem>, value: u32) -> Result<()> {
+        (*iomem)
+            .try_access()
+            .ok_or(ENODEV)?
+            .try_write32(value, self.0)
+    }
+}
+
+pub(crate) const CSF_GLB_DOORBELL_ID: usize = 0;
diff --git a/drivers/gpu/drm/tyr/sched.rs b/drivers/gpu/drm/tyr/sched.rs
new file mode 100644
index 0000000000..56de197ee0
--- /dev/null
+++ b/drivers/gpu/drm/tyr/sched.rs
@@ -0,0 +1,429 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use group::Group;
+use kernel::bits::bit_u32;
+use kernel::bits::genmask_u32;
+use kernel::c_str;
+use kernel::dma_fence::UserFence;
+use kernel::drm::syncobj::SyncObj;
+use kernel::kvec;
+use kernel::prelude::*;
+use kernel::sizes::SZ_4K;
+use kernel::sync::Arc;
+use kernel::time::Delta;
+use kernel::time::Instant;
+use kernel::workqueue::OwnedQueue;
+use kernel::workqueue::WqFlags;
+use queue::Queue;
+
+use crate::driver::TyrDevice;
+use crate::file::QueueSubmit;
+use crate::fw::global::cs::CommandStream;
+use crate::fw::global::cs::StreamState;
+use crate::fw::global::csg;
+use crate::fw::global::csg::Priority;
+use crate::fw::global::csg::MAX_CSGS;
+use crate::fw::SharedSectionEntry;
+use crate::gem;
+use crate::TyrDriver;
+
+mod events;
+pub(crate) mod group;
+pub(crate) mod job;
+pub(crate) mod queue;
+mod syncs;
+mod tick;
+
+const MAX_CSG_PRIO: u32 = 0xf;
+
+/// The scheduler object.
+pub(crate) enum SchedulerState {
+    /// The driver is probing.
+    Disabled,
+    /// The firmware has booted and the scheduler has been initialized.
+    Enabled(Scheduler),
+}
+
+impl SchedulerState {
+    pub(crate) fn init(&mut self, tdev: &TyrDevice) -> Result {
+        let scheduler = Scheduler::init(tdev)?;
+        *self = SchedulerState::Enabled(scheduler);
+        Ok(())
+    }
+
+    pub(crate) fn enabled_mut(&mut self) -> Result<&mut Scheduler> {
+        match self {
+            SchedulerState::Enabled(scheduler) => Ok(scheduler),
+            SchedulerState::Disabled => Err(EINVAL),
+        }
+    }
+}
+
+pub(crate) struct Scheduler {
+    /// Groups that have at least one queue that can be currently scheduled.
+    runnable_groups: [KVec<Arc<Group>>; Priority::num_priorities()],
+    /// Groups that have all their queues idle, either because they have nothing
+    /// to execute, or because they are blocked.
+    idle_groups: [KVec<Arc<Group>>; Priority::num_priorities()],
+    /// List of groups whose queues are blocked on a sync object.
+    waiting_groups: [KVec<Arc<Group>>; Priority::num_priorities()],
+
+    /// Groups that have been flagged by a STATUS_UPDATE event, but that have
+    /// not yet been processed.
+    unsynced_groups: KVec<Arc<Group>>,
+
+    csg_slots: [Option<CommandStreamGroupSlot>; 31],
+
+    /// Number of command stream group slots exposed by the firmware.
+    csg_slot_count: u32,
+
+    /// Number of command stream slots per group slot exposed by the firmware.
+    cs_slot_count: u32,
+
+    /// Number of address space slots supported by the MMU.
+    as_slot_count: u32,
+
+    /// Number of command stream group slots currently in use.
+    used_csg_slot_count: u32,
+
+    /// Number of scoreboard slots.
+    sb_slot_count: u32,
+
+    /// Workqueue used by our internal scheduler logic and by the
+    /// [`drm::Scheduler`].
+    ///
+    /// Used for the scheduler tick, group update or other kinds of FW event
+    /// processing that cannot be handled in the threaded interrupt path. Also
+    /// passed to the scheduler instances embedded in our queues.
+    wq: OwnedQueue,
+
+    /// When the next tick should occur.
+    resched_target: Option<Instant>,
+
+    /// Outstanding firmware events.
+    events: Option<u32>,
+}
+
+impl Scheduler {
+    pub(crate) fn init(tdev: &TyrDevice) -> Result<Self> {
+        let (group_num, sb_slot_count, cs_slot_count) =
+            tdev.fw.with_locked_global_iface(|glb_iface| {
+                let glb_control = glb_iface.read_control()?;
+
+                let csg = glb_iface.csg(0).ok_or(EINVAL)?;
+                let csg_control = csg.read_control()?;
+
+                let cs = csg.cs(0).ok_or(EINVAL)?;
+                let cs_control = cs.read_control()?;
+
+                let group_num = glb_control.group_num;
+                let sb_slot_count = cs_control.scoreboards();
+                let cs_slot_count = csg_control.stream_num;
+
+                Ok((group_num, sb_slot_count, cs_slot_count))
+            })?;
+
+        let num_groups = core::cmp::min(MAX_CSGS, group_num);
+
+        // The firmware-side scheduler might deadlock if two groups with the same
+        // priority try to access a set of resources that overlaps, with part of the
+        // resources being allocated to one group and the other part to the other group,
+        // both groups waiting for the remaining resources to be allocated.
+        //
+        // To avoid that, it is recommended to assign each Command Stream Group (CSG)
+        // a different priority. In theory, several groups could have the same CSG
+        // priority if they don't request the same resources, but that would make the
+        // scheduling logic more complicated.
+        //
+        // For now, the number of CSG slots is clamped to `MAX_CSG_PRIO + 1`.
+        let num_groups = core::cmp::min(MAX_CSG_PRIO + 1, num_groups);
+
+        // We need at least one AS for the MCU and one for the GPU contexts.
+        let gpu_as_count = tdev.gpu_info.as_present & genmask_u32(31, 1);
+        let gpu_as_count = gpu_as_count.count_ones();
+
+        let csg_slot_count = num_groups;
+        let as_slot_count = gpu_as_count;
+
+        let wq = OwnedQueue::new(c_str!("tyr-csf-sched"), WqFlags::UNBOUND, 0)?;
+
+        Ok(Self {
+            runnable_groups: [const { KVec::new() }; Priority::num_priorities()],
+            idle_groups: [const { KVec::new() }; Priority::num_priorities()],
+            waiting_groups: [const { KVec::new() }; Priority::num_priorities()],
+            unsynced_groups: KVec::new(),
+            csg_slots: [const { None }; 31],
+            csg_slot_count,
+            cs_slot_count,
+            as_slot_count,
+            used_csg_slot_count: 0,
+            sb_slot_count,
+            wq,
+            resched_target: None,
+            events: None,
+        })
+    }
+
+    /// Bind a group to a group slot.
+    ///
+    /// A group needs to be bound before it can be programmed into one of the
+    /// firmware slots for execution.
+    pub(crate) fn bind_group(
+        &mut self,
+        tdev: &TyrDevice,
+        group: Arc<Group>,
+        csg_idx: usize,
+    ) -> Result {
+        if csg_idx >= self.csg_slot_count as usize {
+            pr_err!("bind_group: invalid group index {}", csg_idx);
+            return Err(EINVAL);
+        }
+
+        group.with_locked_inner(|inner| {
+            if inner.csg_id.is_some() {
+                pr_err!("bind_group: group already bound to a CSG");
+                return Err(EINVAL);
+            }
+            Ok(())
+        })?;
+
+        if self.csg_slots[csg_idx].is_some() {
+            pr_err!("bind_group: group slot already in use");
+            return Err(EINVAL);
+        }
+
+        let gpu_info = &tdev.gpu_info;
+        let iomem = &tdev.iomem;
+
+        tdev.with_locked_mmu(|mmu| mmu.bind_vm(group.vm.clone(), gpu_info, iomem))?;
+
+        self.csg_slots[csg_idx] = Some(CommandStreamGroupSlot {
+            group: group.clone(),
+            priority: Priority::Low,
+            idle: true,
+        });
+
+        group.with_locked_inner(|inner| {
+            inner.csg_id = Some(csg_idx);
+            // Dummy doorbell allocation: doorbell is assigned to the group and all
+            // queues use the same doorbell.
+            //
+            // TODO: Implement LRU-based doorbell assignment, so the most often
+            // updated queues get their own doorbell, thus avoiding useless checks
+            // on queues belonging to the same group that are rarely updated.
+            for queue in &mut inner.queues {
+                queue.doorbell_id = Some(csg_idx + 1);
+            }
+
+            Ok(())
+        })
+    }
+
+    /// Program a group (and its queues) into a firmware slot. This will make
+    /// the group eligible for execution from a FW perspective.
+    // TODO: this can be private
+    pub(crate) fn program_csg_slot(
+        &mut self,
+        tdev: &TyrDevice,
+        csg_idx: usize,
+        priority: Priority,
+    ) -> Result {
+        if priority as u32 > MAX_CSG_PRIO {
+            pr_err!("program_csg_slot: invalid priority {}\n", priority as u32);
+            return Err(EINVAL);
+        }
+
+        if csg_idx > MAX_CSGS as usize {
+            pr_err!("program_csg_slot: invalid csg {}\n", csg_idx);
+            return Err(EINVAL);
+        }
+
+        let slot = self.csg_slots[csg_idx].as_ref().ok_or(EINVAL)?;
+        let group = slot.group.clone();
+        let as_nr = group
+            .vm
+            .lock()
+            .address_space()
+            .map(|a| a as u32)
+            .ok_or(EINVAL)?;
+
+        // let group_inner = group.inner.lock();
+
+        let fw = &tdev.fw;
+
+        // Controls which CSn doorbells will be rung.
+        //
+        // This will process any requests in the CSn request field, and also
+        // check for new work on the ring buffer.
+        let queue_mask = fw.with_locked_global_iface(|glb_iface| {
+            group.with_locked_inner(|inner| {
+                if let group::State::Active = inner.state {
+                    pr_err!("program_csg_slot: group is already active\n");
+                    return Err(EINVAL);
+                }
+
+                let mut queue_mask = 0;
+
+                let csg_iface = glb_iface.csg_mut(csg_idx).ok_or(EINVAL)?;
+                for (cs_idx, queue) in inner.queues.iter().enumerate() {
+                    let cs_iface = csg_iface.cs_mut(cs_idx).ok_or(EINVAL)?;
+
+                    self.program_cs_slot(queue, cs_iface)?;
+                    queue_mask |= bit_u32(cs_idx as u32);
+                }
+
+                Ok(queue_mask)
+            })
+        })?;
+
+        let mut input = fw.with_locked_global_iface(|glb_iface| {
+            glb_iface.csg_mut(csg_idx).ok_or(EINVAL)?.read_input()
+        })?;
+
+        input.allow_compute = group.compute_core_mask;
+        input.allow_fragment = group.fragment_core_mask;
+        input.allow_other = group.tiler_core_mask.try_into()?;
+
+        input.set_endpoint_req(
+            group.max_compute_cores.into(),
+            group.max_fragment_cores.into(),
+            group.max_tiler_cores.into(),
+            priority,
+        );
+
+        input.csg_config = as_nr;
+
+        input.suspend_buf = group.suspend_buf.kernel_va().ok_or(EINVAL)?.start;
+        input.protm_suspend_buf = group.protm_suspend_buf.kernel_va().ok_or(EINVAL)?.start;
+
+        input.ack_irq_mask = u32::MAX;
+
+        fw.with_locked_global_iface(|glb_iface| {
+            let csg_iface = glb_iface.csg_mut(csg_idx).ok_or(EINVAL)?;
+            csg_iface.write_input(input)?;
+
+            let db_req = csg_iface.doobell_request()?;
+            db_req.toggle_reqs(queue_mask)?;
+
+            glb_iface.set_csg_state(0, csg::GroupState::Start)?;
+            glb_iface.ring_csg_doorbell(0)
+        })
+    }
+
+    /// Program a queue in a firmware slot. This makes the queue eligible for
+    /// execution from a FW perspective.
+    ///
+    /// Queues are alloted slots when their group is itself programmed into a
+    /// CSG slot.
+    fn program_cs_slot(&mut self, queue: &Queue, cs_iface: &mut CommandStream) -> Result {
+        let doorbell_id = queue.doorbell_id.ok_or(EINVAL)?;
+        let mut cs_input = cs_iface.read_input()?;
+
+        cs_input.ringbuf_base = queue.ringbuf.kernel_va().ok_or(EINVAL)?.start;
+        cs_input.ringbuf_size = queue.ringbuf.size() as u32;
+
+        cs_input.ringbuf_input = queue.interfaces.input_va.start;
+        cs_input.ringbuf_output = queue.interfaces.output_va.start;
+
+        cs_input.set_priority(queue.priority)?;
+        cs_input.set_doorbell_id(doorbell_id as u32)?;
+        cs_input.ack_irq_mask = u32::MAX;
+
+        cs_iface.write_input(cs_input)?;
+
+        cs_iface.set_state(StreamState::Start)
+    }
+
+    // TODO: This is here just for debug purposes. Remove this soon.
+    pub(crate) fn bind0(&mut self, tdev: &TyrDevice, group: Arc<Group>) -> Result {
+        self.bind_group(tdev, group, 0)?;
+        self.program_csg_slot(tdev, 0, Priority::Low)
+    }
+
+    // place a dummy instruction in the first CS for the given group and kick
+    // it, just to make sure the ringbuf code is working.
+    pub(crate) fn issue_dummy_instr(&mut self, group: Arc<Group>, tdev: &TyrDevice) -> Result {
+        self.bind0(tdev, group.clone())?;
+
+        let iomem = tdev.iomem.clone();
+
+        use crate::mmu::vm::map_flags;
+        let flags =
+            map_flags::Flags::from(map_flags::NOEXEC) | map_flags::Flags::from(map_flags::UNCACHED);
+
+        let mut debug_gem = gem::new_kernel_object(
+            tdev,
+            iomem,
+            group.vm.clone(),
+            gem::KernelVaPlacement::Auto { size: SZ_4K },
+            flags,
+        )?;
+
+        let mut instrs = kvec![];
+
+        // load the source register ([r64; r65]) with the right address to store.
+        let opcode = 0x1;
+        let reg_num = 64;
+        let immd = debug_gem.kernel_va().ok_or(EINVAL)?.start;
+        let mov48: u64 = opcode << 56 | reg_num << 48 | immd;
+
+        instrs.extend_from_slice(&mov48.to_le_bytes(), GFP_KERNEL)?;
+
+        // load a known constant into r66.
+        let opcode = 0x1;
+        let reg_num = 66;
+        let immd = 0xdeadbeef;
+        let mov48: u64 = opcode << 56 | reg_num << 48 | immd;
+
+        instrs.extend_from_slice(&mov48.to_le_bytes(), GFP_KERNEL)?;
+
+        let opcode = 0x15; // STORE_MULTIPLE
+        let register_bitmap = 1; // store the first register
+        let sr = 66; // starting from register 66
+        let src0 = 64; // to the address pointed to by [r64; r65]
+        let offset = 0; // and this offset
+
+        let store_multiple: u64 =
+            opcode << 56 | sr << 48 | src0 << 40 | register_bitmap << 16 | offset;
+
+        instrs.extend_from_slice(&store_multiple.to_le_bytes(), GFP_KERNEL)?;
+
+        group.with_locked_inner(|inner| {
+            let queue = inner.queues.get_mut(0).ok_or(EINVAL)?;
+            queue.append_instrs(&instrs)?;
+            queue.kick()
+        })?;
+
+        // We are not using any syncobjs, so we must sleep for a while to check
+        // for completion.
+        kernel::time::delay::fsleep(Delta::from_millis(100));
+
+        // Read the address where the GPU is supposed to have written the value.
+        let vmap = debug_gem.vmap()?.as_slice();
+        let value = u32::from_le_bytes(vmap[0..4].try_into().unwrap());
+
+        pr_info!("issue_dummy_instr expected 0xdeadbeef, got 0x{:x}\n", value);
+        Ok(())
+    }
+
+    pub(crate) fn submit(
+        &mut self,
+        in_syncs: KVec<SyncObj<TyrDriver>>,
+        out_syncs: KVec<SyncObj<TyrDriver>>,
+        group: Arc<Group>,
+        queue_submits: KVec<QueueSubmit>,
+    ) -> Result<KVec<UserFence<job::Fence>>> {
+        group.submit(in_syncs, out_syncs, queue_submits)
+    }
+}
+
+pub(crate) struct CommandStreamGroupSlot {
+    /// The group that is bound to this slot.
+    pub(crate) group: Arc<Group>,
+
+    /// Group priority.
+    pub(crate) priority: Priority,
+
+    /// The if the group bound to the slot is idle.
+    pub(crate) idle: bool,
+}
diff --git a/drivers/gpu/drm/tyr/sched/events.rs b/drivers/gpu/drm/tyr/sched/events.rs
new file mode 100644
index 0000000000..ad5f040e6f
--- /dev/null
+++ b/drivers/gpu/drm/tyr/sched/events.rs
@@ -0,0 +1,273 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+//! Firmware event processing.
+//!
+//! The firmware events are used to notify the driver of the overall progress of
+//! the work currently submitted, as well as other scheduler-related events,
+//! like device idleness, CSG/CS interrupts, fault decoding and etc.
+
+use core::ops::Deref;
+
+use kernel::dma_fence::FenceOps;
+use kernel::dma_fence::RawDmaFence;
+use kernel::impl_has_work;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::workqueue::WorkItem;
+
+use crate::driver::TyrData;
+use crate::driver::TyrDevice;
+use crate::fw::global::cs;
+use crate::fw::global::csg;
+use crate::fw::global::csg::CommandStreamGroup;
+use crate::fw::global::GlobalInterface;
+use crate::fw::SharedSectionEntry;
+use crate::regs::JOB_INT_GLOBAL_IF;
+use crate::sched::Scheduler;
+
+use super::group::Group;
+use super::syncs;
+
+impl Scheduler {
+    pub(crate) fn process_events(&mut self, data: Arc<TyrData>) -> Result {
+        // TODO: we need to annotate this function with the dma signalling token.
+
+        let fw = &data.fw;
+        let mut events = self.events.take().unwrap_or_default();
+
+        if events & JOB_INT_GLOBAL_IF != 0 {
+            // We don't support global events yet.
+            events &= !JOB_INT_GLOBAL_IF;
+        }
+
+        fw.with_locked_global_iface(|glb| {
+            while events != 0 {
+                let csg_id = events.trailing_zeros();
+                let mask = kernel::bits::bit_u32(csg_id);
+
+                self.process_csg_irq(data.clone(), glb, csg_id)?;
+                events &= !mask;
+            }
+
+            Ok(())
+        })?;
+
+        Ok(())
+    }
+
+    fn process_csg_irq(
+        &mut self,
+        data: Arc<TyrData>,
+        glb: &mut GlobalInterface,
+        csg_id: u32,
+    ) -> Result {
+        // TODO: we need to annotate this function with the dma signalling token.
+
+        let csg = glb.csg_mut(csg_id as usize).ok_or(EINVAL)?;
+
+        let mut input = csg.read_input()?;
+        let output = csg.read_output()?;
+
+        let csg_events = (input.req ^ output.ack) & csg::constants::CSG_EVT_MASK;
+
+        // // We may have no pending CSG/CS interrupts to process.
+        if input.req == output.ack && output.irq_req == input.irq_ack {
+            return Ok(());
+        }
+
+        let mut cs_irqs = output.irq_req ^ input.irq_ack;
+
+        // Immediately set IRQ_ACK bits to be same as the IRQ_REQ bits before
+        // examining the CS_ACK & CS_REQ bits. This would ensure that Host
+        // doesn't miss an interrupt for the CS in the race scenario where
+        // whilst Host is servicing an interrupt for the CS, firmware sends
+        // another interrupt for that CS.
+        input.irq_ack = output.irq_req;
+        csg.write_input(input)?;
+
+        let req = csg.input_request()?;
+        let reenable_mask = csg::constants::CSG_SYNC_UPDATE;
+
+        req.update_reqs(csg.read_output()?.ack, reenable_mask)?;
+        let mut ring_cs_db_mask = 0;
+
+        while cs_irqs != 0 {
+            let cs_id = cs_irqs.trailing_zeros();
+            let mask = kernel::bits::bit_u32(cs_id);
+
+            let processed = self.process_cs_irq(csg, csg_id, cs_id)?;
+
+            if processed {
+                ring_cs_db_mask |= mask;
+            }
+
+            cs_irqs &= !mask;
+        }
+
+        if ring_cs_db_mask != 0 {
+            let req = csg.doobell_request()?;
+            req.toggle_reqs(ring_cs_db_mask)?;
+        }
+
+        if csg_events & csg::constants::CSG_SYNC_UPDATE != 0 {
+            let group = self.csg_slots[csg_id as usize]
+                .as_mut()
+                .ok_or(EINVAL)?
+                .group
+                .clone();
+
+            self.unsynced_groups.push(group, GFP_KERNEL)?;
+
+            if self.wq.enqueue::<_, 3>(data.clone()).is_err() {
+                pr_err!("Failed to enqueue the group update work\n");
+            }
+        }
+
+        glb.ring_csg_doorbell(csg_id as usize)
+    }
+
+    fn process_cs_irq(
+        &mut self,
+        csg: &mut CommandStreamGroup,
+        csg_id: u32,
+        cs_id: u32,
+    ) -> Result<bool> {
+        let cs = csg.cs_mut(cs_id as usize).ok_or(EINVAL)?;
+
+        let input = cs.read_input()?;
+        let output = cs.read_output()?;
+
+        let cs_events = (input.req ^ output.ack) & cs::constants::CS_EVT_MASK;
+
+        let faulty =
+            cs_events & cs::constants::CS_FATAL != 0 || cs_events & cs::constants::CS_FAULT != 0;
+
+        if cs_events & cs::constants::CS_FATAL != 0 {
+            cs.decode_fatal()?;
+            if let Some(slot) = &mut self.csg_slots[csg_id as usize] {
+                slot.group.with_locked_inner(|inner| {
+                    inner.fatal_queues |= 1 << cs_id;
+                    Ok(())
+                })?;
+            }
+        }
+
+        if cs_events & cs::constants::CS_FAULT != 0 {
+            cs.decode_fault()?;
+        }
+
+        if faulty {
+            // TODO: we cannot sleep in the signalling path.
+            self.csg_slots[csg_id as usize]
+                .as_mut()
+                .ok_or(EINVAL)?
+                .group
+                .with_locked_inner(|inner| {
+                    for job_fence in &inner.queues[cs_id as usize].in_flight_jobs {
+                        // Just mark everything in flight as failed.
+                        //
+                        // This is not exactly the right thing to do, but while
+                        // the driver is being developed, this will let us at
+                        // least signal all fences, even if we have errored out.
+                        //
+                        // Also, there is no error recovery for now. If we have
+                        // failed, we just want to stop everything and further
+                        // debug the driver code.
+                        job_fence.set_error(EINVAL);
+                        job_fence.signal()?;
+                    }
+
+                    // Let's also mark this group as destroyed just so we don't
+                    // take anymore work. We will come back to this when the
+                    // driver is more developed.
+                    inner.destroyed = true;
+                    Ok(())
+                })?;
+        }
+
+        let ring_db = cs_events & cs::constants::CS_FAULT != 0;
+        Ok(ring_db)
+    }
+
+    pub(crate) fn set_events(&mut self, tdev: &TyrDevice, events: u32) {
+        self.events = Some(events);
+
+        if self.wq.enqueue::<_, 2>(tdev.deref().clone()).is_err() {
+            pr_err!("Failed to enqueue firmware events work\n");
+        }
+    }
+
+    fn update_group(&mut self, group: &Group) -> Result {
+        // TODO: we need to annotate this function with the dma signalling token.
+        // TODO: we cannot sleep in the signalling path.
+        group.with_locked_inner(|inner| {
+            for (queue_idx, queue) in inner.queues.iter_mut().enumerate() {
+                let sync_offset = queue_idx * core::mem::size_of::<syncs::SyncObj64b>();
+                let sync_obj = syncs::SyncObj64b::read(&mut inner.syncobjs, sync_offset)?;
+
+                // TODO: this has to be moved somewhere else. It should probably
+                // be in TyrData, or anywhere else we can easily access from
+                // here. It should also be protected by a SpinLock instead,
+                // because we cannot sleep in the signalling path.
+                for job_fence in &queue.in_flight_jobs {
+                    // We have executed everything up until this point.
+                    if sync_obj.seqno < job_fence.seqno() {
+                        break;
+                    }
+
+                    // Add this debug aid for a while. It will be important
+                    // while we develop the driver.
+                    pr_info!("Signalling fence: {}\n", job_fence.seqno());
+
+                    job_fence.signal()?;
+                }
+
+                // Ok: this does not allocate, so it is ok to use in the signalling path.
+                queue.in_flight_jobs.retain(|fence| !fence.signaled());
+            }
+
+            Ok(())
+        })?;
+
+        Ok(())
+    }
+}
+
+impl_has_work! {
+    impl HasWork<Self, 2> for TyrData {
+        self.fw_events_work
+    }
+}
+
+impl WorkItem<2> for TyrData {
+    type Pointer = Arc<Self>;
+
+    fn run(this: Self::Pointer) {
+        let _ = this.with_locked_scheduler(|sched| {
+            sched.process_events(this.clone()).inspect_err(|e| {
+                pr_err!("Failed to process firmware events: {}", e.to_errno());
+            })
+        });
+    }
+}
+
+impl_has_work! {
+    impl HasWork<Self, 3> for TyrData {
+        self.group_upd_work
+    }
+}
+
+impl WorkItem<3> for TyrData {
+    type Pointer = Arc<Self>;
+
+    fn run(this: Self::Pointer) {
+        let _ = this.with_locked_scheduler(|sched| {
+            while let Some(group) = sched.unsynced_groups.pop() {
+                sched.update_group(&group).inspect_err(|e| {
+                    pr_err!("Failed to process firmware events: {}", e.to_errno());
+                })?;
+            }
+
+            Ok(())
+        });
+    }
+}
diff --git a/drivers/gpu/drm/tyr/sched/group.rs b/drivers/gpu/drm/tyr/sched/group.rs
new file mode 100644
index 0000000000..5af07c302e
--- /dev/null
+++ b/drivers/gpu/drm/tyr/sched/group.rs
@@ -0,0 +1,412 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use core::sync::atomic::AtomicUsize;
+
+use kernel::bits::genmask_u32;
+use kernel::dma_fence::UserFence;
+use kernel::drm::syncobj::SyncObj;
+use kernel::kvec;
+use kernel::new_mutex;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::sync::Mutex;
+use kernel::xarray;
+use kernel::xarray::XArray;
+
+use crate::driver::TyrDevice;
+use crate::file::DrmFile;
+use crate::file::QueueSubmit;
+use crate::fw::global::csg;
+use crate::fw::global::csg::Priority;
+use crate::fw::SharedSectionEntry;
+use crate::gem;
+use crate::mmu::vm::map_flags;
+use crate::mmu::vm::PreparedVm;
+use crate::mmu::vm::Vm;
+use crate::sched::syncs::SyncObj64b;
+use crate::TyrDriver;
+
+use super::job;
+use super::queue::Queue;
+use super::syncs;
+use super::Scheduler;
+
+/// The part of the state protected under the group lock.
+#[pin_data]
+pub(crate) struct GroupInner {
+    /// The group state.
+    pub(crate) state: State,
+
+    /// The group's queues.
+    pub(crate) queues: KVec<Queue>,
+
+    /// The ID of the FW group slot if the group is active.
+    pub(crate) csg_id: Option<usize>,
+
+    /// Bitmask reflecting the blocked queues.
+    pub(crate) blocked_queues: u32,
+
+    /// Bitmask reflecting the idle queues.
+    pub(crate) idle_queues: u32,
+
+    /// Bitmask reflecting the fatal queues.
+    pub(crate) fatal_queues: u32,
+
+    /// True when the group has been destroyed.
+    ///
+    /// If a group is destroyed it becomes useless: no further jobs can be
+    /// submitted to its queues. We simply wait for all references to be
+    /// dropped.
+    pub(crate) destroyed: bool,
+
+    /// The buffer with all the KMD synchronization objects for the group.
+    ///
+    /// There is one syncobj per queue.
+    pub(super) syncobjs: gem::ObjectRef,
+}
+
+impl GroupInner {
+    pub(crate) fn submit(
+        &mut self,
+        in_syncs: &KVec<SyncObj<TyrDriver>>,
+        out_syncs: &KVec<SyncObj<TyrDriver>>,
+        group: Arc<Group>,
+        queue_submit: QueueSubmit,
+        prepared_vm: &PreparedVm<'_>,
+    ) -> Result<UserFence<job::Fence>> {
+        let queue = self
+            .queues
+            .get_mut(queue_submit.queue_index as usize)
+            .ok_or(EINVAL)?;
+
+        let sync_addr = self.syncobjs.kernel_va().ok_or(EINVAL)?;
+        let sync_addr = sync_addr.start
+            + u64::from(queue_submit.queue_index)
+                * core::mem::size_of::<syncs::SyncObj64b>() as u64;
+
+        queue.submit(
+            in_syncs,
+            out_syncs,
+            group,
+            sync_addr,
+            queue_submit,
+            prepared_vm,
+        )
+    }
+}
+
+/// A scheduling group object, usually backing an execution context, e.g.: a
+/// VkQueue or similar.
+///
+/// Commands are submitted to groups via the `GROUP_SUBMIT` ioctl.
+///
+/// Groups are eventually scheduled into hardware CSG slots, and the group's
+/// queues are then scheduled into hardware CS slots for execution.
+#[pin_data]
+pub(crate) struct Group {
+    #[pin]
+    inner: Mutex<GroupInner>,
+
+    /// VM bound to the group.
+    pub(super) vm: Arc<Mutex<Vm>>,
+
+    /// Mask of shader cores that can be used for compute jobs.
+    pub(super) compute_core_mask: u64,
+
+    /// Mask of shader cores that can be used for fragment jobs.
+    pub(super) fragment_core_mask: u64,
+
+    /// Mask of tiler cores that can be used for tiler jobs.
+    pub(super) tiler_core_mask: u64,
+
+    /// Maximum number of shader cores used for compute jobs.
+    pub(super) max_compute_cores: u8,
+
+    /// Maximum number of shader cores used for fragment jobs.
+    pub(super) max_fragment_cores: u8,
+
+    /// Maximum number of tiler cores used for tiler jobs.
+    pub(super) max_tiler_cores: u8,
+
+    /// Suspend buffer.
+    ///
+    /// Stores the state of the group and its queues when a group is suspended.
+    ///
+    /// Used at resume time to restore the group to its previous state.
+    ///
+    /// The size of the suspend buffer is exposed through the FW interface.
+    pub(super) suspend_buf: gem::ObjectRef,
+
+    /// Protected-mode suspend buffer.
+    ///
+    /// Stores the state of the group and its queues when a group is suspended.
+    ///
+    /// Used at resume time to restore the group to its previous state.
+    ///
+    /// The size of the suspend buffer is exposed through the FW interface.
+    pub(super) protm_suspend_buf: gem::ObjectRef,
+
+    /// The group's priority.
+    pub(super) priority: Priority,
+}
+
+impl Group {
+    pub(super) fn create(
+        tdev: &TyrDevice,
+        file: &DrmFile,
+        group_args: &kernel::uapi::drm_panthor_group_create,
+        queue_args: KVec<crate::file::QueueCreate>,
+    ) -> Result<Arc<Self>> {
+        let gpu_info = &tdev.gpu_info;
+        let fw = &tdev.fw;
+
+        if group_args.pad != 0 {
+            pr_err!("group_create: invalid padding {}", group_args.pad);
+            return Err(EINVAL);
+        }
+
+        if group_args.priority > csg::Priority::num_priorities() as u8 {
+            pr_err!("group_create: invalid priority {}", group_args.priority);
+            return Err(EINVAL);
+        }
+
+        if (group_args.compute_core_mask & !gpu_info.shader_present) != 0
+            || (group_args.fragment_core_mask & !gpu_info.shader_present) != 0
+            || (group_args.tiler_core_mask & !gpu_info.tiler_present) != 0
+        {
+            pr_err!("group_create: invalid core mask");
+            return Err(EINVAL);
+        }
+
+        if group_args.compute_core_mask.count_ones() > group_args.max_compute_cores as u32
+            || group_args.fragment_core_mask.count_ones() > group_args.max_fragment_cores as u32
+            || group_args.tiler_core_mask.count_ones() > group_args.max_tiler_cores as u32
+        {
+            pr_err!("group_create: asking for more cores than the maximum allowed for the group");
+            return Err(EINVAL);
+        }
+
+        let vm = file
+            .inner()
+            .vm_pool()
+            .get_vm(group_args.vm_id as usize)
+            .ok_or(EINVAL)?;
+
+        let (suspend_buf_size, protm_suspend_buf_size) =
+            fw.with_locked_global_iface(|glb_iface| {
+                let csg = glb_iface.csg(0).ok_or(EINVAL)?;
+                let control = csg.read_control()?;
+
+                Ok((control.suspend_size, control.protm_suspend_size))
+            })?;
+
+        let suspend_buf = fw.alloc_suspend_buf(tdev, suspend_buf_size as usize)?;
+        let protm_suspend_buf = fw.alloc_suspend_buf(tdev, protm_suspend_buf_size as usize)?;
+
+        let num_syncs = group_args.queues.count as usize * core::mem::size_of::<SyncObj64b>();
+        let mut syncobjs = gem::new_kernel_object(
+            tdev,
+            tdev.iomem.clone(),
+            vm.clone(),
+            gem::KernelVaPlacement::Auto { size: num_syncs },
+            map_flags::Flags::from(map_flags::NOEXEC) | map_flags::Flags::from(map_flags::UNCACHED),
+        )?;
+
+        let vmap = syncobjs.vmap()?;
+        vmap.as_mut_slice().fill(0);
+
+        let mut queues = kvec![];
+        for i in 0..group_args.queues.count {
+            let queue = Queue::new(tdev, &queue_args[i as usize], vm.clone())?;
+            queues.push(queue, GFP_KERNEL)?;
+        }
+
+        let idle_queues = genmask_u32(queues.len() as u32 - 1, 0);
+        let priority = group_args.priority.try_into()?;
+
+        Arc::pin_init(
+            pin_init!(Group {
+                inner <- new_mutex!(GroupInner {
+                    state: State::Created,
+                    queues,
+                    csg_id: None,
+                    blocked_queues: 0,
+                    idle_queues,
+                    fatal_queues:0,
+                    destroyed: false,
+                    syncobjs,
+                }),
+                vm,
+                compute_core_mask: group_args.compute_core_mask,
+                fragment_core_mask: group_args.fragment_core_mask,
+                tiler_core_mask: group_args.tiler_core_mask,
+                max_compute_cores: group_args.max_compute_cores,
+                max_fragment_cores: group_args.max_fragment_cores,
+                max_tiler_cores: group_args.max_tiler_cores,
+                suspend_buf,
+                protm_suspend_buf,
+                priority,
+            }),
+            GFP_KERNEL,
+        )
+    }
+
+    pub(super) fn idle(&self, sched: &Scheduler) -> bool {
+        let inner = self.inner.lock();
+        if let Some(csg_id) = inner.csg_id {
+            match &sched.csg_slots[csg_id] {
+                Some(csg) => csg.idle,
+                None => true,
+            }
+        } else {
+            let inactive_queues = inner.blocked_queues | inner.idle_queues;
+            inactive_queues.count_ones() == inner.queues.len() as u32
+        }
+    }
+
+    /// Provide access to the part of the group we may want to mutate.
+    ///
+    /// This uses a closure in order to reduce the scope of the lock.
+    pub(crate) fn with_locked_inner<F, R>(&self, f: F) -> Result<R>
+    where
+        F: FnOnce(&mut GroupInner) -> Result<R>,
+    {
+        let mut inner = self.inner.lock();
+        f(&mut inner)
+    }
+
+    pub(super) fn submit(
+        self: Arc<Self>,
+        in_syncs: KVec<SyncObj<TyrDriver>>,
+        out_syncs: KVec<SyncObj<TyrDriver>>,
+        queue_submits: KVec<QueueSubmit>,
+    ) -> Result<KVec<UserFence<job::Fence>>> {
+        if self.vm.lock().address_space().is_none() {
+            pr_err!("group_submit: invalid address space");
+            return Err(EINVAL);
+        }
+
+        let destroyed = self.with_locked_inner(|inner| Ok(inner.destroyed))?;
+
+        if destroyed {
+            pr_err!("group_submit: invalid group: group is destroyed");
+            return Err(EINVAL);
+        }
+
+        let mut fences = KVec::with_capacity(queue_submits.len(), GFP_KERNEL)?;
+
+        let vm = self.vm.lock();
+        vm.with_prepared_vm(queue_submits.len() as u32, |locked_vm| {
+            queue_submits.into_iter().try_for_each(|queue_submit| {
+                let fence = self.with_locked_inner(|inner| {
+                    inner.submit(
+                        &in_syncs,
+                        &out_syncs,
+                        self.clone(),
+                        queue_submit,
+                        &locked_vm,
+                    )
+                })?;
+
+                fences.push(fence, GFP_KERNEL)?;
+                Ok(())
+            })
+        })?;
+
+        Ok(fences)
+    }
+}
+
+/// Represents the scheduling group state.
+#[derive(Debug, Copy, Clone, PartialEq, Eq)]
+pub(crate) enum State {
+    /// Group was created, but not scheduled yet.
+    Created,
+
+    /// Group is currently scheduled.
+    Active,
+
+    /// Group was scheduled at least once, but is inactive/suspended right now.
+    Suspended,
+
+    /// Group was terminated.
+    ///
+    /// Can no longer be scheduled. The only allowed action is destruction.
+    Terminated,
+
+    /// Group is in an unknown state.
+    ///
+    /// The firmware returned an inconsistent state. The group is flagged as
+    /// unusable and can no longer be scheduled. The only allowed action is
+    /// destruction.
+    ///
+    /// When this happens, a firmware reset is also scheduled to start from a
+    /// fresh state.
+    Unknown,
+}
+
+/// The group pool.
+///
+/// Each context (i.e. DrmFile) has its own group pool.
+// TODO: this is essentially the same as vm/pool.rs. It can be trivially
+// refactored into a single type later.
+pub(crate) struct Pool {
+    xa: Pin<KBox<XArray<Arc<Group>>>>,
+    free_index: AtomicUsize,
+}
+
+impl Pool {
+    pub(crate) fn create() -> Result<Self> {
+        let xa = KBox::pin_init(XArray::new(xarray::AllocKind::Alloc1), GFP_KERNEL)?;
+
+        Ok(Self {
+            xa,
+            free_index: AtomicUsize::new(1),
+        })
+    }
+
+    pub(crate) fn create_group(
+        self: Pin<&Self>,
+        tdev: &TyrDevice,
+        groupcreate: &mut kernel::uapi::drm_panthor_group_create,
+        file: &DrmFile,
+        queue_args: KVec<crate::file::QueueCreate>,
+    ) -> Result<usize> {
+        let group = Group::create(tdev, file, groupcreate, queue_args)?;
+
+        tdev.with_locked_scheduler(|sched| {
+            sched.idle_groups[group.priority as usize]
+                .push(group.clone(), GFP_KERNEL)
+                .map_err(|_| ENOMEM)
+        })?;
+
+        let index = self
+            .free_index
+            .fetch_add(1, core::sync::atomic::Ordering::Relaxed);
+
+        let xa = self.xa.as_ref();
+        let mut guard = xa.lock();
+        guard.store(index, group, GFP_KERNEL).map_err(|_| EINVAL)?;
+
+        Ok(index)
+    }
+
+    pub(crate) fn group(self: Pin<&Self>, index: usize) -> Option<Arc<Group>> {
+        let xa = self.xa.as_ref();
+        let guard = xa.lock();
+        let group = guard.get(index)?;
+        Some(group.into())
+    }
+
+    pub(crate) fn destroy_group(self: Pin<&Self>, index: usize) -> Result {
+        let xa = self.xa.as_ref();
+
+        let mut guard = xa.lock();
+        let group = guard.remove(index).ok_or(EINVAL)?;
+
+        let mut group = group.inner.lock();
+        group.destroyed = true;
+
+        Ok(())
+    }
+}
diff --git a/drivers/gpu/drm/tyr/sched/job.rs b/drivers/gpu/drm/tyr/sched/job.rs
new file mode 100644
index 0000000000..88a510a7fd
--- /dev/null
+++ b/drivers/gpu/drm/tyr/sched/job.rs
@@ -0,0 +1,218 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use kernel::bits::genmask_u32;
+use kernel::bits::genmask_u64;
+use kernel::c_str;
+use kernel::dma_fence;
+use kernel::dma_fence::FenceObject;
+use kernel::dma_fence::FenceOps;
+use kernel::dma_fence::RawDmaFence;
+use kernel::drm::sched::JobImpl;
+use kernel::kvec;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+
+use crate::sched::group::Group;
+
+pub(crate) struct Job {
+    /// The group whose queue this job will be pushed to.
+    group: Arc<Group>,
+
+    /// Index of the queue inside the group.
+    queue_idx: usize,
+
+    /// Start address of the userspace command stream.
+    stream_addr: u64,
+
+    /// Size of the userspace command stream.
+    stream_size: u32,
+
+    /// The position of the job in the ringbuffer, if any.
+    ringbuf_pos: Option<RingBufferPosition>,
+
+    /// The fence to signal when the job is done.
+    done_fence: dma_fence::UserFence<Fence>,
+
+    /// The address of the sync object for the queue.
+    ///
+    /// This is here for convenience, so it's ready to be consumed in the run
+    /// callback.
+    sync_addr: u64,
+}
+
+impl Job {
+    pub(crate) fn create(
+        qsubmit: crate::file::QueueSubmit,
+        group: Arc<Group>,
+        done_fence: dma_fence::UserFence<Fence>,
+        sync_addr: u64,
+    ) -> Result<Self> {
+        if qsubmit.pad != 0 {
+            pr_err!("job_create: invalid padding {}\n", qsubmit.pad);
+            return Err(EINVAL);
+        }
+
+        if (qsubmit.stream_size == 0) != (qsubmit.stream_addr == 0) {
+            pr_err!("job_create: stream address and stream size must be both 0 or non-zero\n");
+            return Err(EINVAL);
+        }
+
+        if qsubmit.stream_addr & 63 != 0 || qsubmit.stream_size & 7 != 0 {
+            pr_err!("job_create: stream address must be aligned to 64 bytes and stream size must be aligned to 8 bytes\n");
+            return Err(EINVAL);
+        }
+
+        if qsubmit.latest_flush & genmask_u32(30, 24) != 0 {
+            pr_err!("job_create: latest_flust[30:24] must be zero\n");
+            return Err(EINVAL);
+        }
+
+        Ok(Job {
+            group: group.clone(),
+            queue_idx: qsubmit.queue_index as usize,
+            stream_addr: qsubmit.stream_addr,
+            stream_size: qsubmit.stream_size,
+            ringbuf_pos: None,
+            done_fence,
+            sync_addr,
+        })
+    }
+}
+
+impl JobImpl for Job {
+    // This is in the dma signalling path. Do _not_ allocate here.
+    fn run(job: &mut kernel::drm::sched::Job<Self>) -> Result<Option<kernel::dma_fence::Fence>> {
+        // TODO: use a fixed-size array instead.
+        let mut instrs = kvec![];
+
+        // We are choosing these registers arbitrarily, but they might be used
+        // by userspace. Down the line, we will have to address this.
+        let addr_reg = 92;
+        let val_reg = 94;
+
+        let opcode = 2; // MOV32
+        let latest_flush_regnum = val_reg;
+        let latest_flush = 0;
+        let mov_latest_flush: u64 = opcode << 56 | latest_flush_regnum << 48 | latest_flush;
+
+        let opcode = 36; //FLUSH_CACHE2
+        let flush_cache: u64 = opcode << 56 | 0 << 48 | latest_flush_regnum << 40 | 0 << 16 | 0x233;
+
+        let opcode = 1; // MOV48
+        let cs_start_regnum = addr_reg;
+        let mov_cs_start: u64 = opcode << 56 | cs_start_regnum << 48 | job.stream_addr;
+
+        let opcode = 2; // MOV32
+        let cs_size_regnum = val_reg;
+        let mov_cs_size: u64 = opcode << 56 | cs_size_regnum << 48 | u64::from(job.stream_size);
+
+        let opcode = 32; // CALL
+        let call: u64 = opcode << 56 | cs_start_regnum << 40 | cs_size_regnum << 32;
+
+        let opcode = 1; // MOV48
+        let sync_addr_regnum = addr_reg;
+        let mov_sync_addr: u64 = opcode << 56 | sync_addr_regnum << 48 | job.sync_addr;
+
+        // Load the actual "1" constant into a register. SYNC_ADD cannot take
+        // this as an immediate.
+        let opcode = 1; // MOV48
+        let sync_val_regnum = val_reg;
+        let mov_sync_val: u64 = opcode << 56 | sync_val_regnum << 48 | 1;
+
+        // Wait before _all_ assynchronous work spawned by the user CS is done.
+        let opcode = 3; // WAIT(all)
+
+        // Use this default for now. This should work for the rk3588 where it's
+        // being tested.
+        let wait_all_mask = genmask_u64(7, 0);
+        let wait_all: u64 = opcode << 56 | wait_all_mask << 16;
+
+        let opcode = 51; // SYNC_ADD64
+        let sync_sb_entry = 0;
+        let sync_sb_mask = 0;
+        let sync_scope = 0;
+        let sync_err_propagate = 1;
+        let sync_add: u64 = opcode << 56
+            | sync_sb_entry << 48
+            | sync_addr_regnum << 40
+            | sync_val_regnum << 32
+            | sync_sb_mask << 16
+            | sync_scope << 1
+            | sync_err_propagate;
+
+        let opcode = 47; // ERROR_BARRIER
+        let error_barrier: u64 = opcode << 56;
+
+        instrs.extend_from_slice(&mov_latest_flush.to_le_bytes(), GFP_KERNEL)?;
+        instrs.extend_from_slice(&flush_cache.to_le_bytes(), GFP_KERNEL)?;
+        instrs.extend_from_slice(&mov_cs_start.to_le_bytes(), GFP_KERNEL)?;
+        instrs.extend_from_slice(&mov_cs_size.to_le_bytes(), GFP_KERNEL)?;
+        instrs.extend_from_slice(&call.to_le_bytes(), GFP_KERNEL)?;
+        instrs.extend_from_slice(&mov_sync_addr.to_le_bytes(), GFP_KERNEL)?;
+        instrs.extend_from_slice(&mov_sync_val.to_le_bytes(), GFP_KERNEL)?;
+        instrs.extend_from_slice(&wait_all.to_le_bytes(), GFP_KERNEL)?;
+        instrs.extend_from_slice(&sync_add.to_le_bytes(), GFP_KERNEL)?;
+        instrs.extend_from_slice(&error_barrier.to_le_bytes(), GFP_KERNEL)?;
+
+        let pad = instrs.len().next_multiple_of(8) - instrs.len();
+
+        // Pad until the next 8-byte boundary with NOPs to please the
+        // prefetcher.
+        for _ in 0..pad {
+            instrs.push(0, GFP_KERNEL)?;
+        }
+
+        let ringbuf_pos = job.group.with_locked_inner(|inner| {
+            let queue = inner.queues.get_mut(job.queue_idx).ok_or(EINVAL)?;
+            let input = queue.interfaces.read_input()?;
+
+            let ringbuf_pos = RingBufferPosition {
+                start: input.insert,
+                end: input.insert + instrs.len() as u64,
+            };
+
+            queue.append_instrs(&instrs)?;
+
+            // Push the fence before kicking the queue.
+            queue
+                .in_flight_jobs
+                .push(job.done_fence.clone(), GFP_KERNEL)?;
+
+            queue.kick()?;
+            Ok(ringbuf_pos)
+        })?;
+
+        job.ringbuf_pos = Some(ringbuf_pos);
+
+        Ok(Some(kernel::dma_fence::Fence::from_fence(&job.done_fence)))
+    }
+
+    fn timed_out(job: &mut kernel::drm::sched::Job<Self>) -> kernel::drm::sched::Status {
+        pr_err!("Job timed out\n");
+
+        job.done_fence.set_error(ETIMEDOUT);
+        let _ = job.done_fence.signal();
+
+        kernel::drm::sched::Status::NoDevice
+    }
+}
+
+struct RingBufferPosition {
+    start: u64,
+    end: u64,
+}
+
+pub(crate) struct Fence;
+
+#[vtable]
+impl FenceOps for Fence {
+    const USE_64BIT_SEQNO: bool = true;
+
+    fn get_driver_name<'a>(self: &'a FenceObject<Self>) -> &'a CStr {
+        c_str!("tyr")
+    }
+
+    fn get_timeline_name<'a>(self: &'a FenceObject<Self>) -> &'a CStr {
+        c_str!("tyr_fence")
+    }
+}
diff --git a/drivers/gpu/drm/tyr/sched/queue.rs b/drivers/gpu/drm/tyr/sched/queue.rs
new file mode 100644
index 0000000000..ec81dc859e
--- /dev/null
+++ b/drivers/gpu/drm/tyr/sched/queue.rs
@@ -0,0 +1,271 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use core::ops::Range;
+
+use kernel::c_str;
+use kernel::devres::Devres;
+use kernel::dma_fence::FenceContexts;
+use kernel::dma_fence::UserFence;
+use kernel::drm::sched;
+use kernel::drm::sched::Entity;
+use kernel::drm::sched::Scheduler;
+use kernel::drm::syncobj::SyncObj;
+use kernel::io::mem::IoMem;
+use kernel::prelude::*;
+use kernel::sizes::SZ_4K;
+use kernel::sizes::SZ_64K;
+use kernel::sync::Arc;
+use kernel::sync::Mutex;
+
+use crate::driver::TyrDevice;
+use crate::file::QueueCreate;
+use crate::file::QueueSubmit;
+use crate::fw::global::cs::RingBufferInput;
+use crate::fw::global::cs::RingBufferOutput;
+use crate::gem;
+use crate::mmu::vm::map_flags;
+use crate::mmu::vm::PreparedVm;
+use crate::mmu::vm::Vm;
+use crate::regs::Doorbell;
+use crate::sched::job::Job;
+use crate::TyrDriver;
+
+use super::group::Group;
+use super::job;
+
+const JOB_TIMEOUT_MS: usize = 5000;
+pub(crate) const CSF_MAX_QUEUE_PRIO: u32 = 15;
+
+/// Represents a hardware executiion queue.
+pub(crate) struct Queue {
+    /// The DRM scheduler used for this queue.
+    scheduler: Scheduler<Job>,
+
+    /// The DRM entity used for this queue.
+    entity: Entity<Job>,
+
+    /// A priority number, between 0 and 15.
+    pub(crate) priority: u8,
+
+    // Doorbell assigned to this queue, if any.
+    //
+    // Doorbell assignment happens when the group that owns this queue is bound
+    // to a specific hardware slot.
+    //
+    // Right now, all groups share the same doorbell, and the doorbell ID
+    // is assigned to `group_slot + 1` when the group is assigned a slot.
+    // However, we might decide to provide fine-grained doorbell assignment
+    // at some point, so we don't have to wake up all queues in a group
+    // every time one of them is updated.
+    pub(crate) doorbell_id: Option<usize>,
+
+    /// The ring buffer used to communicate with the firmware.
+    pub(super) ringbuf: gem::ObjectRef,
+
+    pub(super) interfaces: Interfaces,
+
+    iomem: Arc<Devres<IoMem>>,
+
+    pub(super) fence_ctx: FenceContexts,
+
+    /// The in-flight jobs for this queue.
+    pub(super) in_flight_jobs: KVec<UserFence<job::Fence>>,
+}
+
+impl Queue {
+    pub(crate) fn new(
+        tdev: &TyrDevice,
+        queue_args: &QueueCreate,
+        vm: Arc<Mutex<Vm>>,
+    ) -> Result<Self> {
+        // ugh..
+        let queue_args = &queue_args.0;
+
+        if queue_args.pad[0] != 0 || queue_args.pad[1] != 0 || queue_args.pad[2] != 0 {
+            return Err(EINVAL);
+        }
+
+        if queue_args.ringbuf_size < SZ_4K as u32
+            || queue_args.ringbuf_size > SZ_64K as u32
+            || !queue_args.ringbuf_size.is_power_of_two()
+        {
+            pr_err!("Invalid ring buffer size: {:#x}\n", queue_args.ringbuf_size);
+            return Err(EINVAL);
+        }
+
+        if u32::from(queue_args.priority) > CSF_MAX_QUEUE_PRIO {
+            pr_err!("Invalid queue priority: {:#x}\n", queue_args.priority);
+            return Err(EINVAL);
+        }
+
+        let priority = queue_args.priority;
+        let credit_limit = queue_args.ringbuf_size / core::mem::size_of::<u64>() as u32;
+
+        let scheduler = Scheduler::new(
+            tdev.as_ref(),
+            1,
+            credit_limit,
+            0,
+            JOB_TIMEOUT_MS,
+            c_str!("tyr-queue"),
+        )?;
+
+        let entity = Entity::new(&scheduler, sched::Priority::Kernel)?;
+
+        let iomem = tdev.iomem.clone();
+        let ringbuf = gem::new_kernel_object(
+            tdev,
+            iomem.clone(),
+            vm.clone(),
+            gem::KernelVaPlacement::Auto {
+                size: queue_args.ringbuf_size as usize,
+            },
+            map_flags::Flags::from(map_flags::NOEXEC) | map_flags::Flags::from(map_flags::UNCACHED),
+        )?;
+
+        let mem = tdev.fw.alloc_queue_mem(tdev)?;
+
+        let input_va = mem.kernel_va().ok_or(EINVAL)?;
+        let output_start = input_va.start + SZ_4K as u64;
+        let output_end = output_start + SZ_4K as u64;
+        let output_va = output_start..output_end;
+
+        let interfaces = Interfaces {
+            mem,
+            input_va,
+            output_va,
+            input_offset: 0,
+            output_offset: SZ_4K,
+        };
+
+        let fence_ctx = FenceContexts::new(1, c_str!("tyr_fence"), None)?;
+
+        Ok(Queue {
+            scheduler,
+            entity,
+            doorbell_id: None,
+            priority,
+            ringbuf,
+            interfaces,
+            iomem,
+            fence_ctx,
+            in_flight_jobs: KVec::new(),
+        })
+    }
+
+    /// Append instructions to this queue for execution.
+    ///
+    /// The queue's doorbell needs to be rung after this function is called in
+    /// order to get CSF to act on the new values.
+    pub(crate) fn append_instrs(&mut self, instrs: &[u8]) -> Result {
+        let mut ringbuf_input = self.interfaces.read_input()?;
+        let ringbuf_sz = self.ringbuf.size() as u64;
+
+        let cs_insert = ringbuf_input.insert & (ringbuf_sz - 1);
+        let cs_insert = cs_insert as usize;
+
+        let range = cs_insert..cs_insert + instrs.len();
+
+        let ringbuf = self.ringbuf.vmap()?;
+        let ringbuf = ringbuf.as_mut_slice();
+
+        ringbuf[range].copy_from_slice(instrs);
+
+        // Make sure that the ring buffer is updated before the INSERT register.
+        kernel::sync::barrier::smp_wmb();
+
+        ringbuf_input.insert += instrs.len() as u64;
+
+        self.interfaces.write_input(ringbuf_input)?;
+        Ok(())
+    }
+
+    /// Kick the queue. This will notify CSF that new instructions are ready to
+    /// be executed.
+    pub(crate) fn kick(&self) -> Result {
+        Doorbell::new(self.doorbell_id.ok_or(EINVAL)?).write(&self.iomem, 1)
+    }
+
+    pub(crate) fn submit(
+        &mut self,
+        in_syncs: &KVec<SyncObj<TyrDriver>>,
+        out_syncs: &KVec<SyncObj<TyrDriver>>,
+        group: Arc<Group>,
+        sync_addr: u64,
+        queue_submit: QueueSubmit,
+        _: &PreparedVm<'_>,
+    ) -> Result<UserFence<job::Fence>> {
+        let fence: UserFence<_> = self
+            .fence_ctx
+            .new_fence(0, crate::sched::job::Fence)?
+            .into();
+
+        let job = Job::create(queue_submit, group, fence.clone(), sync_addr)?;
+
+        let mut job = self.entity.new_job(1, job)?.arm();
+        let out_fence = job.fences().finished();
+
+        job.push();
+
+        for sync in out_syncs {
+            sync.replace_fence(Some(&out_fence));
+        }
+
+        Ok(fence)
+    }
+}
+
+/// The interface for ring buffer control.
+pub(crate) struct Interfaces {
+    /// The memory used to hold the user input/output blocks.
+    mem: gem::ObjectRef,
+
+    /// The input VA range for the interface.
+    pub(super) input_va: Range<u64>,
+
+    /// The output VA range for the interface.
+    pub(super) output_va: Range<u64>,
+
+    /// The input area for the ring buffer control.
+    input_offset: usize,
+    /// The output area for the ring buffer control.
+    output_offset: usize,
+}
+
+impl Interfaces {
+    pub(super) fn read_input(&mut self) -> Result<RingBufferInput> {
+        let vmap = self.mem.vmap()?;
+        let input = unsafe {
+            vmap.as_mut_ptr()
+                .add(self.input_offset)
+                .cast::<RingBufferInput>()
+                .read_volatile()
+        };
+
+        Ok(input)
+    }
+
+    pub(super) fn write_input(&mut self, value: RingBufferInput) -> Result {
+        let vmap = self.mem.vmap()?;
+        unsafe {
+            vmap.as_mut_ptr()
+                .add(self.input_offset)
+                .cast::<RingBufferInput>()
+                .write_volatile(value)
+        };
+
+        Ok(())
+    }
+
+    pub(super) fn read_output(&mut self) -> Result<RingBufferOutput> {
+        let vmap = self.mem.vmap()?;
+        let output = unsafe {
+            vmap.as_mut_ptr()
+                .add(self.output_offset)
+                .cast::<RingBufferOutput>()
+                .read_volatile()
+        };
+
+        Ok(output)
+    }
+}
diff --git a/drivers/gpu/drm/tyr/sched/syncs.rs b/drivers/gpu/drm/tyr/sched/syncs.rs
new file mode 100644
index 0000000000..a652b47975
--- /dev/null
+++ b/drivers/gpu/drm/tyr/sched/syncs.rs
@@ -0,0 +1,124 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! GPU synchronization objects.
+//!
+//! Synchronization objects allows for general synchronization between command
+//! streams and any other actor (e.g. host CPU, other command streams or other
+//! hardware devices).
+
+use kernel::prelude::*;
+
+use crate::gem;
+
+/// Represents a 32-bit firmware synchronization object.
+#[repr(C)]
+pub(crate) struct SyncObj32b {
+    /// Sequence number.
+    pub(crate) seqno: u32,
+
+    /// Status.
+    ///
+    /// Non-zero on failure.
+    pub(crate) status: u32,
+}
+
+/// Represents a 64-bit firmware synchronization object.
+#[repr(C)]
+pub(crate) struct SyncObj64b {
+    /// Sequence number.
+    pub(crate) seqno: u64,
+
+    /// Status.
+    ///
+    /// Non-zero on failure.
+    pub(crate) status: u32,
+
+    /// Padding (must be zero).
+    pub(crate) pad: u32,
+}
+
+pub(crate) enum SyncObj {
+    /// 32-bit sync object.
+    SyncObj32(SyncObj32b),
+
+    /// 64-bit sync object.
+    SyncObj64(SyncObj64b),
+}
+
+pub(crate) struct SyncRef {
+    /// The memory where the synchronization object is stored.
+    mem: gem::ObjectRef,
+
+    /// The offset in the memory where the synchronization object is stored.
+    mem_offset: usize,
+
+    /// Whether this is a sync64 object.
+    is_sync64: bool,
+
+    /// Whether this is a greater than comparison against the reference value.
+    pub(super) greater_than: bool,
+
+    /// The reference value to compare against.
+    pub(super) reference_value: u64,
+}
+
+impl SyncRef {
+    pub(super) fn read(&mut self) -> Result<SyncObj> {
+        if self.is_sync64 {
+            let sync_obj = SyncObj64b::read(&mut self.mem, self.mem_offset)?;
+            Ok(SyncObj::SyncObj64(sync_obj))
+        } else {
+            let sync_obj = SyncObj32b::read(&mut self.mem, self.mem_offset)?;
+            Ok(SyncObj::SyncObj32(sync_obj))
+        }
+    }
+}
+
+macro_rules! impl_sync_rw {
+    ($type:ty) => {
+        impl $type {
+            /// Reads a synchronization object at a given offset.
+            ///
+            /// Note that the area pointed to by `ptr` is shared with the GPU, so we
+            /// cannot simply parse it or cast it to &Self.
+            ///
+            /// Merely taking a reference to it would be UB, as the GPU can change the
+            /// underlying memory at any time, as it is a core running on its own.
+            pub(super) fn read(mem: &mut gem::ObjectRef, offset: usize) -> Result<Self> {
+                if offset > mem.size() {
+                    return Err(EINVAL);
+                }
+
+                let vmap = mem.vmap()?;
+                let ptr = unsafe { vmap.as_mut_ptr().add(offset).cast::<Self>() };
+                // SAFETY: we know that this pointer is aligned and valid for reads for
+                // at least size_of::<Self>() bytes.
+                Ok(unsafe { core::ptr::read_volatile(ptr) })
+            }
+
+            /// Writes a synchronization object at a given offset.
+            ///
+            /// Note that the area pointed to by `ptr` is shared with the GPU, so we
+            /// cannot simply parse it or cast it to &Self.
+            ///
+            /// Merely taking a reference to it would be UB, as the GPU can change the
+            /// underlying memory at any time, as it is a core running on its own.
+            pub(super) fn write(mem: &mut gem::ObjectRef, offset: usize, value: Self) -> Result {
+                if offset > mem.size() {
+                    return Err(EINVAL);
+                }
+
+                let vmap = mem.vmap()?;
+                let ptr = unsafe { vmap.as_mut_ptr().add(offset).cast::<Self>() };
+                // SAFETY: we know that this pointer is aligned and valid for writes for
+                // at least size_of::<Self>() bytes.
+                unsafe { core::ptr::write_volatile(ptr, value) };
+
+                Ok(())
+            }
+        }
+    };
+}
+
+impl_sync_rw!(SyncObj32b);
+impl_sync_rw!(SyncObj64b);
diff --git a/drivers/gpu/drm/tyr/sched/tick.rs b/drivers/gpu/drm/tyr/sched/tick.rs
new file mode 100644
index 0000000000..e8d2df55bd
--- /dev/null
+++ b/drivers/gpu/drm/tyr/sched/tick.rs
@@ -0,0 +1,36 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+use kernel::impl_has_work;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::workqueue::WorkItem;
+
+use crate::driver::TyrData;
+use crate::sched::group::Group;
+
+use super::Scheduler;
+
+pub(super) struct Tick {
+    groups: KVec<Arc<Group>>,
+    idle_group_count: usize,
+}
+
+impl Tick {
+    fn full(&self, sched: &Scheduler) -> bool {
+        self.groups.len() as u32 == sched.csg_slot_count
+    }
+}
+
+impl_has_work! {
+    impl HasWork<Self, 1> for TyrData {
+        self.tick_work
+    }
+}
+
+impl WorkItem<1> for TyrData {
+    type Pointer = Arc<Self>;
+
+    fn run(this: Self::Pointer) {
+        let _ = this.with_locked_scheduler(|sched| Ok(()));
+    }
+}
diff --git a/drivers/gpu/drm/tyr/tyr.rs b/drivers/gpu/drm/tyr/tyr.rs
new file mode 100644
index 0000000000..3ed9f97ba0
--- /dev/null
+++ b/drivers/gpu/drm/tyr/tyr.rs
@@ -0,0 +1,55 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//!Rust driver for ARM Mali CSF-based GPUs
+//!
+//! The skeleton is basically taken from Nova and also rust_platform_driver.rs.
+//!
+//! So far, this is just a very early-stage experiment, but it looks promissing:
+//!
+//! - We use the same uAPI as Panthor, although this needs a bit of work, since
+//!   bindgen cannot translate #defines into Rust.
+//!
+//! - The DRM registration and a few IOCTLs are implemented. There is an igt
+//!   branch with tests.
+//!
+//! - Basic iomem and register set implementation, so it's possible to program
+//! the device.
+//!
+//! - IRQ handling, so we can receive notifications from the device.
+//!
+//! - We can boot the firmware.
+//!
+//! - We can communicate with CSF using the global interface. We can submit
+//!   requests and the MCU will appropriately respond in the ack field.
+//!
+//! - There is GEM_CREATE and VM_BIND support.
+//! - We can send a PING request to CSF, and it will acknowledge it
+//!   successfully.
+//!
+//! Notably missing (apart from literally everything else):
+//! - Job subission logic through drm_scheduler and completion through dma_fences
+//! - Devfreq, pm_idle, etc.
+//!
+//! The name "Tyr" is inspired by Norse mythology, reflecting ARM's tradition of
+//! naming their GPUs after Nordic mythological figures and places.
+
+use crate::driver::TyrDriver;
+
+mod driver;
+mod file;
+mod flags;
+mod fw;
+mod gem;
+mod gpu;
+mod mmu;
+mod regs;
+mod sched;
+mod wait;
+
+kernel::module_platform_driver! {
+    type: TyrDriver,
+    name: "tyr",
+    author: "The Tyr driver authors",
+    description: "Rust driver for ARM Mali CSF-based GPUs",
+    license: "Dual MIT/GPL",
+}
diff --git a/drivers/gpu/drm/tyr/wait.rs b/drivers/gpu/drm/tyr/wait.rs
new file mode 100644
index 0000000000..74066be2de
--- /dev/null
+++ b/drivers/gpu/drm/tyr/wait.rs
@@ -0,0 +1,142 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! Code to wait on CSF responses.
+
+use kernel::new_condvar;
+use kernel::new_mutex;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::sync::CondVar;
+use kernel::sync::Mutex;
+use kernel::time::msecs_to_jiffies;
+
+#[pin_data]
+/// A convenience type to wait for GPU responses.
+pub(crate) struct Wait<T = ()> {
+    /// The actual wait/signal mechanism.
+    #[pin]
+    cond: CondVar,
+
+    /// Serializes the waking up process.
+    ///
+    /// All waiters will attempt to reacquire this lock, thereby providing
+    /// mutual exclusion between themselves.
+    ///
+    /// Any other locks can be acquired through the variables captured by the
+    /// closure in [`Self::wait_interruptible_timeout`].
+    #[pin]
+    lock: Mutex<T>,
+}
+
+impl Wait<()> {
+    /// A convenience function to initialize the `Wait` struct.
+    pub(crate) fn new() -> Result<Arc<Self>> {
+        Arc::pin_init(
+            pin_init!(Self {
+                cond <- new_condvar!(),
+                lock <- new_mutex!(()),
+            }),
+            GFP_KERNEL,
+        )
+    }
+}
+
+impl<T> Wait<T> {
+    /// A convenience function to initialize the `Wait` struct.
+    ///
+    /// `data` is automatically protected by the Wait instance.
+    pub(crate) fn new_with_data(data: T) -> Result<Arc<Self>> {
+        Arc::pin_init(
+            pin_init!(Self {
+                cond <- new_condvar!(),
+                lock <- new_mutex!(data),
+            }),
+            GFP_KERNEL,
+        )
+    }
+
+    /// Wait until the GPU responds.
+    ///
+    /// This will trigger on all responses and it is up to the caller to react
+    /// using the passed-in closure `on_woken`.
+    ///
+    /// If the wakeup is spurious, or caused by an unrelated response, return [`WaitResult::Retry`].
+    pub(crate) fn wait_interruptible_timeout<F>(
+        &self,
+        timeout_ms: u32,
+        mut on_woken: F,
+    ) -> Result<()>
+    where
+        F: FnMut(&mut T) -> Result<WaitResult>,
+    {
+        let mut guard = self.lock.lock();
+        let mut remaining_time = msecs_to_jiffies(timeout_ms);
+
+        loop {
+            // Before going to sleep, we must give the caller one final opportunity
+            // to check if the condition is true while holding the lock.
+            //
+            // Skipping this step could lead to a race condition where another
+            // thread has already signaled us, but we missed it because we had not
+            // yet gone to sleep.
+            //
+            // With the lock held at this point, such a race condition is no longer
+            // possible.
+            if let WaitResult::Ok = on_woken(&mut guard)? {
+                return Ok(());
+            }
+
+            match self
+                .cond
+                .wait_interruptible_timeout(&mut guard, remaining_time)
+            {
+                kernel::sync::CondVarTimeoutResult::Woken { jiffies } => {
+                    match on_woken(&mut guard)? {
+                        WaitResult::Ok => return Ok(()),
+                        WaitResult::Retry => {
+                            remaining_time = remaining_time.saturating_sub(jiffies)
+                        }
+                    }
+                }
+                kernel::sync::CondVarTimeoutResult::Timeout => {
+                    // Try one last time before giving up.
+                    if let WaitResult::Ok = on_woken(&mut guard)? {
+                        return Ok(());
+                    }
+                    return Err(ETIMEDOUT);
+                }
+                kernel::sync::CondVarTimeoutResult::Signal { .. } => return Err(ERESTARTSYS),
+            }
+        }
+    }
+
+    pub(crate) fn notify_one(&self) {
+        let _guard = self.lock.lock();
+        self.cond.notify_one();
+    }
+
+    pub(crate) fn notify_all(&self) {
+        let _guard = self.lock.lock();
+        self.cond.notify_all();
+    }
+
+    /// Provides mutable access to the data protected by the lock.
+    pub(crate) fn with_locked_data<F, R>(&self, f: F) -> Result<R>
+    where
+        F: FnOnce(&mut T) -> Result<R>,
+    {
+        let mut data = self.lock.lock();
+        f(&mut data)
+    }
+}
+
+/// The result of a wait operation.
+///
+/// Use [`WaitResult::Ok`] to indicate that the wait was successful.
+///
+/// If the wakeup is spurious, or caused by an unrelated response, use
+/// [`WaitResult::Retry`].
+pub(crate) enum WaitResult {
+    Ok,
+    Retry,
+}
diff --git a/rust/kernel/device.rs b/rust/kernel/device.rs
index 5902b3714a..e70f73ef4c 100644
--- a/rust/kernel/device.rs
+++ b/rust/kernel/device.rs
@@ -243,7 +243,7 @@ pub unsafe fn drvdata_borrow<T: ForeignOwnable>(&self) -> T::Borrowed<'_> {
 
 impl<Ctx: DeviceContext> Device<Ctx> {
     /// Obtain the raw `struct device *`.
-    pub(crate) fn as_raw(&self) -> *mut bindings::device {
+    pub fn as_raw(&self) -> *mut bindings::device {
         self.0.get()
     }
 
diff --git a/rust/kernel/sync/aref.rs b/rust/kernel/sync/aref.rs
index 7c00a7d860..64832cc4c4 100644
--- a/rust/kernel/sync/aref.rs
+++ b/rust/kernel/sync/aref.rs
@@ -125,7 +125,7 @@ pub fn into_raw(me: Self) -> NonNull<T> {
     ///
     /// Callers must ensure that they have exclusive access to `T`. This also implies that no other
     /// `ARef`s may be taken out for `T`.
-    pub(crate) unsafe fn as_inner_mut(&mut self) -> &mut T {
+    pub unsafe fn as_inner_mut(&mut self) -> &mut T {
         // SAFETY: Our safety contract guarantees we have exclusive access to `T`
         unsafe { self.ptr.as_mut() }
     }
diff --git a/rust/uapi/uapi_helper.h b/rust/uapi/uapi_helper.h
index 1409441359..d4a239cf2a 100644
--- a/rust/uapi/uapi_helper.h
+++ b/rust/uapi/uapi_helper.h
@@ -9,6 +9,7 @@
 #include <uapi/asm-generic/ioctl.h>
 #include <uapi/drm/drm.h>
 #include <uapi/drm/nova_drm.h>
+#include <uapi/drm/panthor_drm.h>
 #include <uapi/linux/mdio.h>
 #include <uapi/linux/mii.h>
 #include <uapi/linux/ethtool.h>
-- 
2.51.0

