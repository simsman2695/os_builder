From 2776fc9044e3132176c6ee5776a851f8639f1510 Mon Sep 17 00:00:00 2001
From: Daniel Almeida <daniel.almeida@collabora.com>
Date: Tue, 14 Oct 2025 15:48:25 -0300
Subject: [PATCH 117/161] tyr: support intra-batch dependencies

---
 drivers/gpu/drm/tyr/file.rs        |  41 ++-
 drivers/gpu/drm/tyr/sched.rs       |  10 +-
 drivers/gpu/drm/tyr/sched/deps.rs  | 394 +++++++++++++++++++++++++++++
 drivers/gpu/drm/tyr/sched/group.rs |  97 +++----
 drivers/gpu/drm/tyr/sched/queue.rs |  35 +--
 5 files changed, 473 insertions(+), 104 deletions(-)
 create mode 100644 drivers/gpu/drm/tyr/sched/deps.rs

diff --git a/drivers/gpu/drm/tyr/file.rs b/drivers/gpu/drm/tyr/file.rs
index 06187af752..6ab465e336 100644
--- a/drivers/gpu/drm/tyr/file.rs
+++ b/drivers/gpu/drm/tyr/file.rs
@@ -335,11 +335,25 @@ pub(crate) fn group_submit(
 
             for _ in 0..queue.syncs.count {
                 let sync: SyncOp = sync_reader.read()?;
-                if sync.flags & !uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_SIGNAL as u32
-                    != 0
+
+                let valid_flags = (uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_SIGNAL
+                    | uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_WAIT
+                    | uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_MASK)
+                    as u32;
+
+                if sync.flags & !valid_flags != 0 {
+                    pr_err!("group_submit: invalid sync op flags: 0x{:x}", sync.flags);
+                    return Err(EINVAL);
+                }
+
+                // Validate handle type
+                let handle_type = sync.flags
+                    & uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_MASK as u32;
+                if handle_type != uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ as u32
+                    && handle_type != uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ as u32
                 {
-                    pr_err!("We only support DRM_PANTHOR_SYNC_OP_SIGNAL for now");
-                    return Err(ENOTSUPP);
+                    pr_err!("group_submit: invalid sync handle type: 0x{:x}", handle_type);
+                    return Err(EINVAL);
                 }
 
                 syncs.push(sync, GFP_KERNEL)?;
@@ -348,16 +362,6 @@ pub(crate) fn group_submit(
             queue_submits.push(queue, GFP_KERNEL)?;
         }
 
-        let mut out_syncs = kvec![];
-        for sync in syncs.iter().filter(|sync| {
-            sync.flags & uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_SIGNAL as u32 != 0
-        }) {
-            out_syncs.push(
-                drm::syncobj::SyncObj::lookup_handle(file, sync.handle)?,
-                GFP_KERNEL,
-            )?;
-        }
-
         let group = file
             .inner()
             .group_pool()
@@ -366,13 +370,7 @@ pub(crate) fn group_submit(
 
         tdev.with_locked_scheduler(|sched| {
             sched.bind(tdev, group.clone())?;
-            sched.submit(
-                kvec![],
-                out_syncs,
-                group,
-                queue_submits,
-                file.get_client_id(),
-            )
+            sched.submit(syncs, group, queue_submits, file)
         })?;
 
         Ok(0)
@@ -491,6 +489,7 @@ unsafe impl FromBytes for VmBindOp {}
 unsafe impl FromBytes for QueueCreate {}
 
 #[repr(transparent)]
+#[derive(Copy, Clone)]
 pub(crate) struct QueueSubmit(pub uapi::drm_panthor_queue_submit);
 
 // XXX: we cannot implement this trait for the uapi type directly, hence the
diff --git a/drivers/gpu/drm/tyr/sched.rs b/drivers/gpu/drm/tyr/sched.rs
index 9e8ad7938b..8da71ac67a 100644
--- a/drivers/gpu/drm/tyr/sched.rs
+++ b/drivers/gpu/drm/tyr/sched.rs
@@ -17,7 +17,9 @@
 use queue::Queue;
 
 use crate::driver::TyrDevice;
+use crate::file::DrmFile;
 use crate::file::QueueSubmit;
+use crate::file::SyncOp;
 use crate::fw::global::cs::CommandStream;
 use crate::fw::global::cs::StreamState;
 use crate::fw::global::csg;
@@ -28,6 +30,7 @@
 use crate::mmu::vm::WithLockedVm;
 use crate::TyrDriver;
 
+pub(crate) mod deps;
 mod events;
 pub(crate) mod group;
 pub(crate) mod job;
@@ -417,13 +420,12 @@ pub(crate) fn issue_dummy_instr(&mut self, group: Arc<Group>, tdev: &TyrDevice)
 
     pub(crate) fn submit(
         &mut self,
-        in_syncs: KVec<SyncObj<TyrDriver>>,
-        out_syncs: KVec<SyncObj<TyrDriver>>,
+        syncs: KVec<SyncOp>,
         group: Arc<Group>,
         queue_submits: KVec<QueueSubmit>,
-        client_id: u64,
+        file: &DrmFile,
     ) -> Result<KVec<UserFence<job::Fence>>> {
-        group.submit(in_syncs, out_syncs, queue_submits, client_id)
+        group.submit(syncs, queue_submits, file)
     }
 }
 
diff --git a/drivers/gpu/drm/tyr/sched/deps.rs b/drivers/gpu/drm/tyr/sched/deps.rs
new file mode 100644
index 0000000000..9e98649f67
--- /dev/null
+++ b/drivers/gpu/drm/tyr/sched/deps.rs
@@ -0,0 +1,394 @@
+// SPDX-License-Identifier: GPL-2.0 or MIT
+
+//! A way to track the internal dependencies of a group submit.
+
+use kernel::alloc::KVec;
+use kernel::dma_fence::Fence;
+use kernel::dma_fence::FenceChain;
+use kernel::drm::sched::Entity;
+use kernel::drm::syncobj::SyncObj;
+use kernel::prelude::*;
+use kernel::sync::Arc;
+use kernel::uapi;
+
+use crate::driver::TyrDriver;
+use crate::file::DrmFile;
+use crate::sched::job::Job;
+
+pub(crate) enum SyncHandle {
+    Binary { handle: u32 },
+    Timeline { handle: u32, timeline_value: u64 },
+}
+
+impl SyncHandle {
+    /// Get the handle for this sync operation
+    pub fn handle(&self) -> u32 {
+        match self {
+            SyncHandle::Binary { handle } => *handle,
+            SyncHandle::Timeline { handle, .. } => *handle,
+        }
+    }
+
+    /// Get the timeline value, or 0 for binary syncobjs
+    pub fn timeline_value(&self) -> u64 {
+        match self {
+            SyncHandle::Binary { .. } => 0,
+            SyncHandle::Timeline { timeline_value, .. } => *timeline_value,
+        }
+    }
+}
+
+pub(crate) struct SyncOp {
+    ty: SyncOpType,
+    /// The sync handle.
+    handle: SyncHandle,
+}
+
+impl TryFrom<&crate::file::SyncOp> for SyncOp {
+    type Error = Error;
+
+    fn try_from(uapi_sync: &crate::file::SyncOp) -> Result<Self> {
+        let handle_type = uapi_sync.flags
+            & uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_MASK as u32;
+
+        let ty = if uapi_sync.flags
+            & uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_SIGNAL as u32
+            != 0
+        {
+            SyncOpType::Signal
+        } else {
+            SyncOpType::Wait
+        };
+
+        // Create appropriate SyncHandle based on handle type
+        let handle = if handle_type
+            == uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ
+                as u32
+        {
+            SyncHandle::Timeline {
+                handle: uapi_sync.handle,
+                timeline_value: uapi_sync.timeline_value,
+            }
+        } else if handle_type
+            == uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ as u32
+        {
+            // Binary syncobjs should have timeline_value == 0
+            if uapi_sync.timeline_value != 0 {
+                return Err(EINVAL);
+            }
+            SyncHandle::Binary {
+                handle: uapi_sync.handle,
+            }
+        } else {
+            return Err(EINVAL);
+        };
+
+        Ok(SyncOp { ty, handle })
+    }
+}
+
+impl SyncOp {
+    /// Convert a slice of UAPI sync operations to internal representation
+    pub(crate) fn from_uapi_slice(uapi_syncs: &[crate::file::SyncOp]) -> Result<Arc<KVec<Self>>> {
+        let mut syncs = KVec::with_capacity(uapi_syncs.len(), GFP_KERNEL)?;
+        for uapi_sync in uapi_syncs.iter() {
+            syncs.push(Self::try_from(uapi_sync)?, GFP_KERNEL)?;
+        }
+        Ok(Arc::new(syncs, GFP_KERNEL)?)
+    }
+}
+
+/// Fence type for sync signals
+enum SyncFence {
+    /// Binary syncobj - just a fence
+    Binary(Option<Fence>),
+    /// Timeline syncobj - uses a fence chain, and stores current fence at this point
+    Timeline {
+        chain: FenceChain,
+        current_fence: Option<Fence>,
+    },
+}
+
+/// Internal sync signal tracking structure
+struct SyncSignal {
+    /// The syncobj handle
+    handle: u32,
+    /// The syncobj point (0 for regular syncobjs, non-zero for timeline syncobjs)
+    point: u64,
+    /// The syncobj reference
+    syncobj: SyncObj<TyrDriver>,
+    /// The fence or fence chain for this signal
+    fence_type: SyncFence,
+}
+
+impl SyncSignal {
+    /// Get the current fence for this signal (if any)
+    fn current_fence(&self) -> Option<&Fence> {
+        match &self.fence_type {
+            SyncFence::Binary(fence) => fence.as_ref(),
+            SyncFence::Timeline { current_fence, .. } => current_fence.as_ref(),
+        }
+    }
+}
+
+/// Job state tracking for a job in the submission context
+enum JobState {
+    /// Job is ready to be processed
+    Pending(Job),
+    /// Job has been taken out and is being processed
+    Taken,
+}
+
+pub(crate) struct JobContext {
+    state: JobState,
+
+    // The sync operations for the submission.
+    syncops: Arc<KVec<SyncOp>>,
+}
+
+#[repr(i32)]
+pub(crate) enum SyncOpType {
+    Wait = kernel::uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_WAIT,
+    Signal = kernel::uapi::drm_panthor_sync_op_flags_DRM_PANTHOR_SYNC_OP_SIGNAL,
+}
+
+/// A context for intra-batch job synchronization.
+///
+/// This implements roughly the same algorithms as Panthor:
+///
+/// - First the jobs need to be added to the context.
+///
+/// - Once all jobs are added, `collect_signal_ops` must be called to gather
+///   all signal operations in the batch.
+///
+/// - Then `add_deps_and_push_jobs` can be called to process all jobs:
+///
+/// - This will collect all dependency fences for each job, arm the job, update
+///   the signal operations with the job's done fence, and push the job to the
+///   scheduler.
+///
+/// - The dependencies come from either the signals collected in the context
+///   itself, or from a previously submitted job (in which case, we find the fence
+///   with `drm_syncobj_find_fence`).
+pub(crate) struct Context<'a> {
+    /// The DRM file that this submission happens on.
+    file: &'a DrmFile,
+
+    /// The list of jobs in this submission.
+    jobs: KVec<JobContext>,
+
+    /// Internal signal registry for intra-batch dependencies
+    ///
+    /// This tracks all signal operations that will be produced by jobs in this batch,
+    /// allowing later jobs to depend on earlier jobs' completion fences.
+    signals: KVec<SyncSignal>,
+}
+
+impl<'a> Context<'a> {
+    pub(crate) fn new(file: &'a DrmFile) -> Self {
+        Context {
+            file,
+            jobs: KVec::new(),
+            signals: KVec::new(),
+        }
+    }
+
+    pub(crate) fn add_job(&mut self, job: Job, syncops: Arc<KVec<SyncOp>>) -> Result {
+        self.jobs
+            .push(
+                JobContext {
+                    state: JobState::Pending(job),
+                    syncops,
+                },
+                GFP_KERNEL,
+            )
+            .map_err(Into::into)
+    }
+
+    /// Collect all signal operations in a batch.
+    pub(crate) fn collect_signal_ops(&mut self, syncops: &[SyncOp]) -> Result {
+        for syncop in syncops.iter() {
+            if matches!(syncop.ty, SyncOpType::Signal) {
+                self.get_sync_signal(syncop.handle.handle(), syncop.handle.timeline_value())?;
+            }
+        }
+        Ok(())
+    }
+
+    /// Add jobs dependencies, arm jobs, and push them to the scheduler
+    ///
+    /// This method takes the entity as a parameter, processes all jobs, and pushes them
+    /// immediately to avoid lifetime conflicts.
+    pub(crate) fn add_deps_and_push_jobs(&mut self, entity: &mut Entity<Job>) -> Result {
+        for job_idx in 0..self.jobs.len() {
+            let fences = self.collect_job_deps(job_idx)?;
+
+            let job = match core::mem::replace(&mut self.jobs[job_idx].state, JobState::Taken) {
+                JobState::Pending(job) => job,
+                JobState::Taken => {
+                    // This should never happen - we're processing jobs sequentially
+                    return Err(EINVAL);
+                }
+            };
+
+            let mut pending_job = entity.new_job(1, 0, job)?;
+
+            for fence in fences {
+                pending_job.add_dependency(fence)?;
+            }
+
+            let mut armed_job = pending_job.arm();
+
+            // Update the sync signal fences with the job's completion fence
+            self.update_job_syncs(job_idx, armed_job.fences().finished())?;
+
+            // Note: In the C code, there's an `upd_resvs` callback here
+            // that updates reservation objects. We're skipping that for now.
+            armed_job.push();
+        }
+
+        Ok(())
+    }
+
+    /// Push signal fences to their associated syncobjs
+    ///
+    /// This is the last step of a submission procedure, and is done once we know
+    /// the submission is effective and job fences are guaranteed to be signaled
+    /// in finite time.
+    pub(crate) fn push_fences(self) {
+        for sig_sync in self.signals.into_iter() {
+            match sig_sync.fence_type {
+                SyncFence::Binary(fence) => {
+                    // For binary syncobjs, replace the fence
+                    if let Some(fence) = fence {
+                        sig_sync.syncobj.replace_fence(Some(&fence));
+                    }
+                }
+                SyncFence::Timeline {
+                    chain,
+                    current_fence,
+                } => {
+                    // For timeline syncobjs, add a point using the fence chain
+                    if let Some(fence) = current_fence {
+                        sig_sync.syncobj.add_point(chain, &fence, sig_sync.point);
+                    }
+                }
+            }
+        }
+    }
+
+    fn search_sync_signal(&self, handle: u32, point: u64) -> Option<&SyncSignal> {
+        self.signals
+            .iter()
+            .find(|sig| sig.handle == handle && sig.point == point)
+    }
+
+    fn add_sync_signal(&mut self, handle: u32, point: u64) -> Result {
+        let syncobj = SyncObj::lookup_handle(self.file, handle)?;
+
+        // Retrieve the current fence attached to that point.
+        //
+        // If we get a None here it just means there's no fence attached to that
+        // point yet.
+        //
+        // For binary syncobjs, point will be 0; for timeline syncobjs, it's the
+        // actual point
+        let current_fence =
+            SyncObj::<TyrDriver>::find_fence(self.file, handle, point, 0).unwrap_or(None);
+
+        let fence_type = if point > 0 {
+            let chain = FenceChain::new()?;
+            SyncFence::Timeline {
+                chain,
+                current_fence,
+            }
+        } else {
+            SyncFence::Binary(current_fence)
+        };
+
+        let signal = SyncSignal {
+            handle,
+            point,
+            syncobj,
+            fence_type,
+        };
+
+        self.signals.push(signal, GFP_KERNEL)?;
+        Ok(())
+    }
+
+    fn get_sync_signal(&mut self, handle: u32, point: u64) -> Result {
+        if self.search_sync_signal(handle, point).is_some() {
+            return Ok(());
+        }
+
+        self.add_sync_signal(handle, point)
+    }
+
+    fn collect_job_deps(&self, job_idx: usize) -> Result<KVec<Fence>> {
+        let syncops = &self.jobs[job_idx].syncops;
+        let mut deps = KVec::new();
+
+        for syncop in syncops.iter() {
+            if !matches!(syncop.ty, SyncOpType::Wait) {
+                continue;
+            }
+
+            let handle = syncop.handle.handle();
+            let point = syncop.handle.timeline_value();
+
+            // First check if we have this signal in our internal context.
+            let fence = if let Some(sig_sync) = self.search_sync_signal(handle, point) {
+                match sig_sync.current_fence() {
+                    Some(f) => f.clone(),
+                    None => return Err(EINVAL),
+                }
+            } else {
+                // Otherwise, this is from a different submission - look it up.
+                match SyncObj::<TyrDriver>::find_fence(self.file, handle, point, 0)? {
+                    Some(f) => f,
+                    None => return Err(EINVAL), // A wait for which we can't find a fence is broken.
+                }
+            };
+
+            deps.push(fence, GFP_KERNEL)?;
+        }
+
+        Ok(deps)
+    }
+
+    fn update_job_syncs(&mut self, job_idx: usize, done_fence: Fence) -> Result {
+        // Get the sync operations for this job
+        let syncops = self.jobs[job_idx].syncops.clone();
+
+        for syncop in syncops.iter() {
+            if !matches!(syncop.ty, SyncOpType::Signal) {
+                continue;
+            }
+
+            let handle = syncop.handle.handle();
+            let point = syncop.handle.timeline_value();
+
+            // Find the signal in our internal registry
+            if let Some(sig_sync) = self
+                .signals
+                .iter_mut()
+                .find(|sig| sig.handle == handle && sig.point == point)
+            {
+                // Update the fence in the signal with the job's done fence
+                match &mut sig_sync.fence_type {
+                    SyncFence::Binary(fence) => {
+                        *fence = Some(done_fence.clone());
+                    }
+                    SyncFence::Timeline { current_fence, .. } => {
+                        *current_fence = Some(done_fence.clone());
+                    }
+                }
+            } else {
+                return Err(EINVAL);
+            }
+        }
+
+        Ok(())
+    }
+}
diff --git a/drivers/gpu/drm/tyr/sched/group.rs b/drivers/gpu/drm/tyr/sched/group.rs
index b68b4a1f96..3948549160 100644
--- a/drivers/gpu/drm/tyr/sched/group.rs
+++ b/drivers/gpu/drm/tyr/sched/group.rs
@@ -16,6 +16,7 @@
 use crate::driver::TyrDevice;
 use crate::file::DrmFile;
 use crate::file::QueueSubmit;
+use crate::file::SyncOp;
 use crate::fw::global::csg;
 use crate::fw::global::csg::Priority;
 use crate::fw::SharedSectionEntry;
@@ -24,6 +25,7 @@
 use crate::mmu::vm::PreparedVm;
 use crate::mmu::vm::Vm;
 use crate::mmu::vm::WithLockedVm;
+use crate::sched::deps;
 use crate::sched::syncs::SyncObj64b;
 use crate::TyrDriver;
 
@@ -66,38 +68,6 @@ pub(crate) struct GroupInner {
     pub(super) syncobjs: gem::ObjectRef,
 }
 
-impl GroupInner {
-    pub(crate) fn submit(
-        &mut self,
-        in_syncs: &KVec<SyncObj<TyrDriver>>,
-        out_syncs: &KVec<SyncObj<TyrDriver>>,
-        group: Arc<Group>,
-        queue_submit: QueueSubmit,
-        prepared_vm: &PreparedVm<'_>,
-        client_id: u64,
-    ) -> Result<UserFence<job::Fence>> {
-        let queue = self
-            .queues
-            .get_mut(queue_submit.queue_index as usize)
-            .ok_or(EINVAL)?;
-
-        let sync_addr = self.syncobjs.kernel_va().ok_or(EINVAL)?;
-        let sync_addr = sync_addr.start
-            + u64::from(queue_submit.queue_index)
-                * core::mem::size_of::<syncs::SyncObj64b>() as u64;
-
-        queue.submit(
-            in_syncs,
-            out_syncs,
-            group,
-            sync_addr,
-            queue_submit,
-            prepared_vm,
-            client_id,
-        )
-    }
-}
-
 /// A scheduling group object, usually backing an execution context, e.g.: a
 /// VkQueue or similar.
 ///
@@ -284,10 +254,9 @@ pub(crate) fn with_locked_inner<F, R>(&self, f: F) -> Result<R>
 
     pub(super) fn submit(
         self: Arc<Self>,
-        in_syncs: KVec<SyncObj<TyrDriver>>,
-        out_syncs: KVec<SyncObj<TyrDriver>>,
+        syncs: KVec<SyncOp>,
         queue_submits: KVec<QueueSubmit>,
-        client_id: u64,
+        file: &DrmFile,
     ) -> Result<KVec<UserFence<job::Fence>>> {
         if self.vm.lock().address_space().is_none() {
             pr_err!("group_submit: invalid address space");
@@ -301,26 +270,62 @@ pub(super) fn submit(
             return Err(EINVAL);
         }
 
+        // Convert UAPI sync operations to internal representation
+        let internal_syncs = deps::SyncOp::from_uapi_slice(&syncs)?;
+
+        let mut ctx = deps::Context::new(file);
+
         let mut fences = KVec::with_capacity(queue_submits.len(), GFP_KERNEL)?;
 
         let vm = self.vm.lock();
+
+        // Create all jobs and add them to the context
         self.with_locked_inner(|inner| {
             vm.with_prepared_vm(queue_submits.len() as u32, |locked_vm| {
-                queue_submits.into_iter().try_for_each(|queue_submit| {
-                    let fence = inner.submit(
-                        &in_syncs,
-                        &out_syncs,
-                        self.clone(),
-                        queue_submit,
-                        &locked_vm,
-                        client_id,
-                    )?;
+                for queue_submit in queue_submits.iter() {
+                    let queue = inner
+                        .queues
+                        .get_mut(queue_submit.queue_index as usize)
+                        .ok_or(EINVAL)?;
+
+                    let sync_addr = inner.syncobjs.kernel_va().ok_or(EINVAL)?;
+                    let sync_addr = sync_addr.start
+                        + u64::from(queue_submit.queue_index)
+                            * core::mem::size_of::<syncs::SyncObj64b>() as u64;
+
+                    let fence: UserFence<_> = queue
+                        .fence_ctx
+                        .new_fence(0, crate::sched::job::Fence)?
+                        .into();
+
+                    let job =
+                        job::Job::create(*queue_submit, self.clone(), fence.clone(), sync_addr)?;
+
+                    ctx.add_job(job, internal_syncs.clone())?;
+
+                    // Store the fence to return later
                     fences.push(fence, GFP_KERNEL)?;
-                    Ok(())
-                })
+                }
+                Ok(())
             })
         })?;
 
+        ctx.collect_signal_ops(&internal_syncs)?;
+
+        // Now process jobs through their respective queue entities
+        // For now, we assume single queue submit (as enforced in file.rs)
+        // In the future, this will need to be extended to handle multiple queues
+        if !queue_submits.is_empty() {
+            let queue_idx = queue_submits[0].queue_index as usize;
+            self.with_locked_inner(|inner| {
+                let queue = inner.queues.get_mut(queue_idx).ok_or(EINVAL)?;
+                ctx.add_deps_and_push_jobs(&mut queue.entity)
+            })?;
+        }
+
+        // Push all signal fences to their syncobjs
+        ctx.push_fences();
+
         Ok(fences)
     }
 }
diff --git a/drivers/gpu/drm/tyr/sched/queue.rs b/drivers/gpu/drm/tyr/sched/queue.rs
index 856b13ab8b..ddf28bba0f 100644
--- a/drivers/gpu/drm/tyr/sched/queue.rs
+++ b/drivers/gpu/drm/tyr/sched/queue.rs
@@ -4,12 +4,12 @@
 
 use kernel::c_str;
 use kernel::devres::Devres;
+use kernel::dma_fence::FenceChain;
 use kernel::dma_fence::FenceContexts;
 use kernel::dma_fence::UserFence;
 use kernel::drm::sched;
 use kernel::drm::sched::Entity;
 use kernel::drm::sched::Scheduler;
-use kernel::drm::syncobj::SyncObj;
 use kernel::io::mem::IoMem;
 use kernel::prelude::*;
 use kernel::sizes::SZ_4K;
@@ -19,12 +19,10 @@
 
 use crate::driver::TyrDevice;
 use crate::file::QueueCreate;
-use crate::file::QueueSubmit;
 use crate::fw::global::cs::RingBufferInput;
 use crate::fw::global::cs::RingBufferOutput;
 use crate::gem;
 use crate::mmu::vm::map_flags;
-use crate::mmu::vm::PreparedVm;
 use crate::mmu::vm::Vm;
 use crate::regs::Doorbell;
 use crate::sched::job::Job;
@@ -42,7 +40,7 @@ pub(crate) struct Queue {
     scheduler: Scheduler<Job>,
 
     /// The DRM entity used for this queue.
-    entity: Entity<Job>,
+    pub(super) entity: Entity<Job>,
 
     /// A priority number, between 0 and 15.
     pub(crate) priority: u8,
@@ -189,35 +187,6 @@ pub(crate) fn append_instrs(&mut self, instrs: &[u8]) -> Result {
     pub(crate) fn kick(&self) -> Result {
         Doorbell::new(self.doorbell_id.ok_or(EINVAL)?).write(&self.iomem, 1)
     }
-
-    pub(crate) fn submit(
-        &mut self,
-        _in_syncs: &KVec<SyncObj<TyrDriver>>,
-        out_syncs: &KVec<SyncObj<TyrDriver>>,
-        group: Arc<Group>,
-        sync_addr: u64,
-        queue_submit: QueueSubmit,
-        _: &PreparedVm<'_>,
-        client_id: u64,
-    ) -> Result<UserFence<job::Fence>> {
-        let fence: UserFence<_> = self
-            .fence_ctx
-            .new_fence(0, crate::sched::job::Fence)?
-            .into();
-
-        let job = Job::create(queue_submit, group, fence.clone(), sync_addr)?;
-
-        let mut job = self.entity.new_job(1, client_id, job)?.arm();
-        let out_fence = job.fences().finished();
-
-        job.push();
-
-        for sync in out_syncs {
-            sync.replace_fence(Some(&out_fence));
-        }
-
-        Ok(fence)
-    }
 }
 
 /// The interface for ring buffer control.
-- 
2.51.0

