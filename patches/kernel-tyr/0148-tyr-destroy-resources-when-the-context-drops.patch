From 95e9be6083eaad0dc07ac1c2e6367f425068bdf6 Mon Sep 17 00:00:00 2001
From: Daniel Almeida <daniel.almeida@collabora.com>
Date: Wed, 29 Oct 2025 14:09:36 -0300
Subject: [PATCH 148/161] tyr: destroy resources when the context drops

Userspace expects all allocated resources to be implicitly dropped when
it closes the file descriptor.
---
 drivers/gpu/drm/tyr/file.rs        | 26 +++++++++++++++++++++++++-
 drivers/gpu/drm/tyr/mmu/vm/pool.rs | 18 ++++++++++++++++++
 drivers/gpu/drm/tyr/sched/group.rs | 18 ++++++++++++++++++
 3 files changed, 61 insertions(+), 1 deletion(-)

diff --git a/drivers/gpu/drm/tyr/file.rs b/drivers/gpu/drm/tyr/file.rs
index 45ebd1ad1e..14d3ff2b9f 100644
--- a/drivers/gpu/drm/tyr/file.rs
+++ b/drivers/gpu/drm/tyr/file.rs
@@ -16,6 +16,7 @@
 use kernel::uapi;
 use kernel::xarray;
 use kernel::xarray::XArray;
+use pin_init::pinned_drop;
 
 use crate::driver::TyrDevice;
 use crate::driver::TyrDriver;
@@ -31,7 +32,7 @@
 use crate::sched::deps;
 use crate::sched::group;
 
-#[pin_data]
+#[pin_data(PinnedDrop)]
 pub(crate) struct File {
     /// A pool storing our VMs for this particular context.
     #[pin]
@@ -44,6 +45,9 @@ pub(crate) struct File {
     /// Each VM can have its own heap pool for tiler heap management.
     /// The heap pool is created on-demand when the first heap context is created.
     heap_pools: Pin<KBox<XArray<KBox<heap::Pool>>>>,
+
+    /// Reference to the device for cleanup
+    tdev: ARef<TyrDevice>,
 }
 
 /// Convenience type alias for our DRM `File` type
@@ -55,17 +59,37 @@ impl drm::file::DriverFile for File {
     fn open(dev: &DrmDevice<Self::Driver>) -> Result<Pin<KBox<Self>>> {
         dev_dbg!(dev.as_ref(), "drm::device::Device::open\n");
 
+        let tdev = ARef::from(dev);
+
         KBox::try_pin_init(
             try_pin_init!(Self {
                 vm_pool: Pool::create()?,
                 group_pool: group::Pool::create()?,
                 heap_pools <- KBox::pin_init(XArray::new(xarray::AllocKind::Alloc1), GFP_KERNEL)?,
+                tdev,
             }),
             GFP_KERNEL,
         )
     }
 }
 
+#[pinned_drop]
+impl PinnedDrop for File {
+    fn drop(self: Pin<&mut Self>) {
+        pr_info!("Cleaning up DRM file resources\n");
+
+        // Destroy all groups (this unbinds them if necessary)
+        if let Err(e) = self.as_ref().group_pool().destroy_all(&self.tdev) {
+            pr_err!("Failed to destroy all groups: {:?}\n", e);
+        }
+
+        // Destroy all VMs
+        if let Err(e) = self.as_ref().vm_pool().destroy_all(self.tdev.iomem.clone()) {
+            pr_err!("Failed to destroy all VMs: {:?}\n", e);
+        }
+    }
+}
+
 impl File {
     pub(crate) fn dev_query(
         tdev: &TyrDevice,
diff --git a/drivers/gpu/drm/tyr/mmu/vm/pool.rs b/drivers/gpu/drm/tyr/mmu/vm/pool.rs
index 7b54c134cf..30a4e4be21 100644
--- a/drivers/gpu/drm/tyr/mmu/vm/pool.rs
+++ b/drivers/gpu/drm/tyr/mmu/vm/pool.rs
@@ -77,4 +77,22 @@ pub(crate) fn destroy_vm(self: Pin<&Self>, index: usize, iomem: Arc<Devres<IoMem
         vm.destroyed = true;
         vm.unmap_all(iomem)
     }
+
+    /// Destroy all VMs in the pool.
+    ///
+    /// This is called when the file is being closed to ensure all VMs
+    /// are properly unmapped before being dropped.
+    pub(crate) fn destroy_all(self: Pin<&Self>, iomem: Arc<Devres<IoMem>>) -> Result {
+        let max_index = self.free_index.load(core::sync::atomic::Ordering::Relaxed);
+
+        // Try to destroy all possible VMs from 0 to free_index, as there is no
+        // iterator implementation in xarray.rs.
+        for index in 0..max_index {
+            if let Ok(_) = self.destroy_vm(index, iomem.clone()) {
+                pr_info!("Destroyed VM at index {}\n", index);
+            }
+        }
+
+        Ok(())
+    }
 }
diff --git a/drivers/gpu/drm/tyr/sched/group.rs b/drivers/gpu/drm/tyr/sched/group.rs
index 88df6a2a34..3e03631a6b 100644
--- a/drivers/gpu/drm/tyr/sched/group.rs
+++ b/drivers/gpu/drm/tyr/sched/group.rs
@@ -445,4 +445,22 @@ pub(crate) fn destroy_group(self: Pin<&Self>, tdev: &TyrDevice, index: usize) ->
             Ok(())
         })
     }
+
+    /// Destroy all groups in the pool.
+    ///
+    /// This is called when the file is being closed to ensure all groups
+    /// are properly cleaned up (unbound if necessary) before being dropped.
+    pub(crate) fn destroy_all(self: Pin<&Self>, tdev: &TyrDevice) -> Result {
+        let max_index = self.free_index.load(core::sync::atomic::Ordering::Relaxed);
+
+        // Try to destroy all possible groups from 0 to free_index as there's no
+        // iterator implementation in xarray.rs.
+        for index in 0..max_index {
+            if let Ok(_) = self.destroy_group(tdev, index) {
+                pr_info!("Destroyed group at index {}\n", index);
+            }
+        }
+
+        Ok(())
+    }
 }
-- 
2.51.0

