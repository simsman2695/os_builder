From 54f54c224fbfe503441b73923c8f1c7b81c6f59b Mon Sep 17 00:00:00 2001
From: Alice Ryhl <aliceryhl@google.com>
Date: Tue, 3 Jun 2025 09:48:53 +0000
Subject: [PATCH 089/161] tyr: manage AS slots

Signed-off-by: Alice Ryhl <aliceryhl@google.com>
---
 drivers/gpu/drm/tyr/mmu.rs                |  75 ++++++++------
 drivers/gpu/drm/tyr/mmu/slot_allocator.rs | 118 ++++++++++++----------
 drivers/gpu/drm/tyr/sched/events.rs       |   1 +
 3 files changed, 111 insertions(+), 83 deletions(-)

diff --git a/drivers/gpu/drm/tyr/mmu.rs b/drivers/gpu/drm/tyr/mmu.rs
index f594029a57..46090bcea7 100644
--- a/drivers/gpu/drm/tyr/mmu.rs
+++ b/drivers/gpu/drm/tyr/mmu.rs
@@ -25,27 +25,23 @@
 mod as_lock;
 mod faults;
 pub(crate) mod irq;
+mod slot_allocator;
 pub(crate) mod vm;
 
+use self::slot_allocator::SlotAllocator;
+
 pub(crate) struct Mmu {
     /// List containing all VMs.
     vms: KVec<Arc<Mutex<Vm>>>,
     /// Tracks which of the 32 AS slots are free.
-    free_slots: usize,
-    // slot_allocator: Arc<Mutex<SlotAllocator>>,
+    slots: SlotAllocator,
 }
 
 impl Mmu {
     pub(crate) fn new() -> Result<Self> {
         Ok(Self {
             vms: KVec::new(),
-            // slot_allocator: Arc::pin_init(
-            //     new_mutex!(SlotAllocator {
-            //         free_mask: u32::MAX,
-            //     }),
-            //     GFP_KERNEL,
-            // )?,
-            free_slots: usize::MAX & !1,
+            slots: SlotAllocator::new(),
         })
     }
 
@@ -75,16 +71,6 @@ fn flush_range(
         Self::do_as_command(iomem, as_nr, AS_COMMAND_FLUSH_PT, range)
     }
 
-    fn allocate_as(&mut self) -> Result<usize> {
-        let slot = self.free_slots.trailing_zeros();
-        if slot == 32 {
-            return Err(EBUSY);
-        }
-
-        self.free_slots |= 1 << slot;
-        Ok(slot as usize)
-    }
-
     fn wait_ready(iomem: &Devres<IoMem>, as_nr: usize) -> Result {
         let op = || as_status(as_nr)?.read(iomem);
         let cond = |status: &u32| -> bool { *status & AS_STATUS_ACTIVE == 0 };
@@ -98,12 +84,6 @@ fn wait_ready(iomem: &Devres<IoMem>, as_nr: usize) -> Result {
         Ok(())
     }
 
-    /// TODO: The code to manage AS slots is still TODO.
-    #[allow(unused)]
-    fn free_as(&mut self, as_nr: usize) {
-        self.free_slots &= !(1 << as_nr);
-    }
-
     fn do_as_command(
         iomem: &Devres<IoMem>,
         as_nr: usize,
@@ -141,14 +121,25 @@ pub(crate) fn bind_vm(
             | as_transcfg_ina_bits((55 - va_bits).into());
 
         let memattr = vm.memattr;
-        let as_nr = if vm.for_mcu { 0 } else { self.allocate_as()? };
-
-        Self::enable_as(iomem, as_nr, transtab, transcfg, memattr)?;
-
+        let as_nr = self.slots.find_slot(vm.for_mcu)?;
+        Self::enable_as(iomem, as_nr, transtab, transcfg.into(), memattr)?;
+        self.slots.alloc_slot(as_nr);
         vm.address_space = Some(as_nr);
         Ok(())
     }
 
+    pub(crate) fn unbind_vm(&mut self, vm: &Arc<Mutex<Vm>>, iomem: &Devres<IoMem>) -> Result {
+        let mut vm = vm.lock();
+        let as_nr = vm
+            .address_space
+            .ok_or(EINVAL)
+            .inspect_err(|_| pr_warn!("Unbinding vm without AS"))?;
+        Self::disable_as(iomem, as_nr)?;
+        vm.address_space = None;
+        self.slots.free_slot(as_nr);
+        Ok(())
+    }
+
     fn enable_as(
         iomem: &Devres<IoMem>,
         as_nr: usize,
@@ -181,8 +172,32 @@ fn enable_as(
         as_memattr_lo(as_nr)?.write(iomem, memattr_lo)?;
         as_memattr_hi(as_nr)?.write(iomem, memattr_hi)?;
 
+        let op = || as_status(as_nr)?.read(iomem);
+        let cond = |status: &u32| -> bool { *status & AS_STATUS_ACTIVE == 0 };
+        let _ = io::poll::read_poll_timeout(
+            op,
+            cond,
+            Delta::from_millis(0),
+            Some(Delta::from_micros(200)),
+        )?;
+
         as_command(as_nr)?.write(iomem, AS_COMMAND_UPDATE)?;
 
+        Ok(())
+    }
+
+    fn disable_as(iomem: &Devres<IoMem>, as_nr: usize) -> Result {
+        Self::do_as_command(iomem, as_nr, AS_COMMAND_FLUSH_MEM, 0..u64::MAX)?;
+
+        as_transtab_lo(as_nr)?.write(iomem, 0)?;
+        as_transtab_hi(as_nr)?.write(iomem, 0)?;
+
+        as_memattr_lo(as_nr)?.write(iomem, 0)?;
+        as_memattr_hi(as_nr)?.write(iomem, 0)?;
+
+        as_transcfg_lo(as_nr)?.write(iomem, AS_TRANSCFG_ADRMODE_UNMAPPED as u32)?;
+        as_transcfg_hi(as_nr)?.write(iomem, 0)?;
+
         let op = || as_status(as_nr)?.read(iomem);
         let cond = |status: &u32| -> bool { *status & AS_STATUS_ACTIVE == 0 };
         let _ = io::poll::read_poll_timeout(
@@ -192,6 +207,8 @@ fn enable_as(
             Delta::from_micros(200),
         )?;
 
+        as_command(as_nr)?.write(iomem, AS_COMMAND_UPDATE)?;
+
         Ok(())
     }
 }
diff --git a/drivers/gpu/drm/tyr/mmu/slot_allocator.rs b/drivers/gpu/drm/tyr/mmu/slot_allocator.rs
index 8ba5f8cd3d..bc4c5ce053 100644
--- a/drivers/gpu/drm/tyr/mmu/slot_allocator.rs
+++ b/drivers/gpu/drm/tyr/mmu/slot_allocator.rs
@@ -3,57 +3,67 @@
 //! All VMs have to be placed on a physical slot to become active. This file
 //! implements an allocator to track which slots are active, and later to evict
 //! the least recently used one if needed.
-//!
-//! Implementing this allocator is a TODO. For now, we just return EBUSY when
-//! all slots are taken, and slots are never freed once inactive.
-
-// /// Alocates HW AS slots, which represent a physical slot where a VM can be
-// /// placed in.
-// ///
-// /// Panthor keeps a LRU list for the purposes of evicting VMs when a slot is
-// /// requested but no one is free. We defer this to a future implementation.
-// ///
-// /// Note that this is still TODO: this type doesn't yet track any VMs.
-// struct SlotAllocator {
-//     /// How many slots are free.
-//     free_mask: u32,
-// }
-
-// impl SlotAllocator {
-//     fn alloc_slot(allocator: Arc<Mutex<Self>>, vm: &mut Vm) {
-//         let mut alloc = allocator.lock();
-//         let slot = alloc.free_mask.trailing_zeros();
-
-//         if slot < 32 {
-//             alloc.free_mask |= 1 << slot;
-//             let slot_allocation = SlotAllocation {
-//                 allocator: allocator.clone(),
-//                 slot: slot as u8,
-//             };
-//             vm.binding = Some(slot_allocation);
-//         }
-//     }
-
-//     fn free_slot(vm: &mut Vm) {
-//         vm.binding = None;
-//     }
-// }
-
-// /// Represents a slot allocation.
-// ///
-// /// This type returns the slot to the allocator once it is dropped.
-// ///
-// ///
-// /// Note that this is still TODO: this type doesn't yet track any VMs.
-// struct SlotAllocation {
-//     /// The allocator that allocated this slot.
-//     allocator: Arc<Mutex<SlotAllocator>>,
-//     /// The actual slot value.
-//     slot: u8,
-// }
-
-// impl Drop for SlotAllocation {
-//     fn drop(&mut self) {
-//         self.allocator.lock().free_mask &= !(1 << self.slot);
-//     }
-// }
+
+use kernel::{bits::bit_u32, prelude::*};
+
+fn as_nr_mask(as_nr: usize) -> u32 {
+    bit_u32(as_nr as u32)
+}
+
+/// Alocates HW AS slots, which represent a physical slot where a VM can be
+/// placed in.
+///
+/// Panthor keeps a LRU list for the purposes of evicting VMs when a slot is
+/// requested but no one is free. We defer this to a future implementation.
+pub(crate) struct SlotAllocator {
+    /// Which AS slots are free.
+    occupied_mask: u32,
+}
+
+impl SlotAllocator {
+    pub(crate) fn new() -> Self {
+        Self { occupied_mask: 0 }
+    }
+
+    pub(crate) fn find_slot(&mut self, for_mcu: bool) -> Result<usize> {
+        if for_mcu {
+            self.find_slot_zero()
+        } else {
+            self.find_slot_inner()
+        }
+    }
+
+    fn find_slot_zero(&mut self) -> Result<usize> {
+        if self.occupied_mask & 1 == 0 {
+            Ok(0)
+        } else {
+            Err(EBUSY)
+        }
+    }
+
+    fn find_slot_inner(&mut self) -> Result<usize> {
+        let as_nr = (self.occupied_mask | 1).trailing_ones();
+
+        if as_nr < 32 {
+            Ok(as_nr as usize)
+        } else {
+            Err(EBUSY)
+        }
+    }
+
+    pub(crate) fn alloc_slot(&mut self, as_nr: usize) {
+        let mask = as_nr_mask(as_nr);
+        if self.occupied_mask & mask != 0 {
+            pr_err!("AS slot {as_nr} already allocated.\n");
+        }
+        self.occupied_mask |= mask;
+    }
+
+    pub(crate) fn free_slot(&mut self, as_nr: usize) {
+        let mask = as_nr_mask(as_nr);
+        if self.occupied_mask & mask == 0 {
+            pr_err!("AS slot {as_nr} already free.\n");
+        }
+        self.occupied_mask &= !mask;
+    }
+}
diff --git a/drivers/gpu/drm/tyr/sched/events.rs b/drivers/gpu/drm/tyr/sched/events.rs
index 79e95b67a4..9f1b700306 100644
--- a/drivers/gpu/drm/tyr/sched/events.rs
+++ b/drivers/gpu/drm/tyr/sched/events.rs
@@ -246,6 +246,7 @@ fn update_group(&mut self, group: Arc<Group>, data: &TyrData) -> Result {
     }
 
     fn mark_group_idle(&mut self, group: Arc<Group>, data: &TyrData) -> Result {
+        data.with_locked_mmu(|mmu| mmu.unbind_vm(&group.vm, &data.iomem))?;
         self.idle_groups[group.priority as usize].push(group, GFP_KERNEL)?;
         Ok(())
     }
-- 
2.51.0

